{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U kaleido\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import significantdigits as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path to your JSON file\n",
    "Results_tableNoConf=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/Result_tableNoConf.pkl')\n",
    "Results_tableWConf=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/Result_tableWConf.pkl')\n",
    "subjects_to_remove = ['sub-172260', 'sub-43046', 'sub-43045', 'sub-116870', 'sub-160040', 'sub-168195',  'sub-162941', 'sub-156734', 'sub-140041']#they have RLSplit so has been removed \n",
    "Results_tableWConf = Results_tableWConf[~Results_tableWConf['subject'].isin(subjects_to_remove)]\n",
    "\n",
    "Results_tableNoConf = Results_tableNoConf[~Results_tableNoConf['subject'].isin(subjects_to_remove)]\n",
    "\n",
    "# filtered_results = Results_table[(Results_table['subject'] == 'sub-133486') &  (Results_table['session'] == 1) &  (Results_table['acquisition'] == 'acq-AP') &    (Results_table['repetition'] == 1)]\n",
    "# Define the column names\n",
    "Anatcolumns = [\n",
    "    'repetition', \n",
    "    'session',\n",
    "    'acquisition', \n",
    "    'stacked_matrices',\n",
    "    'std_matrix',\n",
    "    'avr_matrix',\n",
    "    'significantdigits_matrix'\n",
    "]\n",
    "# Define the column names\n",
    "Numcolumns = [\n",
    "    'subject', \n",
    "    'session',\n",
    "    'acquisition', \n",
    "    'stacked_matrices',\n",
    "    'std_matrix',\n",
    "    'avr_matrix',\n",
    "    'significantdigits_matrix',\n",
    "    'std',\n",
    "    'mean'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.DataFrame(columns=Anatcolumns)\n",
    "all_iteration_data = []\n",
    "r=Results_tableNoConf.groupby([\"repetition\",\"session\",\"acquisition\"]).apply(lambda x : x)\n",
    "for i in r.repetition.unique():\n",
    "    for j in r.session.unique():\n",
    "        for k in r.acquisition.unique():\n",
    "            x = Results_tableNoConf[(Results_tableNoConf['repetition'] == i) & (Results_tableNoConf['session']==j) & (Results_tableNoConf['acquisition']==k)]\n",
    "            if len(x)<2:\n",
    "                continue\n",
    "            # mean = np.array(x[['correlation_matrix']]).squeeze().mean(axis=0)\n",
    "            # std = np.array(x[['correlation_matrix']]).squeeze().std(axis=0)\n",
    "            # print(type(np.array(x[['correlation_matrix']]).squeeze()))\n",
    "            # print(np.array(x[['correlation_matrix']]).squeeze().shape)\n",
    "            # print(type(mean), mean.shape)\n",
    "            correlation_matrices = np.stack(x['correlation_matrix'].values)\n",
    "            mean=np.mean(correlation_matrices,axis=0)\n",
    "            std=np.std(correlation_matrices,axis=0)\n",
    "            # display(correlation_matrices.dtype, correlation_matrices.shape)\n",
    "            sigdig_matrix = sd.significant_digits(\n",
    "                    correlation_matrices, \n",
    "                    reference=mean, \n",
    "                    basis=10,\n",
    "                    dtype=np.float32\n",
    "                )\n",
    "            sigdig_matrix[sigdig_matrix<0] = 0   \n",
    "                            # Compute the standard deviation along axis 0 (across the 10 matrices)\n",
    "\n",
    "            std_matrixx = np.std(sigdig_matrix)\n",
    "            avr_matrixx = np.mean(sigdig_matrix)  \n",
    "            sigdig_matrix[sigdig_matrix>=5] = np.nan\n",
    "\n",
    "                            # Create a dictionary to store data for this subject, session, and acquisition\n",
    "            data = {\n",
    "                'repetition': i,\n",
    "                'session': j,\n",
    "                'acquisition': k,\n",
    "                'stacked_matrices': correlation_matrices,\n",
    "                'std_matrix':std_matrixx,\n",
    "                'avr_matrix':avr_matrixx,\n",
    "                'significant_digits_matrix': sigdig_matrix,\n",
    "                'std' : std,\n",
    "                'mean' : mean\n",
    "            }\n",
    "            \n",
    "            # Append the dictionary to the list\n",
    "            all_iteration_data.append(data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "y = pd.DataFrame(all_iteration_data)\n",
    "np.mean(y['avr_matrix'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with the defined columns\n",
    "Significantdigits_AnattableNoConf = pd.DataFrame(columns=Anatcolumns)\n",
    "# Assuming you know the size of your correlation matrices (100, 100)\n",
    "matrix_size = (100, 100)\n",
    "# Columns to exclude\n",
    "columns_to_exclude = [\n",
    "    'degree_centralities', 'eigenvector_centralities', \n",
    "    'betweenness_centralities', 'clustering_coefficients', \n",
    "    'avg_shortest_path_length', 'small_worldness'\n",
    "]\n",
    "\n",
    "# Create an empty list to store dictionaries for each iteration\n",
    "all_iteration_data = []\n",
    "\n",
    "# Loop over each subject\n",
    "for rep in range (1,11):\n",
    "\n",
    "    \n",
    "    # Filter the DataFrame for each subject and drop unnecessary columns once\n",
    "    filtered_df = Results_tableNoConf[Results_tableNoConf['repetition'] == f'rep-{rep}'].drop(columns=columns_to_exclude)\n",
    "    \n",
    "    # Loop over each unique session and acquisition combination\n",
    "    for session in np.unique(filtered_df['session']):\n",
    "        for acquisition in np.unique(filtered_df['acquisition']):\n",
    "            if acquisition not in ['acq-RLsplit1', 'acq-LRsplit1']:\n",
    "                try:\n",
    "                    # Filter for each specific repetition\n",
    "                    filtered_rows = filtered_df[\n",
    "                        (filtered_df['session'] == session) &\n",
    "                        (filtered_df['acquisition'] == acquisition)\n",
    "                    ]\n",
    "                        \n",
    "                    # If no rows match the filter, skip to the next acquisition\n",
    "                    if len(filtered_rows)< 2:\n",
    "                        # print(f\"No data for {subj}, session {session}, acquisition {acquisition}, rep {rep}. Skipping acquisition.\")\n",
    "                        continue\n",
    "                    \n",
    "                    stacked_matrices = np.zeros((len(filtered_rows), *matrix_size))\n",
    "                    initial_stacked_matrices = stacked_matrices.copy()\n",
    "                    for idx, subj in  enumerate(filtered_rows['subject'].unique()): \n",
    "                        # Assign the matrix to the corresponding slice of the 3D array\n",
    "                        corr_matrix = filtered_rows[filtered_rows['subject']==subj]['correlation_matrix'].values[0]\n",
    "                        stacked_matrices[idx] = corr_matrix  \n",
    "                    # if len(stacked_matrices) < 2:\n",
    "                    #     continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with {subj}, session {session}, acquisition {acquisition}, rep {rep}: {e}\")\n",
    "                    continue\n",
    "                # Check if stacked_matrices has changed\n",
    "                if np.array_equal(stacked_matrices, initial_stacked_matrices):\n",
    "                    # print(f\"No changes in stacked_matrices for {subj}, session {session}, acquisition {acquisition}.\")\n",
    "                    continue  # Skip this session/acquisition if no changes occurred\n",
    "                # print(stacked_matrices.shape)\n",
    "                # Compute the significant digits matrix\n",
    "                std = np.std(stacked_matrices, axis=0)\n",
    "                mean = np.mean(stacked_matrices, axis=0)\n",
    "                sigdig_matrix = sd.significant_digits(\n",
    "                    stacked_matrices, \n",
    "                    reference=mean, \n",
    "                    basis=10\n",
    "                )\n",
    "                sigdig_matrix[sigdig_matrix<0] = 0   \n",
    "                                # Compute the standard deviation along axis 0 (across the 10 matrices)\n",
    "                std_matrixx = np.std(sigdig_matrix)\n",
    "                avr_matrixx = np.mean(sigdig_matrix)  \n",
    "                sigdig_matrix[sigdig_matrix>=15] = np.nan\n",
    "          \n",
    "                # Create a dictionary to store data for this subject, session, and acquisition\n",
    "                data = {\n",
    "                    'repetition': rep,\n",
    "                    'session': session,\n",
    "                    'acquisition': acquisition,\n",
    "                    'stacked_matrices': stacked_matrices,\n",
    "                    'std_matrix':std_matrixx,\n",
    "                    'avr_matrix':avr_matrixx,\n",
    "                    'significant_digits_matrix': sigdig_matrix,\n",
    "                    'std' : std,\n",
    "                    'mean' : mean\n",
    "                }\n",
    "                \n",
    "                # Append the dictionary to the list\n",
    "                all_iteration_data.append(data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "Significantdigits_AnattableNoConf = pd.DataFrame(all_iteration_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Significantdigits_AnattableNoConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Anatfiltered_dfNoConf= Significantdigits_AnattableNoConf[(Significantdigits_AnattableNoConf['acquisition']=='acq-AP')| (Significantdigits_AnattableNoConf['acquisition']=='acq-RL')| (Significantdigits_AnattableNoConf['acquisition']=='acq-ep2d')]\n",
    "Anatfiltered_dfNoConf\n",
    "#select 10 Deciles\n",
    "Anatfiltered_dfNoConf = Anatfiltered_dfNoConf.dropna(subset=['avr_matrix'])\n",
    "print(len(Anatfiltered_dfNoConf))\n",
    "Anatsortedsigdig_onAvrg = Anatfiltered_dfNoConf.sort_values('avr_matrix')\n",
    "Anattenselectsigdig_onAvrgNoConf=Anatsortedsigdig_onAvrg.iloc[[0,6,12,18,24,30,36,42,48,59]]\n",
    "\n",
    "Anattenselectsigdig_onAvrgNoConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your DataFrame is named df and contains 'repetition', 'session', 'acquisition', 'std', 'mean', and 'significant_digits_matrix'\n",
    "rows = 2\n",
    "cols = 5\n",
    "df=Anattenselectsigdig_onAvrgNoConf\n",
    "\n",
    "# Create a subplot grid for all three types of heatmaps\n",
    "fig = make_subplots(\n",
    "    rows=rows * 3, cols=cols,  # Multiply rows by 3 to fit all heatmaps\n",
    "    subplot_titles=[\n",
    "        f\"Repetition{row['repetition']} - session{row['session']} - {row['acquisition']}\" \n",
    "        for _ in range(3) for _, row in df.iterrows()   # Repeat each title for 3 heatmaps\n",
    "    ],\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "# Loop through each unique combination in the DataFrame and add heatmaps\n",
    "for i, (index, dfrow) in enumerate(df.iterrows()):\n",
    "    # Calculate positions in the subplot grid for each type of heatmap\n",
    "    base_row = (i // cols) + 1  # Each subject occupies 3 rows\n",
    "    col_idx = i % cols + 1\n",
    "\n",
    "    # 1. Significant Digits Heatmap\n",
    "    sig_digits = np.flipud(dfrow['significant_digits_matrix'])\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=sig_digits,\n",
    "            colorscale='RdYlGn',\n",
    "            coloraxis=\"coloraxis1\",  # Shared color axis for significant digits\n",
    "        ),\n",
    "        row=base_row, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 2. Standard Deviation Heatmap\n",
    "    std_heatmap = np.flipud(np.log10(dfrow['std']))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=std_heatmap,\n",
    "            colorscale='RdYlGn_r',\n",
    "            coloraxis=\"coloraxis2\",  # Separate color axis for standard deviation\n",
    "        ),\n",
    "        row=base_row + 2, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 3. Mean Heatmap\n",
    "    mean_heatmap = np.flipud(dfrow['mean'])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=mean_heatmap,\n",
    "            coloraxis=\"coloraxis3\",  # Use a shared color axis,s\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1\n",
    "        ),\n",
    "        row=base_row +4 , col=col_idx\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Combined Heatmaps for withConfound: Significant Digits, Standard Deviation, and Mean\",\n",
    "    coloraxis1=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Significant Digits\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,  # Adjust length to fit the subplot height\n",
    "            x=1.18,   # Position to the right of the plot\n",
    "            y=1.0,    # Align vertically at the top\n",
    "            yanchor=\"top\"  # Anchor to the top\n",
    "        ),\n",
    "        colorscale='RdYlGn',\n",
    "        cmid=2,\n",
    "        cmin=-0.5,\n",
    "        cmax=3.5\n",
    "    ),\n",
    "    coloraxis2=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"log(Standard Deviation)\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Slightly further right than coloraxis1\n",
    "            y=0.65,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='RdYlGn_r',\n",
    "        cmid=-2.5,\n",
    "        cmin=-5,\n",
    "        cmax=-0.5\n",
    "    ),\n",
    "    coloraxis3=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Mean\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Further right for better spacing\n",
    "            y=0.28,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='brBG',\n",
    "        cmid=0,\n",
    "        cmin=-1,\n",
    "        cmax=1\n",
    "    ),\n",
    "    height=2100,  # Adjust height to fit all subplots\n",
    "    width=1800,  # Adjust width to fit all subplots\n",
    "    annotationdefaults=dict(font=dict(size=8))  # Font size for subplot titles\n",
    ")\n",
    "# Add section titles for anatomical and numerical data\n",
    "fig.add_annotation(\n",
    "    text=\"Significant digits across Anatomical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=1.018,  # Centered horizontally above the first two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=\"Standard deviation across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.68,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "fig.add_annotation(\n",
    "    text=\"Average across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.33,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "# Update subplot titles for better readability\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=12)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with the defined columns\n",
    "Significantdigits_NumtableNoConf = pd.DataFrame(columns=Numcolumns)\n",
    "# Assuming you know the size of your correlation matrices (100, 100)\n",
    "matrix_size = (100, 100)\n",
    "# Columns to exclude\n",
    "columns_to_exclude = [\n",
    "    'degree_centralities', 'eigenvector_centralities', \n",
    "    'betweenness_centralities', 'clustering_coefficients', \n",
    "    'avg_shortest_path_length', 'small_worldness'\n",
    "]\n",
    "\n",
    "# Create an empty list to store dictionaries for each subject\n",
    "all_subject_data = []\n",
    "\n",
    "# Get unique subjects from the Results_table\n",
    "subjects =np.unique(Results_tableNoConf['subject'])\n",
    "\n",
    "# Loop over each subject\n",
    "for subj in subjects:\n",
    "\n",
    "    \n",
    "    # Filter the DataFrame for each subject and drop unnecessary columns once\n",
    "    filtered_df = Results_tableNoConf[Results_tableNoConf['subject'] == subj].drop(columns=columns_to_exclude)\n",
    "    \n",
    "    # Loop over each unique session and acquisition combination\n",
    "    for session in np.unique(filtered_df['session']):\n",
    "        for acquisition in np.unique(filtered_df['acquisition']):\n",
    "            if acquisition not in ['acq-RLsplit1', 'acq-LRsplit1']:\n",
    "                    # Pre-allocate a 3D array to store 10 correlation matrices for this subject\n",
    "                stacked_matrices = np.zeros((10, *matrix_size))\n",
    "                initial_stacked_matrices = stacked_matrices.copy()\n",
    "                # Loop over each repetition to generate correlation matrices\n",
    "                for rep in range(1, 11):\n",
    "                    try:\n",
    "                        # Filter for each specific repetition\n",
    "                        filtered_rows = filtered_df[\n",
    "                            (filtered_df['session'] == session) &\n",
    "                            (filtered_df['acquisition'] == acquisition) &\n",
    "                            (filtered_df['repetition'] == f'rep-{rep}')\n",
    "                        ]\n",
    "                            \n",
    "                        # If no rows match the filter, skip to the next acquisition\n",
    "                        if filtered_rows.empty:\n",
    "                            # print(f\"No data for {subj}, session {session}, acquisition {acquisition}, rep {rep}. Skipping acquisition.\")\n",
    "                            continue\n",
    "\n",
    "                        # Assign the matrix to the corresponding slice of the 3D array\n",
    "                        corr_matrix = filtered_rows['correlation_matrix'].values[0]\n",
    "                        stacked_matrices[rep - 1] = corr_matrix  \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with {subj}, session {session}, acquisition {acquisition}, rep {rep}: {e}\")\n",
    "                        continue\n",
    "                # Check if stacked_matrices has changed\n",
    "                if np.array_equal(stacked_matrices, initial_stacked_matrices):\n",
    "                    # print(f\"No changes in stacked_matrices for {subj}, session {session}, acquisition {acquisition}.\")\n",
    "                    continue  # Skip this session/acquisition if no changes occurred\n",
    "\n",
    "                # Compute the significant digits matrix\n",
    "                std = np.std(stacked_matrices, axis=0)\n",
    "                mean = np.mean(stacked_matrices, axis=0)\n",
    "                sigdig_matrix = sd.significant_digits(\n",
    "                    stacked_matrices, \n",
    "                    reference=mean, \n",
    "                    basis=10\n",
    "                )\n",
    "                sigdig_matrix[sigdig_matrix<0] = 0   \n",
    "                                # Compute the standard deviation along axis 0 (across the 10 matrices)\n",
    "                std_matrixx = np.std(sigdig_matrix)\n",
    "                avr_matrixx = np.mean(sigdig_matrix)  \n",
    "                sigdig_matrix[sigdig_matrix>=15] = np.nan\n",
    "          \n",
    "                # Create a dictionary to store data for this subject, session, and acquisition\n",
    "                data = {\n",
    "                    'subject': subj,\n",
    "                    'session': session,\n",
    "                    'acquisition': acquisition,\n",
    "                    'stacked_matrices': stacked_matrices,\n",
    "                    'std_matrix':std_matrixx,\n",
    "                    'avr_matrix':avr_matrixx,\n",
    "                    'significant_digits_matrix': sigdig_matrix,\n",
    "                    'std' : std,\n",
    "                    'mean' : mean\n",
    "                }\n",
    "                \n",
    "                # Append the dictionary to the list\n",
    "                all_subject_data.append(data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "Significantdigits_NumtableNoConf = pd.DataFrame(all_subject_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create an empty DataFrame with the defined columns\n",
    "Significantdigits_NumtableWConf = pd.DataFrame(columns=Numcolumns)\n",
    "# Assuming you know the size of your correlation matrices (100, 100)\n",
    "matrix_size = (100, 100)\n",
    "# Columns to exclude\n",
    "columns_to_exclude = [\n",
    "    'degree_centralities', 'eigenvector_centralities', \n",
    "    'betweenness_centralities', 'clustering_coefficients', \n",
    "    'avg_shortest_path_length', 'small_worldness'\n",
    "]\n",
    "\n",
    "# Create an empty list to store dictionaries for each subject\n",
    "all_subject_data = []\n",
    "\n",
    "# Get unique subjects from the Results_table\n",
    "subjects =np.unique(Results_tableWConf['subject'])\n",
    "\n",
    "# Loop over each subject\n",
    "for subj in subjects:\n",
    "\n",
    "    \n",
    "    # Filter the DataFrame for each subject and drop unnecessary columns once\n",
    "    filtered_df = Results_tableWConf[Results_tableWConf['subject'] == subj].drop(columns=columns_to_exclude)\n",
    "    \n",
    "    # Loop over each unique session and acquisition combination\n",
    "    for session in np.unique(filtered_df['session']):\n",
    "        for acquisition in np.unique(filtered_df['acquisition']):\n",
    "            if acquisition not in ['acq-RLsplit1', 'acq-LRsplit1']:\n",
    "                    # Pre-allocate a 3D array to store 10 correlation matrices for this subject\n",
    "                stacked_matrices = np.zeros((10, *matrix_size))\n",
    "                initial_stacked_matrices = stacked_matrices.copy()\n",
    "                # Loop over each repetition to generate correlation matrices\n",
    "                for rep in range(1, 11):\n",
    "                    try:\n",
    "                        # Filter for each specific repetition\n",
    "                        filtered_rows = filtered_df[\n",
    "                            (filtered_df['session'] == session) &\n",
    "                            (filtered_df['acquisition'] == acquisition) &\n",
    "                            (filtered_df['repetition'] == f'rep-{rep}')\n",
    "                        ]\n",
    "                            \n",
    "                        # If no rows match the filter, skip to the next acquisition\n",
    "                        if filtered_rows.empty:\n",
    "                            # print(f\"No data for {subj}, session {session}, acquisition {acquisition}, rep {rep}. Skipping acquisition.\")\n",
    "                            continue\n",
    "\n",
    "                        # Assign the matrix to the corresponding slice of the 3D array\n",
    "                        corr_matrix = filtered_rows['correlation_matrix'].values[0]\n",
    "                        stacked_matrices[rep - 1] = corr_matrix  \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error with {subj}, session {session}, acquisition {acquisition}, rep {rep}: {e}\")\n",
    "                        continue\n",
    "                # Check if stacked_matrices has changed\n",
    "                if np.array_equal(stacked_matrices, initial_stacked_matrices):\n",
    "                    # print(f\"No changes in stacked_matrices for {subj}, session {session}, acquisition {acquisition}.\")\n",
    "                    continue  # Skip this session/acquisition if no changes occurred\n",
    "\n",
    "                # Compute the significant digits matrix\n",
    "                std = np.std(stacked_matrices, axis=0)\n",
    "                mean = np.mean(stacked_matrices, axis=0)\n",
    "                sigdig_matrix = sd.significant_digits(\n",
    "                    stacked_matrices, \n",
    "                    reference=mean, \n",
    "                    basis=10\n",
    "                )\n",
    "                sigdig_matrix[sigdig_matrix<0] = 0   \n",
    "                                # Compute the standard deviation along axis 0 (across the 10 matrices)\n",
    "                std_matrixx = np.std(sigdig_matrix)\n",
    "                avr_matrixx = np.mean(sigdig_matrix)  \n",
    "                sigdig_matrix[sigdig_matrix>=15] = np.nan\n",
    "          \n",
    "                # Create a dictionary to store data for this subject, session, and acquisition\n",
    "                data = {\n",
    "                    'subject': subj,\n",
    "                    'session': session,\n",
    "                    'acquisition': acquisition,\n",
    "                    'stacked_matrices': stacked_matrices,\n",
    "                    'std_matrix':std_matrixx,\n",
    "                    'avr_matrix':avr_matrixx,\n",
    "                    'significant_digits_matrix': sigdig_matrix,\n",
    "                    'std' : std,\n",
    "                    'mean' : mean\n",
    "                }\n",
    "                \n",
    "                # Append the dictionary to the list\n",
    "                all_subject_data.append(data)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "Significantdigits_NumtableWConf = pd.DataFrame(all_subject_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significantdigits_NumtableNoConf[Significantdigits_NumtableNoConf['subject']=='sub-140041']\n",
    "# NoConf has one row less than WConf\n",
    "len(Significantdigits_NumtableNoConf)\n",
    "print (np.mean(Significantdigits_NumtableWConf['avr_matrix'])) \n",
    "np.mean(Significantdigits_NumtableNoConf['avr_matrix'])\n",
    "\n",
    "# Identify missing items based on a unique identifier column (change 'ID' to the actual column name)\n",
    "# Identify missing items based on subject and acquisition together\n",
    "# missing_in_pkl2 = Significantdigits_NumtableNoConf[\n",
    "#     ~Significantdigits_NumtableNoConf[['subject', 'acquisition','session']].apply(tuple, axis=1).isin(\n",
    "#         Significantdigits_NumtableWConf[['subject', 'acquisition','session']].apply(tuple, axis=1)\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# missing_in_pkl1 = Significantdigits_NumtableWConf[\n",
    "#     ~Significantdigits_NumtableWConf[['subject', 'acquisition','session']].apply(tuple, axis=1).isin(\n",
    "#         Significantdigits_NumtableNoConf[['subject', 'acquisition','session']].apply(tuple, axis=1)\n",
    "#     )\n",
    "# ]\n",
    "\n",
    "# print(missing_in_pkl1['subject'])\n",
    "# print(missing_in_pkl2['subject'])\n",
    "# Significantdigits_NumtableWConf[Significantdigits_NumtableWConf['subject']=='sub-140041']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tableW = Significantdigits_NumtableWConf[\n",
    "    (Significantdigits_NumtableWConf['acquisition'] == 'acq-RL') |\n",
    "    (Significantdigits_NumtableWConf['acquisition'] == 'acq-ep2d')|\n",
    "    (Significantdigits_NumtableWConf['acquisition'] == 'acq-PA')\n",
    "    \n",
    "]\n",
    "#select 10 Deciles\n",
    "sortedsigdig_onAvrg = filtered_tableW.sort_values('avr_matrix')\n",
    "tenselectsigdig_onAvrgWConf=sortedsigdig_onAvrg.iloc[[0,20,40,60,80,100,120,140,158,177]]\n",
    "tenselectsigdig_onAvrgWConf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the decile subjects from Wbatch2 list in No confound batch2\n",
    "matches = []  # List to store matched rows\n",
    "\n",
    "# Loop through rows of tenselectedsub\n",
    "for _, row in tenselectsigdig_onAvrgWConf.iterrows():\n",
    "    subj = row['subject']\n",
    "    session = row['session']\n",
    "    acq = row['acquisition']\n",
    "\n",
    "    # Filter rows from Significantdigits_Numtable matching the current row\n",
    "    match = Significantdigits_NumtableNoConf[\n",
    "        (Significantdigits_NumtableNoConf['subject'] == subj) &\n",
    "        (Significantdigits_NumtableNoConf['session'] == str(session)) &\n",
    "        (Significantdigits_NumtableNoConf['acquisition'] == acq)\n",
    "    ]\n",
    "    matches.append(match)\n",
    "\n",
    "# Concatenate all matcnhed rows into a single DataFrame\n",
    "tenselectsigdig_onAvrgNoConf = pd.concat(matches, ignore_index=True)\n",
    "tenselectsigdig_onAvrgNoConf\n",
    "# tenselectsigdig_onAvrg\n",
    "# match=Significantdigits_Numtable[Significantdigits_Numtable['subject']=='' & Significantdigits_Numtable['session']=='' & Significantdigits_Numtable['acquisition']=='']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your DataFrame is named df and contains 'repetition', 'session', 'acquisition', 'std', 'mean', and 'significant_digits_matrix'\n",
    "rows = 2\n",
    "cols = 5\n",
    "df=tenselectsigdig_onAvrgWConf\n",
    "\n",
    "# Create a subplot grid for all three types of heatmaps\n",
    "fig = make_subplots(\n",
    "    rows=rows * 3, cols=cols,  # Multiply rows by 3 to fit all heatmaps\n",
    "    subplot_titles=[\n",
    "        f\"{row['subject']} - {row['session']} - {row['acquisition']}\" \n",
    "        for _ in range(3) for _, row in df.iterrows()   # Repeat each title for 3 heatmaps\n",
    "    ],\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "# Loop through each unique combination in the DataFrame and add heatmaps\n",
    "for i, (index, dfrow) in enumerate(df.iterrows()):\n",
    "    # Calculate positions in the subplot grid for each type of heatmap\n",
    "    base_row = (i // cols) + 1  # Each subject occupies 3 rows\n",
    "    col_idx = i % cols + 1\n",
    "\n",
    "    # 1. Significant Digits Heatmap\n",
    "    sig_digits = np.flipud(dfrow['significant_digits_matrix'])\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=sig_digits,\n",
    "            colorscale='RdYlGn',\n",
    "            coloraxis=\"coloraxis1\",  # Shared color axis for significant digits\n",
    "        ),\n",
    "        row=base_row, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 2. Standard Deviation Heatmap\n",
    "    std_heatmap = np.flipud(np.log10(dfrow['std']))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=std_heatmap,\n",
    "            colorscale='RdYlGn_r',\n",
    "            coloraxis=\"coloraxis2\",  # Separate color axis for standard deviation\n",
    "        ),\n",
    "        row=base_row + 2, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 3. Mean Heatmap\n",
    "    mean_heatmap = np.flipud(dfrow['mean'])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=mean_heatmap,\n",
    "            coloraxis=\"coloraxis3\",  # Use a shared color axis,s\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1\n",
    "        ),\n",
    "        row=base_row +4 , col=col_idx\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Combined Heatmaps for withConfound: Significant Digits, Standard Deviation, and Mean\",\n",
    "    coloraxis1=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Significant Digits\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,  # Adjust length to fit the subplot height\n",
    "            x=1.18,   # Position to the right of the plot\n",
    "            y=1.0,    # Align vertically at the top\n",
    "            yanchor=\"top\"  # Anchor to the top\n",
    "        ),\n",
    "        colorscale='RdYlGn',\n",
    "        cmid=2,\n",
    "        cmin=-0.5,\n",
    "        cmax=3.5\n",
    "    ),\n",
    "    coloraxis2=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"log(Standard Deviation)\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Slightly further right than coloraxis1\n",
    "            y=0.65,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='RdYlGn_r',\n",
    "        cmid=-2.5,\n",
    "        cmin=-5,\n",
    "        cmax=-0.5\n",
    "    ),\n",
    "    coloraxis3=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Mean\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Further right for better spacing\n",
    "            y=0.28,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='brBG',\n",
    "        cmid=0,\n",
    "        cmin=-1,\n",
    "        cmax=1\n",
    "    ),\n",
    "    height=2100,  # Adjust height to fit all subplots\n",
    "    width=1800,  # Adjust width to fit all subplots\n",
    "    annotationdefaults=dict(font=dict(size=8))  # Font size for subplot titles\n",
    ")\n",
    "# Add section titles for anatomical and numerical data\n",
    "fig.add_annotation(\n",
    "    text=\"Significant digits across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=1.018,  # Centered horizontally above the first two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=\"Standard deviation across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.68,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "fig.add_annotation(\n",
    "    text=\"Average across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.33,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "# Update subplot titles for better readability\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=12)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your DataFrame is named df and contains 'repetition', 'session', 'acquisition', 'std', 'mean', and 'significant_digits_matrix'\n",
    "rows = 2\n",
    "cols = 5\n",
    "df=tenselectsigdig_onAvrgNoConf\n",
    "\n",
    "# Create a subplot grid for all three types of heatmaps\n",
    "fig = make_subplots(\n",
    "    rows=rows * 3, cols=cols,  # Multiply rows by 3 to fit all heatmaps\n",
    "    subplot_titles=[\n",
    "        f\"{row['subject']} - {row['session']} - {row['acquisition']}\" \n",
    "        for _ in range(3) for _, row in df.iterrows()   # Repeat each title for 3 heatmaps\n",
    "    ],\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "# Loop through each unique combination in the DataFrame and add heatmaps\n",
    "for i, (index, dfrow) in enumerate(df.iterrows()):\n",
    "    # Calculate positions in the subplot grid for each type of heatmap\n",
    "    base_row = (i // cols) + 1  # Each subject occupies 3 rows\n",
    "    col_idx = i % cols + 1\n",
    "\n",
    "    # 1. Significant Digits Heatmap\n",
    "    sig_digits = np.flipud(dfrow['significant_digits_matrix'])\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=sig_digits,\n",
    "            colorscale='RdYlGn',\n",
    "            coloraxis=\"coloraxis1\",  # Shared color axis for significant digits\n",
    "        ),\n",
    "        row=base_row, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 2. Standard Deviation Heatmap\n",
    "    std_heatmap = np.flipud(np.log10(dfrow['std']))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=std_heatmap,\n",
    "            colorscale='RdYlGn_r',\n",
    "            coloraxis=\"coloraxis2\",  # Separate color axis for standard deviation\n",
    "        ),\n",
    "        row=base_row + 2, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 3. Mean Heatmap\n",
    "    mean_heatmap = np.flipud(dfrow['mean'])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=mean_heatmap,\n",
    "            coloraxis=\"coloraxis3\",  # Use a shared color axis,s\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1\n",
    "        ),\n",
    "        row=base_row +4 , col=col_idx\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Combined Heatmaps for No Confound: Significant Digits, Standard Deviation, and Mean with same subjects for With Confounds\",\n",
    "    coloraxis1=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Significant Digits\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,  # Adjust length to fit the subplot height\n",
    "            x=1.18,   # Position to the right of the plot\n",
    "            y=1.0,    # Align vertically at the top\n",
    "            yanchor=\"top\"  # Anchor to the top\n",
    "        ),\n",
    "        colorscale='RdYlGn',\n",
    "        cmid=2,\n",
    "        cmin=-0.5,\n",
    "        cmax=3.5\n",
    "    ),\n",
    "    coloraxis2=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"log(Standard Deviation)\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Slightly further right than coloraxis1\n",
    "            y=0.65,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='RdYlGn_r',\n",
    "        cmid=-2.5,\n",
    "        cmin=-5,\n",
    "        cmax=-0.5\n",
    "    ),\n",
    "    coloraxis3=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Mean\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Further right for better spacing\n",
    "            y=0.28,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='brBG',\n",
    "        cmid=0,\n",
    "        cmin=-1,\n",
    "        cmax=1\n",
    "    ),\n",
    "    height=2100,  # Adjust height to fit all subplots\n",
    "    width=1800,  # Adjust width to fit all subplots\n",
    "    annotationdefaults=dict(font=dict(size=8))  # Font size for subplot titles\n",
    ")\n",
    "# Add section titles for anatomical and numerical data\n",
    "fig.add_annotation(\n",
    "    text=\"Significant digits across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=1.018,  # Centered horizontally above the first two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=\"Standard deviation across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.68,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "fig.add_annotation(\n",
    "    text=\"Average across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.33,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "# Update subplot titles for better readability\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=12)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tableNo = Significantdigits_NumtableNoConf[\n",
    "    (Significantdigits_NumtableNoConf['acquisition'] == 'acq-RL') |\n",
    "    (Significantdigits_NumtableNoConf['acquisition'] == 'acq-ep2d')|\n",
    "    (Significantdigits_NumtableNoConf['acquisition'] == 'acq-PA')\n",
    "    \n",
    "]\n",
    "print(len(filtered_tableNo))\n",
    "#select 10 Deciles\n",
    "sortedsigdig_onAvrgNoConff = filtered_tableNo.sort_values('avr_matrix')\n",
    "tenselectsigdig_onAvrgNoConff=sortedsigdig_onAvrgNoConff.iloc[[0,20,40,60,80,100,120,140,158,177]]\n",
    "tenselectsigdig_onAvrgNoConff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your DataFrame is named df and contains 'repetition', 'session', 'acquisition', 'std', 'mean', and 'significant_digits_matrix'\n",
    "rows = 2\n",
    "cols = 5\n",
    "df=tenselectsigdig_onAvrgNoConff\n",
    "\n",
    "# Create a subplot grid for all three types of heatmaps\n",
    "fig = make_subplots(\n",
    "    rows=rows * 3, cols=cols,  # Multiply rows by 3 to fit all heatmaps\n",
    "    subplot_titles=[\n",
    "        f\"{row['subject']} - {row['session']} - {row['acquisition']}\" \n",
    "        for _ in range(3) for _, row in df.iterrows()   # Repeat each title for 3 heatmaps\n",
    "    ],\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "# Loop through each unique combination in the DataFrame and add heatmaps\n",
    "for i, (index, dfrow) in enumerate(df.iterrows()):\n",
    "    # Calculate positions in the subplot grid for each type of heatmap\n",
    "    base_row = (i // cols) + 1  # Each subject occupies 3 rows\n",
    "    col_idx = i % cols + 1\n",
    "\n",
    "    # 1. Significant Digits Heatmap\n",
    "    sig_digits = np.flipud(dfrow['significant_digits_matrix'])\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=sig_digits,\n",
    "            colorscale='RdYlGn',\n",
    "            coloraxis=\"coloraxis1\",  # Shared color axis for significant digits\n",
    "        ),\n",
    "        row=base_row, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 2. Standard Deviation Heatmap\n",
    "    std_heatmap = np.flipud(np.log10(dfrow['std']))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=std_heatmap,\n",
    "            colorscale='RdYlGn_r',\n",
    "            coloraxis=\"coloraxis2\",  # Separate color axis for standard deviation\n",
    "        ),\n",
    "        row=base_row + 2, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 3. Mean Heatmap\n",
    "    mean_heatmap = np.flipud(dfrow['mean'])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=mean_heatmap,\n",
    "            coloraxis=\"coloraxis3\",  # Use a shared color axis,s\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1\n",
    "        ),\n",
    "        row=base_row +4 , col=col_idx\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Combined Heatmaps for No Confound: Significant Digits, Standard Deviation, and Mean\",\n",
    "    coloraxis1=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Significant Digits\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,  # Adjust length to fit the subplot height\n",
    "            x=1.18,   # Position to the right of the plot\n",
    "            y=1.0,    # Align vertically at the top\n",
    "            yanchor=\"top\"  # Anchor to the top\n",
    "        ),\n",
    "        colorscale='RdYlGn',\n",
    "        cmid=2,\n",
    "        cmin=-0.5,\n",
    "        cmax=3.5\n",
    "    ),\n",
    "    coloraxis2=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"log(Standard Deviation)\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Slightly further right than coloraxis1\n",
    "            y=0.65,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='RdYlGn_r',\n",
    "        cmid=-2.5,\n",
    "        cmin=-5,\n",
    "        cmax=-0.5\n",
    "    ),\n",
    "    coloraxis3=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Mean\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Further right for better spacing\n",
    "            y=0.28,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='brBG',\n",
    "        cmid=0,\n",
    "        cmin=-1,\n",
    "        cmax=1\n",
    "    ),\n",
    "    height=2100,  # Adjust height to fit all subplots\n",
    "    width=1800,  # Adjust width to fit all subplots\n",
    "    annotationdefaults=dict(font=dict(size=8))  # Font size for subplot titles\n",
    ")\n",
    "# Add section titles for anatomical and numerical data\n",
    "fig.add_annotation(\n",
    "    text=\"Significant digits across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=1.018,  # Centered horizontally above the first two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=\"Standard deviation across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.68,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "fig.add_annotation(\n",
    "    text=\"Average across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.33,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "# Update subplot titles for better readability\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=12)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the decile subjects from Wbatch2 list in No confound batch2\n",
    "matchess = []  # List to store matched rows\n",
    "\n",
    "# Loop through rows of tenselectedsub\n",
    "for _, row in tenselectsigdig_onAvrgNoConff.iterrows():\n",
    "    subj = row['subject']\n",
    "    session = row['session']\n",
    "    acq = row['acquisition']\n",
    "\n",
    "    # Filter rows from Significantdigits_Numtable matching the current row\n",
    "    match = Significantdigits_NumtableWConf[\n",
    "        (Significantdigits_NumtableWConf['subject'] == subj) &\n",
    "        (Significantdigits_NumtableWConf['session'] == str(session)) &\n",
    "        (Significantdigits_NumtableWConf['acquisition'] == acq)\n",
    "    ]\n",
    "    matchess.append(match)\n",
    "\n",
    "# Concatenate all matcnhed rows into a single DataFrame\n",
    "tenselectsigdig_onAvrgWConfs = pd.concat(matchess, ignore_index=True)\n",
    "tenselectsigdig_onAvrgWConfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Assuming your DataFrame is named df and contains 'repetition', 'session', 'acquisition', 'std', 'mean', and 'significant_digits_matrix'\n",
    "rows = 2\n",
    "cols = 5\n",
    "df=tenselectsigdig_onAvrgWConfs\n",
    "\n",
    "# Create a subplot grid for all three types of heatmaps\n",
    "fig = make_subplots(\n",
    "    rows=rows * 3, cols=cols,  # Multiply rows by 3 to fit all heatmaps\n",
    "    subplot_titles=[\n",
    "        f\"{row['subject']} - {row['session']} - {row['acquisition']}\" \n",
    "        for _ in range(3) for _, row in df.iterrows()   # Repeat each title for 3 heatmaps\n",
    "    ],\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "# Loop through each unique combination in the DataFrame and add heatmaps\n",
    "for i, (index, dfrow) in enumerate(df.iterrows()):\n",
    "    # Calculate positions in the subplot grid for each type of heatmap\n",
    "    base_row = (i // cols) + 1  # Each subject occupies 3 rows\n",
    "    col_idx = i % cols + 1\n",
    "\n",
    "    # 1. Significant Digits Heatmap\n",
    "    sig_digits = np.flipud(dfrow['significant_digits_matrix'])\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=sig_digits,\n",
    "            colorscale='RdYlGn',\n",
    "            coloraxis=\"coloraxis1\",  # Shared color axis for significant digits\n",
    "        ),\n",
    "        row=base_row, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 2. Standard Deviation Heatmap\n",
    "    std_heatmap = np.flipud(np.log10(dfrow['std']))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=std_heatmap,\n",
    "            colorscale='RdYlGn_r',\n",
    "            coloraxis=\"coloraxis2\",  # Separate color axis for standard deviation\n",
    "        ),\n",
    "        row=base_row + 2, col=col_idx\n",
    "    )\n",
    "\n",
    "    # 3. Mean Heatmap\n",
    "    mean_heatmap = np.flipud(dfrow['mean'])\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=mean_heatmap,\n",
    "            coloraxis=\"coloraxis3\",  # Use a shared color axis,s\n",
    "            zmid=0,\n",
    "            zmin=-1,\n",
    "            zmax=1\n",
    "        ),\n",
    "        row=base_row +4 , col=col_idx\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Combined Heatmaps for with Confound: Significant Digits, Standard Deviation, and Mean with no confond match subjects\",\n",
    "    coloraxis1=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Significant Digits\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,  # Adjust length to fit the subplot height\n",
    "            x=1.18,   # Position to the right of the plot\n",
    "            y=1.0,    # Align vertically at the top\n",
    "            yanchor=\"top\"  # Anchor to the top\n",
    "        ),\n",
    "        colorscale='RdYlGn',\n",
    "        cmid=2,\n",
    "        cmin=-0.5,\n",
    "        cmax=3.5\n",
    "    ),\n",
    "    coloraxis2=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"log(Standard Deviation)\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Slightly further right than coloraxis1\n",
    "            y=0.65,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='RdYlGn_r',\n",
    "        cmid=-2.5,\n",
    "        cmin=-5,\n",
    "        cmax=-0.5\n",
    "    ),\n",
    "    coloraxis3=dict(\n",
    "        colorbar=dict(\n",
    "            title=\"Mean\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Further right for better spacing\n",
    "            y=0.28,\n",
    "            yanchor=\"top\"\n",
    "        ),\n",
    "        colorscale='brBG',\n",
    "        cmid=0,\n",
    "        cmin=-1,\n",
    "        cmax=1\n",
    "    ),\n",
    "    height=2100,  # Adjust height to fit all subplots\n",
    "    width=1800,  # Adjust width to fit all subplots\n",
    "    annotationdefaults=dict(font=dict(size=8))  # Font size for subplot titles\n",
    ")\n",
    "# Add section titles for anatomical and numerical data\n",
    "fig.add_annotation(\n",
    "    text=\"Significant digits across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=1.018,  # Centered horizontally above the first two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=\"Standard deviation across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.68,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "fig.add_annotation(\n",
    "    text=\"Average across Numerical Variability\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.33,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "# Update subplot titles for better readability\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=12)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KS-test for NoConf and WConf with multiple comparison correction\n",
    "def ksTestWithCorrection(wconf, noconf, alt, correction='bonferroni'):\n",
    "    import numpy as np\n",
    "    from scipy.stats import ks_2samp\n",
    "    from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "    # Significant digit matrices for conf and non-conf groups\n",
    "    conf_matrix = wconf \n",
    "    non_conf_matrix = noconf \n",
    "\n",
    "    # Initialize arrays to store KS-statistics and p-values\n",
    "    n_nodes0= conf_matrix.shape[1] \n",
    "    n_nodes1 = conf_matrix.shape[2]    \n",
    "    ks_connectome = np.zeros((n_nodes0, n_nodes1))\n",
    "    pval_connectome = np.zeros((n_nodes0, n_nodes1))\n",
    "\n",
    "    # Iterate over the upper triangle (excluding the diagonal)\n",
    "    for i in range(n_nodes0):\n",
    "        for j in range(i + 1, n_nodes1):  # Upper triangle only\n",
    "            # Extract values across subjects for the edge (i, j)\n",
    "            conf_values = conf_matrix[:, i, j]\n",
    "            non_conf_values = non_conf_matrix[:, i, j]\n",
    "            # Perform KS-test\n",
    "            ks_stat, p_value = ks_2samp(conf_values,non_conf_values, alternative=alt)\n",
    "            \n",
    "            # Store results\n",
    "            ks_connectome[i, j] = ks_stat\n",
    "            pval_connectome[i, j] = p_value\n",
    "    if  n_nodes1!=n_nodes0: \n",
    "        for i in range(100):\n",
    "            # Extract values across subjects for the edge (i, j)\n",
    "            conf_values = conf_matrix[:, 0, i]\n",
    "            non_conf_values = non_conf_matrix[:, 0, i]\n",
    "            # Perform KS-test\n",
    "            ks_stat, p_value = ks_2samp(conf_values,non_conf_values, alternative=alt)\n",
    "            \n",
    "            # Store results\n",
    "            ks_connectome[0,i] = ks_stat\n",
    "            pval_connectome[0,i] = p_value\n",
    "\n",
    "    if n_nodes1==n_nodes0:\n",
    "        # Mirror the upper triangle to the lower triangle for symmetry\n",
    "        ks_connectome = ks_connectome + ks_connectome.T\n",
    "        pval_connectome = pval_connectome + pval_connectome.T\n",
    "         # Flatten the p-value matrix and apply multiple comparison correction\n",
    "        pval_flat = pval_connectome[np.triu_indices(n_nodes0, k=1)]  # Upper triangle only\n",
    "        # print(pval_connectome.shape,pval_flat.shape)\n",
    "        if correction == 'Bonferroni':\n",
    "            # Bonferroni correction\n",
    "            res, pval_corrected, _, _ = multipletests(pval_flat, method='bonferroni')\n",
    "        elif correction == 'FDR':\n",
    "            # Benjamini-Hochberg FDR correctionres\n",
    "            res, pval_corrected, _, _ = multipletests(pval_flat, method='fdr_bh')\n",
    "        else:\n",
    "            raise ValueError(\"Invalid correction method. Use 'Bonferroni' or 'FDR'.\")\n",
    "    else:\n",
    "        pval_flat = pval_connectome.flatten().tolist()\n",
    "        # print(pval_flat)\n",
    "        if correction == 'Bonferroni':\n",
    "            # Bonferroni correction\n",
    "            res, pval_corrected, _, _ = multipletests(pval_flat, method='bonferroni')\n",
    "            pval_corrected_matrix=pval_corrected\n",
    "            res_flt= res.astype(np.float32)\n",
    "            # print(res_flt)\n",
    "        elif correction == 'FDR':\n",
    "            # Benjamini-Hochberg FDR correctionres\n",
    "            res, pval_corrected, _, _ = multipletests(pval_flat, method='fdr_bh')\n",
    "            pval_corrected_matrix=pval_corrected\n",
    "            res_flt= res.astype(np.float32)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid correction method. Use 'Bonferroni' or 'FDR'.\")\n",
    "    if n_nodes1==n_nodes0:\n",
    "        # Map the corrected p-values back to the upper triangle of the matrix\n",
    "        pval_corrected_matrix = np.zeros_like(pval_connectome)\n",
    "        res_flt = np.zeros_like(ks_connectome)\n",
    "        res_flt[np.triu_indices(n_nodes0, k=1)] = res.astype(np.float32)\n",
    "        res_flt = res_flt + res_flt.T\n",
    "        pval_corrected_matrix[np.triu_indices(n_nodes0, k=1)] = pval_corrected  #significant_mask = (pval_corrected < 0.05)\n",
    "        pval_corrected_matrix = pval_corrected_matrix + pval_corrected_matrix.T\n",
    "        # print(res_flt.shape,pval_corrected_matrix.shape)\n",
    "        # print(res.shape,res_flt.shape)\n",
    "    return ks_connectome, pval_connectome, pval_corrected_matrix,res_flt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_sub(d1,d2,mode):\n",
    "    import numpy as np\n",
    "\n",
    "    # Assuming you want a 1D array for storing the means of each subject\n",
    "\n",
    "    # Initialize mat with the correct shape (number of subjects, rows, columns)\n",
    "    d1n_subjects = len(d1)  # Number of subjects\n",
    "    d2n_subjects = len(d2) \n",
    "    n_rows, n_columns = d1.iloc[0][mode].shape[0],d1.iloc[0][mode].shape[1] # Assuming each significant_digits_matrix is 100x100\n",
    "    subj_WconfMat = np.zeros((d1n_subjects, n_rows, n_columns)) # Initialize mat with the number of subjects\n",
    "    subj_NoconfMat = np.zeros((d2n_subjects, n_rows, n_columns)) # Initialize mat with the number of subjects\n",
    "    \n",
    "    # Iterate through each row of the DataFrame\n",
    "    count=0\n",
    "    for idx, row in d1.iterrows():\n",
    "        # Extract the significant_digits_matrix for the current subject\n",
    "        mat1 = row[mode]\n",
    "        # Assign the 2D matrix to the correct position in mat\n",
    "        subj_WconfMat[count] = np.array(mat1)  \n",
    "        count +=1\n",
    "    # Iterate through each row of the DataFrame\n",
    "    count =0\n",
    "    for idx, row in d2.iterrows():\n",
    "        # Extract the significant_digits_matrix for the current subject\n",
    "        mat2= row[mode]\n",
    "        # Assign the 2D matrix to the correct position in mat\n",
    "        subj_NoconfMat[count] = np.array(mat2)  \n",
    "        count +=1\n",
    "    return subj_WconfMat, subj_NoconfMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_table = filtered_tableNo [ (filtered_tableNo['subject'] != 'sub-140041') & (filtered_tableNo['subject'] != 'sub-100898')\n",
    "\n",
    " ]  #exclude 100898 as it has 18 iteration and 140041 as in noconfound analysis has only one session but in with confoun dtwo session\n",
    "filtered_tableWconf = filtered_tableW[(filtered_tableW['subject'] != 'sub-140041') & (filtered_tableW['subject'] != 'sub-100898')\n",
    "    #                                                 \n",
    "  ]\n",
    "\n",
    "missing_subj = filtered_tableWconf[~filtered_tableWconf['subject'].isin(filtered_table['subject'])]\n",
    "print(\"missing in noconf\",missing_subj['subject'])\n",
    "\n",
    "missing_subj = filtered_table[~filtered_table['subject'].isin(filtered_tableWconf['subject'])]\n",
    "print(\"missing in Wconf\",missing_subj['subject'])    \n",
    "\n",
    "print(len(filtered_tableWconf))\n",
    "print(len(filtered_table))\n",
    "print(filtered_table.shape)\n",
    "\n",
    "# print(len(filtered_table),len(filtered_tableWconf))\n",
    "wconfsig,noconfsig=stack_sub(filtered_tableWconf,filtered_table,'significant_digits_matrix')#mode can be : std , mean , significant_digits_matrix\n",
    "wconfstd,noconfstd=stack_sub(filtered_tableWconf,filtered_table,'std')#mode can be : std , mean , significant_digits_matrix\n",
    "wconfavr,noconfavr=stack_sub(filtered_tableWconf,filtered_table,'mean')#mode can be : std , mean , significant_digits_matrix\n",
    "print(wconfsig.shape)\n",
    "kssigtwoside,pvalsigtwoside,pvalcorsigtwoside,ressig_twoside=ksTestWithCorrection(wconfsig,noconfsig,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "kssiggreater,pvalsiggreater,pvalcorsiggreater,ressig_greater=ksTestWithCorrection(wconfsig,noconfsig,'greater','Bonferroni') #FDR,Bonferroni\n",
    "kssigless,pvalsigless,pvalcorsigless,ressig_less=ksTestWithCorrection(wconfsig,noconfsig,'less','Bonferroni') #FDR,Bonferroni\n",
    "ksstdtwoside,pvalstdtwoside,pvalcorstdtwoside,resstd_twoside=ksTestWithCorrection(wconfstd,noconfstd,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "ksstdless,pvalstdless,pvalcorstdless,resstd_less=ksTestWithCorrection(wconfstd,noconfstd,'less','Bonferroni') #FDR,Bonferroni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wconfsig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wconfsig_avr=np.mean(wconfsig,axis=0)\n",
    "noconfsig_avr=np.mean(noconfsig,axis=0)\n",
    "wconfstd_avr=np.mean(wconfstd,axis=0)\n",
    "noconfstd_avr=np.mean(noconfstd,axis=0)\n",
    "wconfavr_avr=np.mean(wconfavr,axis=0)\n",
    "noconfavr_avr=np.mean(noconfavr,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import plotly.express as px\n",
    "# batch1_Noconfsigtable=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/SigNumtableNoConf_batch2.pkl')\n",
    "# batch1_Wconfsigtable=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/SigNumtableWConf_batch2.pkl')\n",
    "\n",
    "# # Store matched rows properly\n",
    "# matches_batch1 = []  \n",
    "\n",
    "# # Loop through rows of tenselectedsub\n",
    "# for _, row in batch1_Noconfsigtable.iterrows():\n",
    "#     subj = row['subject']\n",
    "    \n",
    "#     # Filter rows from filtered_tableNo matching the current row's subject\n",
    "#     matched_rows = filtered_tableNo[filtered_tableNo['subject'] == subj]\n",
    "    \n",
    "#     matches_batch1.append(matched_rows)\n",
    "\n",
    "# # Concatenate all matched rows\n",
    "# Noconf_batch1 = pd.concat(matches_batch1, ignore_index=True)\n",
    "\n",
    "# # Repeat for filtered_tableW\n",
    "# matche_batch1 = []  \n",
    "\n",
    "# for _, row in batch1_Wconfsigtable.iterrows():\n",
    "#     subj = row['subject']\n",
    "    \n",
    "#     matched_rows = filtered_tableW[filtered_tableW['subject'] == subj]\n",
    "    \n",
    "#     matche_batch1.append(matched_rows)\n",
    "\n",
    "# Wconf_batch1 = pd.concat(matche_batch1, ignore_index=True)\n",
    "\n",
    "# # Exclude specific subjects\n",
    "# filtered_table = Noconf_batch1[\n",
    "#     (Noconf_batch1['subject'] != 'sub-140041') & \n",
    "#     (Noconf_batch1['subject'] != 'sub-100898')\n",
    "# ]\n",
    "\n",
    "# filtered_tableWconf = Wconf_batch1[\n",
    "#     (Wconf_batch1['subject'] != 'sub-140041') & \n",
    "#     (Wconf_batch1['subject'] != 'sub-100898')\n",
    "# ]\n",
    "\n",
    "# # Compute statistics\n",
    "# wconfsig, noconfsig = stack_sub(filtered_tableWconf, filtered_table, 'significant_digits_matrix')\n",
    "\n",
    "\n",
    "# # Compute mean difference\n",
    "# diff = noconfsig - wconfsig\n",
    "# average_diff =np.mean(diff,axis=0)\n",
    "# # Remove NaN or -inf values before computing mean\n",
    "# cleaned_diff = average_diff[np.isfinite(average_diff)]\n",
    "# print(np.mean(cleaned_diff))# Plot heatmap\n",
    "\n",
    "# # Plot heatmap\n",
    "# fig = px.imshow(\n",
    "#     average_diff, color_continuous_scale='plasma',\n",
    "#     title='Average Difference in Significant Digits (Without - With Confounds)'\n",
    "# )\n",
    "# fig.update_layout(coloraxis_colorbar_title='Difference')\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "# Compute the difference\n",
    "diff= noconfsig - wconfsig\n",
    "average_diff =np.mean(diff,axis=0)\n",
    "# Remove NaN or -inf values before computing mean\n",
    "cleaned_diff = average_diff[np.isfinite(average_diff)]\n",
    "print(np.mean(cleaned_diff))# Plot heatmap\n",
    "fig = px.imshow(average_diff, color_continuous_scale='plasma',\n",
    "                title='Average Difference in Significant Digits (Without - with Confounds)')\n",
    "fig.update_layout(coloraxis_colorbar_title='Difference')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# Avoid log(0) errors\n",
    "log_wconfstd_avr = np.log10(wconfstd_avr)  \n",
    "log_noconfstd_avr = np.log10(noconfstd_avr)\n",
    "\n",
    "avr1 =round(np.nanmean(wconfsig_avr[~np.isinf(wconfsig_avr)]),3)\n",
    "avr2 = round(np.nanmean(wconfsig_avr[~np.isinf(noconfsig_avr)]),3)\n",
    "avr3 = round(np.nanmean(wconfsig_avr[~np.isinf(log_wconfstd_avr)]),3)\n",
    "avr4 = round(np.nanmean(wconfsig_avr[~np.isinf(log_noconfstd_avr)]),3)\n",
    "avr5 = round(np.nanmean(wconfsig_avr[~np.isinf(wconfavr_avr)]),3)\n",
    "avr6 = round(np.nanmean(wconfsig_avr[~np.isinf(noconfavr_avr)]),3)\n",
    "# List of matrices, titles, shared color axes, and color scales\n",
    "matrices = [\n",
    "    (wconfsig_avr, f\"With Confounds Significant Digits, with avrage values of= {avr1}\", \"coloraxis1\", \"RdYlGn\"),\n",
    "    (noconfsig_avr, f\"No Confounds Significant Digits, with avrage values of= {avr2}\", \"coloraxis1\", \"RdYlGn\"),\n",
    "    (log_wconfstd_avr, f\"With Confounds log(Standard Deviation), with avrage values of= {avr3}\", \"coloraxis2\", \"RdYlGn_r\"),\n",
    "    (log_noconfstd_avr, f\"No Confounds log(Standard Deviation), with avrage values of= {avr4}\", \"coloraxis2\", \"RdYlGn_r\"),\n",
    "    (wconfavr_avr, f\"With Confounds Average, with avrage values of= {avr5}\", \"coloraxis3\", \"brBG\"),\n",
    "    (noconfavr_avr, f\"No Confounds Average, with avrage values of= {avr6}\", \"coloraxis3\", \"brBG\")\n",
    "]\n",
    "\n",
    "# Create a subplot grid with 3 rows and 2 columns\n",
    "fig = make_subplots(\n",
    "    rows=3, cols=2,\n",
    "    subplot_titles=[title for _, title, _, _ in matrices],\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.15,  # Less space between rows\n",
    "    horizontal_spacing=0.08  # Less space between columns\n",
    ")\n",
    "\n",
    "# Add each matrix as a heatmap to the appropriate subplot\n",
    "for i, (matrix, _, coloraxis, colorscale) in enumerate(matrices):\n",
    "    row = i // 2 + 1  \n",
    "    col = i % 2 + 1   \n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=np.flipud(matrix),  \n",
    "            coloraxis=coloraxis\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "# Configure the shared color axes for pairs\n",
    "fig.update_layout(\n",
    "    title=\"Heatmaps of Average Across Subjects (Confounds vs. No Confounds)\",\n",
    "    coloraxis1=dict(colorscale='RdYlGn',         colorbar=dict(\n",
    "            title=\"Significant Digits\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,  # Adjust length to fit the subplot height\n",
    "            x=1.18,   # Position to the right of the plot\n",
    "            y=1.0,    # Align vertically at the top\n",
    "            yanchor=\"top\"  # Anchor to the top\n",
    "        ),    \n",
    "        cmin=-0.5,\n",
    "        cmax=3.5\n",
    "        ),\n",
    "    coloraxis2=dict(colorscale='RdYlGn_r',        colorbar=dict(\n",
    "            title=\"log(Standard Deviation)\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Slightly further right than coloraxis1\n",
    "            y=0.65,\n",
    "            yanchor=\"top\"\n",
    "        ),  \n",
    "        cmid=-2.5,\n",
    "        cmin=-5,\n",
    "        cmax=-0.5),\n",
    "    coloraxis3=dict(colorscale='brBG',         colorbar=dict(\n",
    "            title=\"Mean\",\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.25,\n",
    "            x=1.18,   # Further right for better spacing\n",
    "            y=0.28,\n",
    "            yanchor=\"top\"\n",
    "        ),   \n",
    "        cmid=0,\n",
    "        cmin=-1,\n",
    "        cmax=1),\n",
    "    height=1400,  \n",
    "    width=1100,   \n",
    "    plot_bgcolor=\"white\",\n",
    "    margin=dict(l=60, r=160, t=80, b=60)  # Adjust margins for better spacing\n",
    ")\n",
    "\n",
    "# Update subplot titles for better readability\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=12)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(wconfavr_avr))\n",
    "print(np.mean(noconfavr_avr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Compute ECDF for each sample\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)  # Sort data\n",
    "    y = np.arange(1, len(x) + 1) / len(x)  # Compute cumulative probability\n",
    "    return x, y\n",
    "# Get upper triangle indices (excluding diagonal)\n",
    "triu_indices = np.triu_indices(100, k=1)  # (row_idx, col_idx)\n",
    "\n",
    "# Efficient extraction into NumPy array\n",
    "subj_valwconfsig_matrix = wconfsig[:, triu_indices[0], triu_indices[1]].T  # Shape: (4950, 152)\n",
    "subj_valNoconfsig_matrix = noconfsig[:, triu_indices[0], triu_indices[1]].T  # Shape: (4950, 152)\n",
    "print(subj_valwconfsig_matrix.shape)\n",
    "print(wconfsig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# # Compute ECDF for a given data array\n",
    "# def ecdf(data):\n",
    "#     x = np.sort(data)  # Sort data\n",
    "#     y = np.arange(1, len(x) + 1) / len(x)  # Compute cumulative probability\n",
    "#     return x, y\n",
    "\n",
    "# # Create a Plotly figure\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # Add all ECDFs with transparency\n",
    "# for idx in range(4949):\n",
    "#     # Get ECDF for both conditions\n",
    "#     x_wconf, y_wconf = ecdf(subj_valwconfsig_matrix[idx])\n",
    "#     x_noconf, y_noconf = ecdf(subj_valNoconfsig_matrix[idx])\n",
    "    \n",
    "#     # Add traces with very low opacity for individual lines\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=x_wconf, \n",
    "#         y=y_wconf, \n",
    "#         mode='lines', \n",
    "#         showlegend=False,\n",
    "#         line=dict(color='rgba(31, 119, 180, 0.02)'),  # Blue with very low opacity\n",
    "#         hoverinfo='skip'\n",
    "#     ))\n",
    "    \n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=x_noconf, \n",
    "#         y=y_noconf, \n",
    "#         mode='lines', \n",
    "#         showlegend=False,\n",
    "#         line=dict(color='rgba(255, 127, 14, 0.02)'),  # Orange with very low opacity\n",
    "#         hoverinfo='skip'\n",
    "#     ))\n",
    "\n",
    "# # Add two traces for the legend\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=[None], y=[None],\n",
    "#     mode='lines',\n",
    "#     name='With Confound',\n",
    "#     line=dict(color='rgb(31, 119, 180)', width=2)\n",
    "# ))\n",
    "\n",
    "# fig.add_trace(go.Scatter(\n",
    "#     x=[None], y=[None],\n",
    "#     mode='lines',\n",
    "#     name='No Confound',\n",
    "#     line=dict(color='rgb(255, 127, 14)', width=2)\n",
    "# ))\n",
    "\n",
    "# # Update layout for better readability\n",
    "# fig.update_layout(\n",
    "#     title=\"ECDF of Significant digits  Values across all 4949 Regions pairs\",\n",
    "#     xaxis_title=\" Significant digits Value\",\n",
    "#     yaxis_title=\"Cumulative Probability\",\n",
    "#     legend_title=\"Condition\",\n",
    "#     template=\"plotly_white\"\n",
    "# )\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Compute ECDF for a given data array\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)  # Sort data\n",
    "    y = np.arange(1, len(x) + 1) / len(x)  # Compute cumulative probability\n",
    "    return x, y\n",
    "\n",
    "# Store x-values where ECDF crosses y=0.5\n",
    "half_max_valuesw = []\n",
    "half_max_valuesno=[]\n",
    "# Iterate through all regions\n",
    "for idx in range(4949):\n",
    "    # Get ECDF for both conditions\n",
    "    x_wconf, y_wconf = ecdf(subj_valwconfsig_matrix[idx])\n",
    "    x_noconf, y_noconf = ecdf(subj_valNoconfsig_matrix[idx])\n",
    "    \n",
    "    # Find x-values where y is closest to 0.5\n",
    "    x_50_wconf = x_wconf[np.argmin(np.abs(y_wconf - 0.5))]\n",
    "    x_50_noconf = x_noconf[np.argmin(np.abs(y_noconf - 0.5))]\n",
    "    \n",
    "    # Store the difference between the two conditions at y=0.5\n",
    "    half_max_valuesw.append( x_50_wconf)\n",
    "    half_max_valuesno.append(x_50_noconf)\n",
    "\n",
    "# Convert to a NumPy array for easier operations\n",
    "half_max_valuesw = np.array(half_max_valuesw)\n",
    "half_max_valuesno = np.array(half_max_valuesno)\n",
    "# print(half_max_valuesno)\n",
    "# Find the best and worst regions based on max difference\n",
    "idxw_max,idxw_min=np.argmax(half_max_valuesw),np.argmin(half_max_valuesw)\n",
    "idxN_max,idxN_min=np.argmax(half_max_valuesno),np.argmin(half_max_valuesno)\n",
    "# print(idxN_max)\n",
    "maxdifw = half_max_valuesw[idxw_max] - half_max_valuesw[idxw_min]\n",
    "maxdifno = half_max_valuesno[idxN_max] - half_max_valuesno[idxN_min]\n",
    "\n",
    "print(f'Max difference for with confound is = {maxdifw},for the best region: {idxw_max}, and the worse region: {idxw_min}')\n",
    "print(f'Max difference for without confound is = {maxdifno},for the best region: {idxN_max}, and the worse region: {idxN_min}')\n",
    "\n",
    "# # Print results\n",
    "# print(f\"Best Region (Max x at y=0.5): {region_idx_best}, Value: {half_max_values[region_idx_best, 1]}\")\n",
    "# print(f\"Worst Region (Min x at y=0.5): {region_idx_worst}, Value: {half_max_values[region_idx_worst, 1]}\")\n",
    "# print(f\"Maximum Difference in x at y=0.5: {max_difference}\")\n",
    "# Max difference for with confound is = 0.23861758665730415,for the best region: 4, and the worse region: 2\n",
    "# Max difference for without confound is = 0.38311376159520716,for the best region: 4, and the worse region: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Compute ECDF for a given data array\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)  # Sort data\n",
    "    y = np.arange(1, len(x) + 1) / len(x)  # Compute cumulative probability\n",
    "    return x, y\n",
    "\n",
    "# Choose a subset to plot (e.g., first 10 ECDFs)\n",
    "num_to_plot = 10  # Adjust as needed\n",
    "selected_indices = [3724,2584,3550]\n",
    "\n",
    "# Get a list of colors from Plotly's qualitative palette\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add ECDF traces for selected elements\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    color = colors[i % len(colors)]  # Cycle through colors\n",
    "    x_wconf, y_wconf = ecdf(subj_valwconfsig_matrix[idx])\n",
    "    x_noconf, y_noconf = ecdf(subj_valNoconfsig_matrix[idx])\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=x_wconf, y=y_wconf, mode='lines', name=f'With Confound {idx}', line=dict(color=color)))\n",
    "    fig.add_trace(go.Scatter(x=x_noconf, y=y_noconf, mode='lines', name=f'No Confound {idx}', line=dict(color=color, dash='dash')))\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    title=\"ECDF of Significant digits  Values across all 4949 Regions pairs\",\n",
    "    xaxis_title=\" Significant digits Value\",\n",
    "    yaxis_title=\"Cumulative Probability\",\n",
    "    legend_title=\"Legend\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# List of matrices and their titles\n",
    "matrices = [\n",
    "    (ressig_greater, \"Rejected null hypothesis for alternative= greater for significant digit\"),\n",
    "    (ressig_twoside, \"Rejected null hypothesis for alternative= two side for significant digit\")\n",
    "]\n",
    "\n",
    "# Create a subplot grid with 1 row and 2 columns\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[title for _, title in matrices],\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.1,  # Adjust spacing between rows\n",
    "    horizontal_spacing=0.1  # Adjust spacing between columns\n",
    ")\n",
    "\n",
    "# Add each matrix as a heatmap to the appropriate subplot\n",
    "for i, (matrix, _) in enumerate(matrices):\n",
    "    row = 1  # Only one row\n",
    "    col = i + 1  # Columns are sequential (1-based index)\n",
    "\n",
    "    # Add heatmap to the subplot\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=np.flipud(matrix),  # Flip matrix vertically if needed\n",
    "            coloraxis=\"coloraxis\"  # All share the same color axis\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "# Configure the shared color axis\n",
    "fig.update_layout(\n",
    "    title=\"Heatmaps of KS and P-Value with Bonferroni correction (conf_values, non_conf_values, alternative=alt) for significant digits\",\n",
    "    coloraxis=dict(\n",
    "        colorscale=[[0, 'black'], [1, 'white']],  # Black for 0, white for 1\n",
    "        colorbar=dict(\n",
    "            title=\"Decision\",\n",
    "            tickvals=[0, 1],   # Only show values 0 and 1\n",
    "            ticktext=[\"Not Rejected\", \"Rejected\"],  # Custom labels\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.7,  # Adjust the length of the colorbar\n",
    "            x=1.1,    # Position to the right of the plots\n",
    "            y=0.5,    # Center vertically\n",
    "            yanchor=\"middle\"\n",
    "        )\n",
    "    ),\n",
    "    height=600,  # Adjust height for better visualization\n",
    "    width=1100   # Adjust width for better visualization\n",
    ")\n",
    "\n",
    "# Update subplot titles for better readability\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=10)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# List of matrices and their titles\n",
    "matrices = [\n",
    "    (resstd_less, \"Rejected null hypothesis for alternative= less for standard deviation\"),\n",
    "    (resstd_twoside, \"Rejected null hypothesis for alternative= two side for standard deviation\")\n",
    "]\n",
    "\n",
    "# Create a subplot grid with 1 row and 2 columns\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=[title for _, title in matrices],\n",
    "    shared_xaxes=True, shared_yaxes=True,\n",
    "    vertical_spacing=0.1,  # Adjust spacing between rows\n",
    "    horizontal_spacing=0.1  # Adjust spacing between columns\n",
    ")\n",
    "\n",
    "# Add each matrix as a heatmap to the appropriate subplot\n",
    "for i, (matrix, _) in enumerate(matrices):\n",
    "    row = 1  # Only one row\n",
    "    col = i + 1  # Columns are sequential (1-based index)\n",
    "\n",
    "    # Add heatmap to the subplot\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(\n",
    "            z=np.flipud(matrix),  # Flip matrix vertically if needed\n",
    "            coloraxis=\"coloraxis\"  # All share the same color axis\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "# Configure the shared color axis\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Heatmaps of KS and P-Value with Bonferroni correction (conf_values, non_conf_values, alternative=alt) for standard deviation\",\n",
    "    coloraxis=dict(\n",
    "        colorscale=[[0, 'black'], [1, 'white']],  # Black for 0, white for 1\n",
    "        colorbar=dict(\n",
    "            title=\"Decision\",\n",
    "            tickvals=[0, 1],   # Only show values 0 and 1\n",
    "            ticktext=[\"Not Rejected\", \"Rejected\"],  # Custom labels\n",
    "            ticks=\"outside\",\n",
    "            thickness=15,\n",
    "            len=0.7,  # Adjust the length of the colorbar\n",
    "            x=1.1,    # Position to the right of the plots\n",
    "            y=0.5,    # Center vertically\n",
    "            yanchor=\"middle\"\n",
    "        )\n",
    "    ),\n",
    "    height=600,  # Adjust height for better visualization\n",
    "    width=1100   # Adjust width for better visualization\n",
    ")\n",
    "\n",
    "# Update subplot titles for better readability\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['font'] = dict(size=10)\n",
    "\n",
    "# Show the figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfW=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/Result_tableWConf.pkl')\n",
    "dfN=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/Result_tableNoConf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfhc_Noconf_num=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/NumRegionalGmetric_NoConf_batchhc.pkl')\n",
    "dfhc_Noconf_anat=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/AnatRegionalGmetric_NoConf_batchhc.pkl')\n",
    "dfhc_Wconf_num=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/NumRegionalGmetric_WConf_batchhc.pkl')\n",
    "dfhc_Wconf_anat=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/AnatRegionalGmetric_WConf_batchhc.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makestdevNum_metric(df):\n",
    "    # Initialize DataFrame\n",
    "    # Ensure columns are explicitly set to object dtype\n",
    "    columns_to_set_as_object = [\n",
    "        'degree_(numericalVar)', 'betweeness_(numericalVar)', \n",
    "        'clusteringcoef_(numericalVar)', 'eigenvec_(numericalVar)', \n",
    "        'smallworldness(numericalVar)', 'avg_shortestPathLength(numericalVar)'\n",
    "    ]\n",
    "\n",
    "    # Initialize DataFrame and set appropriate dtypes\n",
    "    avrofStd_WithinSubject = pd.DataFrame(\n",
    "        columns=columns_to_set_as_object + ['subject', 'session', 'acquisition']\n",
    "    )\n",
    "\n",
    "    avrofStd_WithinSubject = avrofStd_WithinSubject.astype({col: 'object' for col in columns_to_set_as_object})\n",
    "\n",
    "    # Get unique subjects from the Results_table\n",
    "    subjects = np.unique(df['subject'])\n",
    "\n",
    "    i = 0\n",
    "    # Loop over each subject\n",
    "    for subj in subjects:\n",
    "        filtered_df = df[df['subject'] == subj]\n",
    "\n",
    "        # Loop over each session and acquisition\n",
    "        for session in np.unique(filtered_df['session']):\n",
    "            for acquisition in np.unique(filtered_df['acquisition']):\n",
    "                if acquisition not in ['acq-RLsplit1', 'acq-LRsplit1']:\n",
    "                    # Initialize lists to collect values for each repetition\n",
    "                    mat_degree_centrality, mat_betweeness_centrality, mat_eigen_centrality, mat_clustering_coef = [], [], [], []\n",
    "                    std_degree_centrality, std_betweeness_centrality, std_eigen_centrality, std_clustering_coef,std_smallworldness, std_avg_shortestPathLength  = [], [], [], [],[],[]\n",
    "                    smallworldness, avg_shortestPathLength = [], []\n",
    "\n",
    "                    # Loop over each repetition to collect centrality metrics\n",
    "                    for rep in range(1, 11):\n",
    "                        filtered_rows = filtered_df[\n",
    "                            (filtered_df['session'] == session) &\n",
    "                            (filtered_df['acquisition'] == acquisition) &\n",
    "                            (filtered_df['repetition'] == f'rep-{rep}')\n",
    "                        ]\n",
    "                        if filtered_rows.empty:\n",
    "                            continue\n",
    "\n",
    "                        # Extract centrality measures and clustering coefficients\n",
    "                        degree_values = list(filtered_rows['degree_centralities'].values[0].values())\n",
    "                        betweeness_values = list(filtered_rows['betweenness_centralities'].values[0].values())\n",
    "                        eigen_values = list(filtered_rows['eigenvector_centralities'].values[0].values())\n",
    "                        clustering_values = list(filtered_rows['clustering_coefficients'].values[0].values())\n",
    "                        \n",
    "                        # Append values for each repetition\n",
    "                        mat_degree_centrality.append(degree_values)\n",
    "                        mat_betweeness_centrality.append(betweeness_values)\n",
    "                        mat_eigen_centrality.append(eigen_values)\n",
    "                        mat_clustering_coef.append(clustering_values)\n",
    "                        smallworldness.append(filtered_rows['small_worldness'].values[0])\n",
    "                        avg_shortestPathLength.append(filtered_rows['avg_shortest_path_length'].values[0])\n",
    "                    # Calculate the standard deviation of the centrality metrics across repetitions\n",
    "                    if len(mat_degree_centrality)==0:\n",
    "                        continue\n",
    "                    std_degree_centrality.append(np.std(mat_degree_centrality, axis=0))\n",
    "                    std_betweeness_centrality.append(np.std(mat_betweeness_centrality, axis=0))\n",
    "                    std_eigen_centrality.append(np.std(mat_eigen_centrality, axis=0))\n",
    "                    std_clustering_coef.append(np.std(mat_clustering_coef, axis=0))\n",
    "                    std_smallworldness=np.std(smallworldness,axis=0)\n",
    "                    std_avg_shortestPathLength =np.std(avg_shortestPathLength, axis=0)\n",
    "                    # Store the other metrics (assuming these are scalar values)\n",
    "                    # Collect the row as a dictionary\n",
    "                    row_data = {\n",
    "                        'degree_(numericalVar)': std_degree_centrality,\n",
    "                        'betweeness_(numericalVar)': std_betweeness_centrality,\n",
    "                        'clusteringcoef_(numericalVar)': std_clustering_coef,\n",
    "                        'eigenvec_(numericalVar)': std_eigen_centrality,\n",
    "                        'smallworldness(numericalVar)':  std_smallworldness,\n",
    "                        'avg_shortestPathLength(numericalVar)':std_avg_shortestPathLength,\n",
    "                        'subject': subj,\n",
    "                        'session': session,\n",
    "                        'acquisition': acquisition\n",
    "                    }\n",
    "\n",
    "                    # Assign the row to the DataFrame\n",
    "                    avrofStd_WithinSubject.loc[i] = row_data\n",
    "\n",
    "                    i += 1\n",
    "    return avrofStd_WithinSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makestedvAnat_metric(df):\n",
    "    # Initialize DataFrame\n",
    "    # Ensure columns are explicitly set to object dtype\n",
    "    columns_to_set_as_object = [\n",
    "        'degree_(AnatomicalVar)', 'betweeness_(AnatomicalVar)', \n",
    "        'clusteringcoef_(AnatomicalVar)', 'eigenvec_(AnatomicalVar)', \n",
    "        'smallworldness(AnatomicalVar)', 'avg_shortestPathLength(AnatomicalVar)'\n",
    "    ]\n",
    "\n",
    "    # Initialize DataFrame and set appropriate dtypes\n",
    "    avrofStd_BetweenSubject = pd.DataFrame(\n",
    "        columns=columns_to_set_as_object + ['iteration', 'session', 'acquisition']\n",
    "    )\n",
    "\n",
    "    avrofStd_BetweenSubject = avrofStd_BetweenSubject.astype({col: 'object' for col in columns_to_set_as_object})\n",
    "\n",
    "    # Get unique subjects from the Results_table\n",
    "    subjects = np.unique(df['subject'])\n",
    "    # Initialize a list to hold data entries for the final DataFrame\n",
    "    subject_data = []\n",
    "    i=0\n",
    "    # Loop over each repetition\n",
    "    for rep in range(1, 11):\n",
    "        # Filter DataFrame for the current repetition and drop unnecessary columns\n",
    "        filtered_df = df[df['repetition'] == f'rep-{rep}']\n",
    "\n",
    "        # Loop over each session\n",
    "        for session in np.unique(filtered_df['session']):\n",
    "            # Loop over each acquisition type\n",
    "            for acquisition in np.unique(filtered_df['acquisition']):\n",
    "                # Skip unwanted acquisitions\n",
    "                if acquisition in ['acq-RLsplit1', 'acq-LRsplit1']:\n",
    "                    continue\n",
    "                \n",
    "                # Initialize lists to store results for each iteration across subjects\n",
    "                acrosSub_mat_degree_centrality = []\n",
    "                acrosSub_mat_betweeness_centrality = []\n",
    "                acrosSub_mat_eigen_centrality = []\n",
    "                acrosSub_mat_clustering_coef = []\n",
    "                acrosSub_std_degree_centrality = []\n",
    "                acrosSub_std_betweeness_centrality = []\n",
    "                acrosSub_std_eigen_centrality = []\n",
    "                acrosSub_std_clustering_coef = []\n",
    "                acrosSub_std_smallworld=[]\n",
    "                acrosSub_std_avrgShortestPathLength=[]\n",
    "                acrosSub_smallworld=[]\n",
    "                acrosSub_avrgShortestPathLength=[]\n",
    "                \n",
    "                # Loop over each subject\n",
    "                for subj in subjects:\n",
    "                    # Filter rows for the specific subject, session, and acquisition\n",
    "                    filtered_rows = filtered_df[\n",
    "                        (filtered_df['session'] == session) &\n",
    "                        (filtered_df['acquisition'] == acquisition) &\n",
    "                        (filtered_df['subject'] == subj)\n",
    "                    ]\n",
    "\n",
    "                    if filtered_rows.empty:\n",
    "                        continue\n",
    "                    degree_values = list(filtered_rows['degree_centralities'].values[0].values())\n",
    "                    betweeness_values = list(filtered_rows['betweenness_centralities'].values[0].values())\n",
    "                    eigen_values = list(filtered_rows['eigenvector_centralities'].values[0].values())\n",
    "                    clustering_values = list(filtered_rows['clustering_coefficients'].values[0].values())\n",
    "\n",
    "                    # Append averages for each subject\n",
    "                    acrosSub_mat_degree_centrality.append(degree_values)\n",
    "                    acrosSub_mat_betweeness_centrality.append(betweeness_values)\n",
    "                    acrosSub_mat_eigen_centrality.append(eigen_values)\n",
    "                    acrosSub_mat_clustering_coef.append(clustering_values)\n",
    "                    acrosSub_smallworld.append(filtered_rows['small_worldness'].values[0])\n",
    "                    acrosSub_avrgShortestPathLength.append(filtered_rows['avg_shortest_path_length'].values[0])\n",
    "\n",
    "                if len(acrosSub_mat_degree_centrality)<2:\n",
    "                    continue         \n",
    "                # Append standard deviations for each subject\n",
    "                acrosSub_std_degree_centrality.append(np.std(acrosSub_mat_degree_centrality,axis=0))\n",
    "                acrosSub_std_betweeness_centrality.append(np.std(acrosSub_mat_betweeness_centrality,axis=0))\n",
    "                acrosSub_std_eigen_centrality.append(np.std(acrosSub_mat_eigen_centrality,axis=0))\n",
    "                acrosSub_std_clustering_coef.append(np.std(acrosSub_mat_clustering_coef,axis=0))\n",
    "                acrosSub_std_smallworld=np.std(acrosSub_smallworld,axis=0)\n",
    "                acrosSub_std_avrgShortestPathLength=np.std(acrosSub_avrgShortestPathLength,axis=0)\n",
    "\n",
    "                # Collect the row as a dictionary\n",
    "                row_data = {\n",
    "                    'degree_(AnatomicalVar)': acrosSub_std_degree_centrality,\n",
    "                    'betweeness_(AnatomicalVar)': acrosSub_std_betweeness_centrality,\n",
    "                    'clusteringcoef_(AnatomicalVar)': acrosSub_std_eigen_centrality,\n",
    "                    'eigenvec_(AnatomicalVar)': acrosSub_std_clustering_coef,\n",
    "                    'smallworldness(AnatomicalVar)':acrosSub_std_smallworld,\n",
    "                    'avg_shortestPathLength(AnatomicalVar)':acrosSub_std_avrgShortestPathLength,\n",
    "                    'iteration': f'iter_{rep}',\n",
    "                    'session': session,\n",
    "                    'acquisition': acquisition\n",
    "                }\n",
    "\n",
    "                # Assign the row to the DataFrame\n",
    "                avrofStd_BetweenSubject.loc[i] = row_data\n",
    "                i=i+1\n",
    "\n",
    "    return avrofStd_BetweenSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Noconf_num=makestdevNum_metric(dfN)\n",
    "df_Noconf_anat=makestedvAnat_metric(dfN)\n",
    "df_Wconf_num=makestdevNum_metric(dfW)\n",
    "df_Wconf_anat=makestedvAnat_metric(dfW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "Noconfmet=Results_tableNoConf  #[Results_tableNoConf['acquisition']=='acq-RL']\n",
    "Wconfmet=Results_tableWConf    #[Results_tableWConf['acquisition']=='acq-RL']\n",
    "\n",
    "# Define the graph metric columns (which contain 100 values each)\n",
    "graph_metrics = [\n",
    "    'degree_centralities', 'betweenness_centralities', \n",
    "    'eigenvector_centralities', 'clustering_coefficients'\n",
    "]\n",
    "Noconfmet_selected = pd.DataFrame()\n",
    "Wconfmet_selected = pd.DataFrame()\n",
    "\n",
    "\n",
    "for metric in graph_metrics:\n",
    "    Noconfmet_selected[metric] = Noconfmet[metric].apply(lambda d: np.array(list(d.values())).reshape(1, -1))\n",
    "    Wconfmet_selected[metric] = Wconfmet[metric].apply(lambda d: np.array(list(d.values())).reshape(1, -1))\n",
    "\n",
    "wconfdeg,noconfdeg=stack_sub(Wconfmet_selected,Noconfmet_selected,'degree_centralities')#mode can be : std , mean , significant_digits_matrix\n",
    "wconfbet,noconfbet=stack_sub(Wconfmet_selected,Noconfmet_selected,'betweenness_centralities')#mode can be : std , mean , significant_digits_matrix\n",
    "wconfclcf,noconfclcf=stack_sub(Wconfmet_selected,Noconfmet_selected,'clustering_coefficients')#mode can be : std , mean , significant_digits_matrix\n",
    "wconfeigen,noconfeigen=stack_sub(Wconfmet_selected,Noconfmet_selected,'eigenvector_centralities')#mode can be : std , mean , significant_digits_matrix\n",
    "# wconfmet.shape\n",
    "_,__,pvalcordegtwoside,resdeg_twoside=ksTestWithCorrection(wconfdeg,noconfdeg,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcordeggreater,resdeg_greater=ksTestWithCorrection(wconfdeg,noconfdeg,'greater','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcordegless,resdeg_less=ksTestWithCorrection(wconfdeg,noconfdeg,'less','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcorbettwoside,resbet_twoside=ksTestWithCorrection(wconfbet,noconfbet,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcorbetgreater,resbet_greater=ksTestWithCorrection(wconfbet,noconfbet,'greater','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcorbetless,resbet_less=ksTestWithCorrection(wconfbet,noconfbet,'less','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcordclcftwoside,resclcf_twoside=ksTestWithCorrection(wconfclcf,noconfclcf,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcordclcfgreater,resclcf_greater=ksTestWithCorrection(wconfclcf,noconfclcf,'greater','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcorclcfless,resclcf_less=ksTestWithCorrection(wconfclcf,noconfclcf,'less','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcoreigentwoside,reseigen_twoside=ksTestWithCorrection(wconfeigen,noconfeigen,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcoreigengreater,reseigen_greater=ksTestWithCorrection(wconfeigen,noconfeigen,'greater','Bonferroni') #FDR,Bonferroni\n",
    "_,__,pvalcoreigenless,reseigen_less=ksTestWithCorrection(wconfeigen,noconfeigen,'less','Bonferroni') #FDR,Bonferroni\n",
    "##############Global metric\n",
    "res1smWt,res2smWt=ks_2samp(Wconfmet['small_worldness'],Noconfmet['small_worldness'],'two-side') #FDR,Bonferroni\n",
    "res1smWg,res2smWg=ks_2samp(Wconfmet['small_worldness'],Noconfmet['small_worldness'],'greater') #FDR,Bonferroni\n",
    "res1smWl,res2smWl=ks_2samp(Wconfmet['small_worldness'],Noconfmet['small_worldness'],'less') #FDR,Bonferroni\n",
    "res1pathLt,res2pathLt=ks_2samp(Wconfmet['avg_shortest_path_length'],Noconfmet['avg_shortest_path_length'],'two-side') #FDR,Bonferroni\n",
    "res1pathLg,res2pathLg=ks_2samp(Wconfmet['avg_shortest_path_length'],Noconfmet['avg_shortest_path_length'],'greater') #FDR,Bonferroni\n",
    "res1pathLl,res2pathLl=ks_2samp(Wconfmet['avg_shortest_path_length'],Noconfmet['avg_shortest_path_length'],'less') #FDR,Bonferroni\n",
    "ressmWt, pval_correctedsmWt, _, _ = multipletests(res2smWt, method='bonferroni')\n",
    "ressmWg, pval_correctedsmWg, _, _ = multipletests(res2smWg, method='bonferroni')\n",
    "ressmWl, pval_correctedsmWl, _, _ = multipletests(res2smWl, method='bonferroni')\n",
    "respathLt, pval_correctedpathLt, _, _ = multipletests(res2pathLt, method='bonferroni')\n",
    "respathLg, pval_correctedpathLg, _, _ = multipletests(res2pathLg, method='bonferroni')\n",
    "respathLl, pval_correctedpathLl, _, _ = multipletests(res2pathLl, method='bonferroni')\n",
    "print(ressmWt,ressmWg,ressmWl)\n",
    "print(respathLt,respathLg,respathLl)\n",
    "# pvalcordeggreater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import ks_2samp\n",
    "# from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# columns_to_convert = ['degree_centralities', 'betweenness_centralities',\t'eigenvector_centralities','clustering_coefficients'] \n",
    "# # \n",
    "# # Convert specific columns from list to numpy array\n",
    "# for column in columns_to_convert:\n",
    "#     if column in Noconfmet.columns:\n",
    "#         Noconfmet[column] = Noconfmet[column].apply(np.array)\n",
    "#         Wconfmet[column] = Wconfmet[column].apply(np.array)\n",
    "# # For Wconfmet DataFrame\n",
    "# Wconfmet_selected = Wconfmet.loc[:, columns_to_convert]\n",
    "# # For Noconfmet DataFrame\n",
    "# Noconfmet_selected = Noconfmet.loc[:, columns_to_convert]\n",
    "\n",
    "# # Noconfmet['degree_(numericalVar)'].iloc[0].shape[0]\n",
    "# wconfdeg,noconfdeg=stack_sub(Wconfmet_selected,Noconfmet_selected,'degree_centralities')#mode can be : std , mean , significant_digits_matrix\n",
    "# wconfbet,noconfbet=stack_sub(Wconfmet_selected,Noconfmet_selected,'betweenness_centralities')#mode can be : std , mean , significant_digits_matrix\n",
    "# wconfclcf,noconfclcf=stack_sub(Wconfmet_selected,Noconfmet_selected,'clustering_coefficients')#mode can be : std , mean , significant_digits_matrix\n",
    "# wconfeigen,noconfeigen=stack_sub(Wconfmet_selected,Noconfmet_selected,'eigenvector_centralities')#mode can be : std , mean , significant_digits_matrix\n",
    "# print(wconfdeg.shape)\n",
    "# # wconfmet.shape\n",
    "# _,__,pvalcordegtwoside,resdeg_twoside=ksTestWithCorrection(wconfdeg,noconfdeg,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcordeggreater,resdeg_greater=ksTestWithCorrection(wconfdeg,noconfdeg,'greater','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcordegless,resdeg_less=ksTestWithCorrection(wconfdeg,noconfdeg,'less','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcorbettwoside,resbet_twoside=ksTestWithCorrection(wconfbet,noconfbet,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcorbetgreater,resbet_greater=ksTestWithCorrection(wconfbet,noconfbet,'greater','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcorbetless,resbet_less=ksTestWithCorrection(wconfbet,noconfbet,'less','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcordclcftwoside,resclcf_twoside=ksTestWithCorrection(wconfclcf,noconfclcf,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcordclcfgreater,resclcf_greater=ksTestWithCorrection(wconfclcf,noconfclcf,'greater','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcorclcfless,resclcf_less=ksTestWithCorrection(wconfclcf,noconfclcf,'less','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcoreigentwoside,reseigen_twoside=ksTestWithCorrection(wconfeigen,noconfeigen,'two-side','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcoreigengreater,reseigen_greater=ksTestWithCorrection(wconfeigen,noconfeigen,'greater','Bonferroni') #FDR,Bonferroni\n",
    "# _,__,pvalcoreigenless,reseigen_less=ksTestWithCorrection(wconfeigen,noconfeigen,'less','Bonferroni') #FDR,Bonferroni\n",
    "\n",
    "# ##############Global metric\n",
    "# res1smWt,res2smWt=ks_2samp(Wconfmet['small_worldness'],Noconfmet['small_worldness'],'two-side') #FDR,Bonferroni\n",
    "# res1smWg,res2smWg=ks_2samp(Wconfmet['small_worldness'],Noconfmet['small_worldness'],'greater') #FDR,Bonferroni\n",
    "# res1smWl,res2smWl=ks_2samp(Wconfmet['small_worldness'],Noconfmet['small_worldness'],'less') #FDR,Bonferroni\n",
    "# res1pathLt,res2pathLt=ks_2samp(Wconfmet['avg_shortest_path_length'],Noconfmet['avg_shortest_path_length'],'two-side') #FDR,Bonferroni\n",
    "# res1pathLg,res2pathLg=ks_2samp(Wconfmet['avg_shortest_path_length'],Noconfmet['avg_shortest_path_length'],'greater') #FDR,Bonferroni\n",
    "# res1pathLl,res2pathLl=ks_2samp(Wconfmet['avg_shortest_path_length'],Noconfmet['aavg_shortest_path_length'],'less') #FDR,Bonferroni\n",
    "# ressmWt, pval_correctedsmWt, _, _ = multipletests(res2smWt, method='bonferroni')\n",
    "# ressmWg, pval_correctedsmWg, _, _ = multipletests(res2smWg, method='bonferroni')\n",
    "# ressmWl, pval_correctedsmWl, _, _ = multipletests(res2smWl, method='bonferroni')\n",
    "# respathLt, pval_correctedpathLt, _, _ = multipletests(res2pathLt, method='bonferroni')\n",
    "# respathLg, pval_correctedpathLg, _, _ = multipletests(res2pathLg, method='bonferroni')\n",
    "# respathLl, pval_correctedpathLl, _, _ = multipletests(res2pathLl, method='bonferroni')\n",
    "# # print(ressmWt,ressmWg,ressmWl)\n",
    "# print(respathLt,respathLg,respathLl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Reshape arrays\n",
    "# wconfdeg = wconfdeg.reshape(100, 144)\n",
    "# noconfdeg = noconfdeg.reshape(100, 144)\n",
    "# # wconfbet = wconfbet.reshape(100, 147)\n",
    "# # noconfbet = noconfbet.reshape(100, 144)\n",
    "# # wconfclcf = wconfclcf.reshape(100, 147)\n",
    "# # noconfclcf = noconfclcf.reshape(100, 144)\n",
    "# # wconfeigen = wconfeigen.reshape(100, 147)\n",
    "# # noconfeigen = noconfeigen.reshape(100, 144)\n",
    "\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "\n",
    "# # Compute ECDF for a given data array\n",
    "# def ecdf(data):\n",
    "#     x = np.sort(data)  # Sort data\n",
    "#     y = np.arange(1, len(x) + 1) / len(x)  # Compute cumulative probability\n",
    "#     return x, y\n",
    "\n",
    "# # Choose a subset to plot (e.g., first 10 ECDFs)\n",
    "# num_to_plot = 100  # Adjust as needed\n",
    "# selected_indices = np.linspace(0, 99, num_to_plot, dtype=int)  # Evenly spaced indices\n",
    "\n",
    "# # Get a list of colors from Plotly's qualitative palette\n",
    "# colors = px.colors.qualitative.Plotly\n",
    "\n",
    "# # Create a Plotly figure\n",
    "# fig = go.Figure()\n",
    "\n",
    "# # Add ECDF traces for selected elements\n",
    "# for i, idx in enumerate(selected_indices):\n",
    "#     color = colors[i % len(colors)]  # Cycle through colors\n",
    "#     # if idx==56:\n",
    "#     #     print(wconfdeg[56])\n",
    "#     x_wconf, y_wconf = ecdf(wconfdeg[idx])\n",
    "#     x_noconf, y_noconf = ecdf(noconfdeg[idx])\n",
    "    \n",
    "#     fig.add_trace(go.Scatter(x=x_wconf, y=y_wconf, mode='lines', name=f'With Confound {idx}', line=dict(color=color)))\n",
    "#     fig.add_trace(go.Scatter(x=x_noconf, y=y_noconf, mode='lines', name=f'No Confound {idx}', line=dict(color=color, dash='dash')))\n",
    "\n",
    "# # Update layout for better readability\n",
    "# fig.update_layout(\n",
    "#     title=\"ECDF of Connectivity Values (Subset of 100 regions)\",\n",
    "#     xaxis_title=\"Connectivity Value\",\n",
    "#     yaxis_title=\"Cumulative Probability\",\n",
    "#     legend_title=\"Legend\",\n",
    "#     template=\"plotly_white\"\n",
    "# )\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# # Reshape arrays\n",
    "# wconfdeg = wconfdeg.reshape(100, 144)\n",
    "# noconfdeg = noconfdeg.reshape(100, 144)\n",
    "# wconfbet = wconfbet.reshape(100, 144)\n",
    "# noconfbet = noconfbet.reshape(100, 144)\n",
    "# wconfclcf = wconfclcf.reshape(100, 144)\n",
    "# noconfclcf = noconfclcf.reshape(100, 144)\n",
    "# wconfeigen = wconfeigen.reshape(100, 144)\n",
    "# noconfeigen = noconfeigen.reshape(100, 144)\n",
    "# # Compute ECDF for a given data array\n",
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# # Compute ECDF for a given data array\n",
    "# def ecdf(data):\n",
    "#     x = np.sort(data)  # Sort data\n",
    "#     y = np.arange(1, len(x) + 1) / len(x)  # Compute cumulative probability\n",
    "#     return x, y\n",
    "\n",
    "# # Create a subplot figure with 2x2 grid\n",
    "# fig = make_subplots(\n",
    "#     rows=2, \n",
    "#     cols=2,\n",
    "#     subplot_titles=[\n",
    "#         \"Degree Centrality ECDF\", \n",
    "#         \"Betweenness Centrality ECDF\",\n",
    "#         \"Closeness Centrality ECDF\",\n",
    "#         \"Eigenvector Centrality ECDF\"\n",
    "#     ],\n",
    "#     vertical_spacing=0.12,\n",
    "#     horizontal_spacing=0.08\n",
    "# )\n",
    "\n",
    "# # Define axis ranges for each metric\n",
    "# x_ranges = {\n",
    "#     'degree': [0,0.05], # np.max(np.concatenate([wconfdeg.flatten(), noconfdeg.flatten()]))],\n",
    "#     'betweenness': [0,0.0001] ,#np.max(np.concatenate([wconfbet.flatten(), noconfbet.flatten()]))],\n",
    "#     'closeness': [0,0.005 ], #np.max(np.concatenate([wconfclcf.flatten(), noconfclcf.flatten()]))],\n",
    "#     'eigenvector': [0,0.005 ], #np.max(np.concatenate([wconfeigen.flatten(), noconfeigen.flatten()]))]\n",
    "# }\n",
    "\n",
    "# # Plot all regions with transparency for each metric\n",
    "# # 1. Degree Centrality\n",
    "# for i in range(100):\n",
    "#     x_wconf, y_wconf = ecdf(wconfdeg[i])\n",
    "#     x_noconf, y_noconf = ecdf(noconfdeg[i])\n",
    "    \n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_wconf, \n",
    "#             y=y_wconf, \n",
    "#             mode='lines', \n",
    "#             showlegend=False,\n",
    "#             line=dict(color='rgba(31, 119, 180, 0.1)'),  # Blue with low opacity\n",
    "#             hoverinfo='skip'\n",
    "#         ),\n",
    "#         row=1, col=1\n",
    "#     )\n",
    "    \n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_noconf, \n",
    "#             y=y_noconf, \n",
    "#             mode='lines', \n",
    "#             showlegend=False,\n",
    "#             line=dict(color='rgba(255, 127, 14, 0.1)'),  # Orange with low opacity\n",
    "#             hoverinfo='skip'\n",
    "#         ),\n",
    "#         row=1, col=1\n",
    "#     )\n",
    "\n",
    "# # 2. Betweenness Centrality\n",
    "# for i in range(100):\n",
    "#     x_wconf, y_wconf = ecdf(wconfbet[i])\n",
    "#     x_noconf, y_noconf = ecdf(noconfbet[i])\n",
    "    \n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_wconf, \n",
    "#             y=y_wconf, \n",
    "#             mode='lines', \n",
    "#             showlegend=False,\n",
    "#             line=dict(color='rgba(31, 119, 180, 0.1)'),  # Blue with low opacity\n",
    "#             hoverinfo='skip'\n",
    "#         ),\n",
    "#         row=1, col=2\n",
    "#     )\n",
    "    \n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_noconf, \n",
    "#             y=y_noconf, \n",
    "#             mode='lines', \n",
    "#             showlegend=False,\n",
    "#             line=dict(color='rgba(255, 127, 14, 0.1)'),  # Orange with low opacity\n",
    "#             hoverinfo='skip'\n",
    "#         ),\n",
    "#         row=1, col=2\n",
    "#     )\n",
    "\n",
    "# # 3. Closeness Centrality\n",
    "# for i in range(100):\n",
    "#     x_wconf, y_wconf = ecdf(wconfclcf[i])\n",
    "#     x_noconf, y_noconf = ecdf(noconfclcf[i])\n",
    "    \n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_wconf, \n",
    "#             y=y_wconf, \n",
    "#             mode='lines', \n",
    "#             showlegend=False,\n",
    "#             line=dict(color='rgba(31, 119, 180, 0.1)'),  # Blue with low opacity\n",
    "#             hoverinfo='skip'\n",
    "#         ),\n",
    "#         row=2, col=1\n",
    "#     )\n",
    "    \n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_noconf, \n",
    "#             y=y_noconf, \n",
    "#             mode='lines', \n",
    "#             showlegend=False,\n",
    "#             line=dict(color='rgba(255, 127, 14, 0.1)'),  # Orange with low opacity\n",
    "#             hoverinfo='skip'\n",
    "#         ),\n",
    "#         row=2, col=1\n",
    "#     )\n",
    "\n",
    "# # 4. Eigenvector Centrality\n",
    "# for i in range(100):\n",
    "#     x_wconf, y_wconf = ecdf(wconfeigen[i])\n",
    "#     x_noconf, y_noconf = ecdf(noconfeigen[i])\n",
    "    \n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_wconf, \n",
    "#             y=y_wconf, \n",
    "#             mode='lines', \n",
    "#             showlegend=False,\n",
    "#             line=dict(color='rgba(31, 119, 180, 0.1)'),  # Blue with low opacity\n",
    "#             hoverinfo='skip'\n",
    "#         ),\n",
    "#         row=2, col=2\n",
    "#     )\n",
    "    \n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_noconf, \n",
    "#             y=y_noconf, \n",
    "#             mode='lines', \n",
    "#             showlegend=False,\n",
    "#             line=dict(color='rgba(255, 127, 14, 0.1)'),  # Orange with low opacity\n",
    "#             hoverinfo='skip'\n",
    "#         ),\n",
    "#         row=2, col=2\n",
    "#     )\n",
    "\n",
    "# # Add two traces for the legend (only need to add once for the whole figure)\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode='lines',\n",
    "#         name='With Confound',\n",
    "#         line=dict(color='rgb(31, 119, 180)', width=2)\n",
    "#     ),\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# fig.add_trace(\n",
    "#     go.Scatter(\n",
    "#         x=[None], y=[None],\n",
    "#         mode='lines',\n",
    "#         name='No Confound',\n",
    "#         line=dict(color='rgb(255, 127, 14)', width=2)\n",
    "#     ),\n",
    "#     row=1, col=1\n",
    "# )\n",
    "\n",
    "# # Update x-axis ranges for each subplot\n",
    "# fig.update_xaxes(range=x_ranges['degree'], title_text=\"Degree Centrality Value\", row=1, col=1)\n",
    "# fig.update_xaxes(range=x_ranges['betweenness'], title_text=\"Betweenness Centrality Value\", row=1, col=2)\n",
    "# fig.update_xaxes(range=x_ranges['closeness'], title_text=\"Clustering coefficient Value\", row=2, col=1)\n",
    "# fig.update_xaxes(range=x_ranges['eigenvector'], title_text=\"Eigenvector Centrality Value\", row=2, col=2)\n",
    "\n",
    "# # Update y-axis title for all subplots\n",
    "# fig.update_yaxes(title_text=\"Cumulative Probability\", row=1, col=1)\n",
    "# fig.update_yaxes(title_text=\"Cumulative Probability\", row=1, col=2)\n",
    "# fig.update_yaxes(title_text=\"Cumulative Probability\", row=2, col=1)\n",
    "# fig.update_yaxes(title_text=\"Cumulative Probability\", row=2, col=2)\n",
    "\n",
    "# # Update layout for the entire figure\n",
    "# fig.update_layout(\n",
    "#     title=\"ECDF Comparison of Network Centrality Metrics (All 100 Regions)\",\n",
    "#     legend_title=\"Condition\",\n",
    "#     template=\"plotly_white\",\n",
    "#     height=1000,  # Adjust for better visualization\n",
    "#     width=1800,  # Adjust for better visualization\n",
    "#     legend=dict(\n",
    "#         orientation=\"h\",\n",
    "#         yanchor=\"bottom\",\n",
    "#         y=1.02,\n",
    "#         xanchor=\"center\",\n",
    "#         x=0.5\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# from plotly.subplots import make_subplots\n",
    "# import numpy as np\n",
    "\n",
    "# # List of matrices and their titles\n",
    "# matrices = [\n",
    "#     (np.expand_dims(resdeg_twoside, axis=0), \"Rejected null hypothesis (two-sided) for Degree Centrality\"),\n",
    "#     (np.expand_dims(resdeg_greater, axis=0), \"Rejected null hypothesis (greater) for Degree Centrality\"),\n",
    "#     (np.expand_dims(resdeg_less, axis=0), \"Rejected null hypothesis (less) for Degree Centrality\"),\n",
    "\n",
    "#     (np.expand_dims(resbet_twoside, axis=0), \"Rejected null hypothesis (two-sided) for Betweenness Centrality\"),\n",
    "#     (np.expand_dims(resbet_greater, axis=0), \"Rejected null hypothesis (greater) for Betweenness Centrality\"),\n",
    "#     (np.expand_dims(resbet_less, axis=0), \"Rejected null hypothesis (less) for Betweenness Centrality\"),\n",
    "\n",
    "#     (np.expand_dims(resclcf_twoside, axis=0), \"Rejected null hypothesis (two-sided) for Clustering Coefficient\"),\n",
    "#     (np.expand_dims(resclcf_greater, axis=0), \"Rejected null hypothesis (greater) for Clustering Coefficient\"),\n",
    "#     (np.expand_dims(resclcf_less, axis=0), \"Rejected null hypothesis (less) for Clustering Coefficient\"),\n",
    "\n",
    "#     (np.expand_dims(reseigen_twoside, axis=0), \"Rejected null hypothesis (two-sided) for Eigenvector Centrality\"),\n",
    "#     (np.expand_dims(reseigen_greater, axis=0), \"Rejected null hypothesis (greater) for Eigenvector Centrality\"),\n",
    "#     (np.expand_dims(reseigen_less, axis=0), \"Rejected null hypothesis (less) for Eigenvector Centrality\")\n",
    "# ]\n",
    "\n",
    "# # Adjust subplot layout (4 rows, 3 columns)\n",
    "# fig = make_subplots(\n",
    "#     rows=4, cols=3,\n",
    "#     subplot_titles=[title for _, title in matrices],\n",
    "#     vertical_spacing=0.15, horizontal_spacing=0.1\n",
    "# )\n",
    "\n",
    "# # Discrete color map\n",
    "# color_map = [[0, 'black'], [1, 'white']]\n",
    "\n",
    "# # Add each matrix to the heatmap\n",
    "# for i, (matrix, _) in enumerate(matrices):\n",
    "#     row = i // 3 + 1\n",
    "#     col = i % 3 + 1\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Heatmap(\n",
    "#             z=matrix, \n",
    "#             coloraxis=\"coloraxis\"\n",
    "#         ),\n",
    "#         row=row, col=col\n",
    "#     )\n",
    "\n",
    "#     # Update the axis to hide y-tick labels for each subplot\n",
    "#     fig.update_yaxes(showticklabels=False, row=row, col=col)\n",
    "\n",
    "# # Update layout with a shared color axis\n",
    "# fig.update_layout(\n",
    "#     title=\"KS Test Results with Bonferroni Correction for Graph Metrics\",\n",
    "#     coloraxis=dict(\n",
    "#         colorscale=color_map,\n",
    "#         colorbar=dict(\n",
    "#             title=\"Decision\",\n",
    "#             tickvals=[0, 1],\n",
    "#             ticktext=[\"Not Rejected: 0\", \"Rejected: 1\"],\n",
    "#             ticks=\"outside\",\n",
    "#             thickness=15,\n",
    "#             len=0.7,\n",
    "#             x=1.1,\n",
    "#             y=0.5,\n",
    "#             yanchor=\"middle\"\n",
    "#         )\n",
    "#     ),\n",
    "#     height=900,  # Increased height\n",
    "#     width=1300   # Increased width\n",
    "# )\n",
    "\n",
    "# # Improve readability of subplot titles\n",
    "# for annotation in fig['layout']['annotations']:\n",
    "#     annotation['font'] = dict(size=11)\n",
    "\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def compute_average_num(df_Noconf_num,df_Wconf_num):\n",
    "    # Assuming 'df' has already been extracted\n",
    "    df = df_Noconf_num[(df_Noconf_num['acquisition'] == 'acq-RL') & \n",
    "                                (df_Noconf_num['session'] == '1')]\n",
    "\n",
    "    # df = df.drop(columns=['session', 'acquisition', 'subject' # Remove the unnecessary column\n",
    "    # ])\n",
    "\n",
    "\n",
    "    cols=['degree_(numericalVar)',\t'betweeness_(numericalVar)',\t'clusteringcoef_(numericalVar)'\t,'eigenvec_(numericalVar)'\t,'smallworldness(numericalVar)','avg_shortestPathLength(numericalVar)']\n",
    "    # Initialize a new DataFrame for the final stacked result\n",
    "    stacked_row = {}\n",
    "\n",
    "    # Loop through each column\n",
    "    for col in cols:\n",
    "        # If column contains matrices or arrays, stack them vertically\n",
    "        stacked_row[col] = np.vstack(df[col].values)\n",
    "\n",
    "    # Convert the dictionary to a DataFrame with one row\n",
    "    stacked_df = pd.DataFrame([stacked_row])\n",
    "    # Create a DataFrame to store the averaged values\n",
    "    avrg_Noconf_num = pd.DataFrame({\n",
    "        col: [np.mean(stacked_df.iloc[0][col], axis=0)] for col in ['degree_(numericalVar)',\t'betweeness_(numericalVar)',\t'clusteringcoef_(numericalVar)' , 'eigenvec_(numericalVar)']\n",
    "    })\n",
    "    # Add 'small_worldness' and 'avg_shortest_path_length' to avrg_Wconf_anat\n",
    "    avrg_Noconf_num['smallworldness(numericalVar)'] =stacked_df['smallworldness(numericalVar)']\n",
    "    avrg_Noconf_num['avg_shortestPathLength(numericalVar)'] = stacked_df['avg_shortestPathLength(numericalVar)']\n",
    "\n",
    "    ##########################################################################################3\n",
    "    # Assuming 'df' has already been extracted\n",
    "    df1= df_Wconf_num[(df_Wconf_num['acquisition'] == 'acq-RL') & \n",
    "                                (df_Wconf_num['session'] == '1')]\n",
    "\n",
    "    # df1 = df1.drop(columns=['session', 'acquisition', 'subject'])\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize a new DataFrame for the final stacked result\n",
    "    stacked_row = {}\n",
    "\n",
    "    # Loop through each column\n",
    "    for col in cols:\n",
    "        # If column contains matrices or arrays, stack them vertically\n",
    "        stacked_row[col] = np.vstack(df1[col].values)\n",
    "\n",
    "    # Convert the dictionary to a DataFrame with one row\n",
    "    stacked_df1 = pd.DataFrame([stacked_row])\n",
    "    # Create a DataFrame to store the averaged values\n",
    "    avrg_Wconf_num = pd.DataFrame({\n",
    "        col: [np.mean(stacked_df1.iloc[0][col], axis=0)] for col in ['degree_(numericalVar)',\t'betweeness_(numericalVar)',\t'clusteringcoef_(numericalVar)' , 'eigenvec_(numericalVar)']\n",
    "    })\n",
    "    # Add 'small_worldness' and 'avg_shortest_path_length' to avrg_Wconf_anat\n",
    "    avrg_Wconf_num['smallworldness(numericalVar)'] =stacked_df1['smallworldness(numericalVar)']\n",
    "    avrg_Wconf_num['avg_shortestPathLength(numericalVar)'] = stacked_df1['avg_shortestPathLength(numericalVar)']\n",
    "    avrg_Wconf_num = avrg_Wconf_num.rename(columns={'degree_(numericalVar)': 'degree_(numericalVarW)', 'betweeness_(numericalVar)': 'betweeness_(numericalVarW)' , 'clusteringcoef_(numericalVar)':'clusteringcoef_(numericalVarW)'   , 'eigenvec_(numericalVar)':'eigenvec_(numericalVarW)'  ,'smallworldness(numericalVar)':'smallworldness(numericalVarW)','avg_shortestPathLength(numericalVar)':'avg_shortestPathLength(numericalVarW)'\n",
    "\n",
    "\t})\n",
    "    return avrg_Noconf_num,avrg_Wconf_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "def compute_average_anat(df_Noconf_anat,df_Wconf_anat):\n",
    "    # Assuming 'df' has already been extracted\n",
    "    df = df_Noconf_anat[(df_Noconf_anat['acquisition'] == 'acq-RL') & \n",
    "                                (df_Noconf_anat['session'] == '1')&(df_Noconf_anat['iteration']==\"iter_1\")]\n",
    "\n",
    "    # df = df.drop(columns=['session', 'acquisition', 'iteration'])\n",
    "\n",
    "    cols=['degree_(AnatomicalVar)'\t,'betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)'\t,'eigenvec_(AnatomicalVar)',\t'smallworldness(AnatomicalVar)',\t'avg_shortestPathLength(AnatomicalVar)']\n",
    "\n",
    "\n",
    "    # Initialize a new DataFrame for the final stacked result\n",
    "    stacked_row1= {}\n",
    "\n",
    "    # Loop through each column\n",
    "    for col in cols:\n",
    "        # If column contains matrices or arrays, stack them vertically\n",
    "        stacked_row1[col] = np.vstack(df[col].values)\n",
    "\n",
    "    # Convert the dictionary to a DataFrame with one row\n",
    "    stacked_df = pd.DataFrame([stacked_row1])\n",
    "    print(stacked_df['degree_(AnatomicalVar)'].values[0].shape)\n",
    "\n",
    "    # Create a DataFrame to store the averaged values\n",
    "    avrg_Noconf_anat = pd.DataFrame({\n",
    "        col: [np.mean(stacked_df.iloc[0][col], axis=0)] for col in ['degree_(AnatomicalVar)',\t'betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)' , 'eigenvec_(AnatomicalVar)']\n",
    "    })\n",
    "    # Add 'small_worldness' and 'avg_shortest_path_length' to avrg_Wconf_anat\n",
    "    avrg_Noconf_anat['smallworldness(AnatomicalVar)'] =stacked_df['smallworldness(AnatomicalVar)']\n",
    "    avrg_Noconf_anat['avg_shortestPathLength(AnatomicalVar)'] = stacked_df['avg_shortestPathLength(AnatomicalVar)']\n",
    "    ###########################################################################################3\n",
    "    # Assuming 'df' has already been extracted\n",
    "    df1 = df_Wconf_anat[(df_Wconf_anat['acquisition'] == 'acq-RL') & \n",
    "                                (df_Wconf_anat['session'] == '1')& (df_Wconf_anat['iteration']==\"iter_1\")]\n",
    "\n",
    "    # df1 = df1.drop(columns=['session', 'acquisition', 'iteration'])\n",
    "\n",
    "\n",
    "\n",
    "    # Initialize a new DataFrame for the final stacked result\n",
    "    stacked_row1= {}\n",
    "\n",
    "    # Loop through each column\n",
    "    for col in cols:\n",
    "        # If column contains matrices or arrays, stack them vertically\n",
    "        stacked_row1[col] = np.vstack(df1[col].values)\n",
    "\n",
    "    # Convert the dictionary to a DataFrame with one row\n",
    "    stacked_df1 = pd.DataFrame([stacked_row1])\n",
    "    # Create a DataFrame to store the averaged values(df_Noconf_anat['iteration']==\"iter_1\")\n",
    "    # Create a DataFrame to store the averaged values\n",
    "    avrg_Wconf_anat = pd.DataFrame({\n",
    "        col: [np.mean(stacked_df1.iloc[0][col], axis=0)] for col in ['degree_(AnatomicalVar)',\t'betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)' , 'eigenvec_(AnatomicalVar)']\n",
    "    })\n",
    "    # Add 'small_worldness' and 'avg_shortest_path_length' to avrg_Wconf_anat\n",
    "    avrg_Wconf_anat['smallworldness(AnatomicalVar)'] =stacked_df1['smallworldness(AnatomicalVar)']\n",
    "    avrg_Wconf_anat['avg_shortestPathLength(AnatomicalVar)'] = stacked_df1['avg_shortestPathLength(AnatomicalVar)']\n",
    "    avrg_Wconf_anat = avrg_Wconf_anat.rename(columns={'degree_(AnatomicalVar)': 'degree_(AnatomicalVarW)', 'betweeness_(AnatomicalVar)': 'betweeness_(AnatomicalVarW)' , 'clusteringcoef_(AnatomicalVar)':'clusteringcoef_(AnatomicalVarW)'   , 'eigenvec_(AnatomicalVar)':'eigenvec_(AnatomicalVarW)','smallworldness(AnatomicalVar)':'smallworldness(AnatomicalVarW)','avg_shortestPathLength(AnatomicalVar)':'avg_shortestPathLength(AnatomicalVarW)'  })\n",
    "\n",
    "    return avrg_Noconf_anat,avrg_Wconf_anat\n",
    "\n",
    "avrg_Noconf_num,avrg_Wconf_num=compute_average_num(df_Noconf_num,df_Wconf_num)\n",
    "avrg_Noconf_anat,avrg_Wconf_anat=compute_average_anat(df_Noconf_anat,df_Wconf_anat)\n",
    "\n",
    "avrghc_Noconf_num,avrghc_Wconf_num=compute_average_num(dfhc_Noconf_num,dfhc_Wconf_num)\n",
    "avrghc_Noconf_anat,avrghc_Wconf_anat=compute_average_anat(dfhc_Noconf_anat,dfhc_Wconf_anat)\n",
    "\n",
    "# stacked_df1\n",
    "# np.array(stacked_df1.iloc[0]['smallworldness(AnatomicalVar)']).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(avrg_Noconf_anat['betweeness_(AnatomicalVar)'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(avrghc_Noconf_anat['betweeness_(AnatomicalVar)'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(arr,col):\n",
    "    # Create the 'region' column with labels\n",
    "    \n",
    "    regions = [f\"region_{i+1}\" for i in range(len(arr))]\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'region': regions, f'{col}': arr})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# import pandas as pd\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# # List of metrics\n",
    "# metrics = ['degree', 'betweeness', 'eigenvec', 'clusteringcoef']\n",
    "\n",
    "# # Create subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=len(metrics), \n",
    "#     cols=1,  \n",
    "#     vertical_spacing=0.05  \n",
    "#     # row_titles=[\"x\",\"y\",\"z\",\"t\"],\n",
    "#     # subplot_titles=[\n",
    "#     #     f\"Degree Centrality with anatomical mean = {avr_BetweenSubject['degree_(AnatomicalVar)'].mean()}\",\n",
    "#     #     f\"Betweeness Centrality with anatomical mean = {avr_BetweenSubject['betweeness_(AnatomicalVar)'].mean()}\",\n",
    "#     #     f\"Eigen Vector Centrality with anatomical mean = {avr_BetweenSubject['eigenvec_(AnatomicalVar)'].mean()}\",\n",
    "#     #     f\"Clustering Coefficient with anatomical mean = {avr_BetweenSubject['clusteringcoef_(AnatomicalVar)'].mean()}\"\n",
    "#     # ]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# for i, metric in enumerate(metrics):\n",
    "#     # Extract columns for each metric\n",
    "#     columns_to_extract_bet1 =  [col for col in avrg_Wconf_anat.columns if metric in col]\n",
    "#     columns_to_extract_with1 = [col for col in avrg_Wconf_num.columns if metric in col]\n",
    "#     statics_bet1 = change(avrg_Wconf_anat[columns_to_extract_bet1].iloc[0][avrg_Wconf_anat[columns_to_extract_bet1].columns[0]],avrg_Wconf_anat[columns_to_extract_bet1].columns[0])\n",
    "#     statics_with1 = change(avrg_Wconf_num[columns_to_extract_with1].iloc[0][avrg_Wconf_num[columns_to_extract_with1].columns[0]],avrg_Wconf_num[columns_to_extract_with1].columns[0])\n",
    "\n",
    "#     columns_to_extract_bet = [col for col in avrg_Noconf_anat.columns if metric in col]\n",
    "#     columns_to_extract_with =  [col for col in avrg_Noconf_num.columns if metric in col]\n",
    "#     statics_bet = change(avrg_Noconf_anat[columns_to_extract_bet].iloc[0][avrg_Noconf_anat[columns_to_extract_bet].columns[0]],avrg_Noconf_anat[columns_to_extract_bet].columns[0])\n",
    "#     statics_with = change(avrg_Noconf_num[columns_to_extract_with].iloc[0][avrg_Noconf_num[columns_to_extract_with].columns[0]],avrg_Noconf_num[columns_to_extract_with].columns[0])\n",
    "    \n",
    "#     # Melt the DataFrames\n",
    "#     bet_melted1 = statics_bet1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "#     with_melted1 = statics_with1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "\n",
    "#     bet_melted = statics_bet.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "#     with_melted = statics_with.melt(id_vars=['region'], var_name='metric', value_name='value')   \n",
    "#     # Plot box plots for Between Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=bet_melted1['metric'], \n",
    "#             y=bet_melted1['value'],\n",
    "#             showlegend=False,\n",
    "#             name=f\"Between Subject {metric.capitalize()}\",\n",
    "#             boxpoints='all',  # Add stripplot-like points\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(34,139,34)'),\n",
    "#             width=0.2\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "#         # Plot box plots for Between Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=bet_melted['metric'], \n",
    "#             y=bet_melted['value'],\n",
    "#             showlegend=False,\n",
    "#             name=f\"Between Subject {metric.capitalize()}\",\n",
    "#             boxpoints='all',  # Add stripplot-like points\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(34,139,34)'),\n",
    "#             width=0.2\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "#     # Plot box plots for Within Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=with_melted1['metric'], \n",
    "#             y=with_melted1['value'],\n",
    "#             showlegend=False,\n",
    "#             width=0.2,  # Smaller box width,\n",
    "#             name=f\"Within Subject {metric.capitalize()}\",\n",
    "#             boxpoints='suspectedoutliers', # only suspected outliers\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0.1,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(8,81,156)',\n",
    "#                         outliercolor='rgb(8,81,156)',\n",
    "#                         line=dict(\n",
    "#                             outliercolor='rgb(8,81,156)',\n",
    "#                             outlierwidth=2)),\n",
    "            \n",
    "#             text=with_melted1['region'],  # Hover text showing subject ID\n",
    "#             hovertemplate=(\n",
    "#                 \"<b>region:</b> %{text}<br>\"  # Display subject ID\n",
    "#                 \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "#                 \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "#                 \"<extra></extra>\"              # Remove default extra info\n",
    "#         )\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "#         # Plot box plots for Within Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=with_melted['metric'], \n",
    "#             y=with_melted['value'],\n",
    "#             showlegend=False,\n",
    "#             width=0.2,  # Smaller box width,\n",
    "#             name=f\"Within Subject {metric.capitalize()}\",\n",
    "#             boxpoints='suspectedoutliers', # only suspected outliers\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0.1,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(8,81,156)',\n",
    "#                         outliercolor='rgb(8,81,156)',\n",
    "#                         line=dict(\n",
    "#                             outliercolor='rgb(8,81,156)',\n",
    "#                             outlierwidth=2)),\n",
    "            \n",
    "#             text=with_melted['region'],  # Hover text showing subject ID\n",
    "#             hovertemplate=(\n",
    "#                 \"<b>region:</b> %{text}<br>\"  # Display subject ID\n",
    "#                 \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "#                 \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "#                 \"<extra></extra>\"              # Remove default extra info\n",
    "#         )\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title=\"Regional Variability (Local Graph Metric)\",\n",
    "#     #    (title=dict(text=\"Your Title Here\", )x=0.5, y=0.95),  # Center title and move it up\n",
    "#     # margin=dict(t=100)  # Increase top margin to avoid overlap\n",
    "#     height=150 * len(metrics),\n",
    "#     width=900,\n",
    "#     showlegend=True,\n",
    "#     legend_title=\"Legend\",\n",
    "#     margin=dict(l=50, r=50, t=50, b=100),\n",
    "#     template='plotly'\n",
    "# )\n",
    "# for row in range(1, len(metrics) + 1):\n",
    "#     fig.update_yaxes(tickformat=\".1e\", row=row, col=1)\n",
    "\n",
    "# # Iterate through rows to remove x-axis tick labels for all subplots\n",
    "# for row in range(1, len(metrics) + 1):\n",
    "#     fig.update_xaxes(showticklabels=False, row=row, col=1)\n",
    "\n",
    "# fig.update_xaxes(\n",
    "#     #title_text=\"Anatomical Variability with confounds\" + \"&nbsp;\" *10 +  \"Anatomical Variability No confounds\" + \"&nbsp;\" *10  + \"Numerical Variability with confounds\" + \"&nbsp;\" *10 + \"Numerical Variability No confounds\",\n",
    "#     row=len(metrics),\n",
    "#     col=1,\n",
    "#     side='bottom',\n",
    "#     title_font=dict(size=11)  # Adjust the font size here\n",
    "\n",
    "# )\n",
    "# fig.add_annotation(\n",
    "#     text=\"Degree Centrality\",\n",
    "#     xref=\"paper\", yref=\"paper\",\n",
    "#     x=0.0, y=1.038,  # Centered horizontally above the first two rows\n",
    "#     showarrow=False,\n",
    "# )\n",
    "\n",
    "# fig.add_annotation(\n",
    "#     text=\"Betweeness Centrality\",\n",
    "#     xref=\"paper\", yref=\"paper\",\n",
    "#     x=0.0, y=0.79 , # Centered horizontally above the last two rows\n",
    "#     showarrow=False,\n",
    "# )\n",
    "# fig.add_annotation(\n",
    "#     text=\"Eigen vector Centrality\",\n",
    "#     xref=\"paper\", yref=\"paper\",\n",
    "#     x=0.0, y=0.5,  # Centered horizontally above the last two rows\n",
    "#     showarrow=False,\n",
    "# )\n",
    "\n",
    "# fig.add_annotation(\n",
    "#     text=\"Clustering coefficient\",\n",
    "#     xref=\"paper\", yref=\"paper\",\n",
    "#     x=0.0, y=0.23,  # Centered horizontally above the last two rows\n",
    "#     showarrow=False,\n",
    "# )\n",
    "# fig.add_annotation(\n",
    "#     x=0.0,  # Adjust position as needed\n",
    "#     y=-0.05,  # Adjust to place below the x-axis\n",
    "#     xref=\"paper\", yref=\"paper\",\n",
    "#     text=\"Anatomical Variability with confounds\" + \"&nbsp;\" * 10 +\n",
    "#          \"Anatomical Variability No confounds\" + \"&nbsp;\" * 10 +\n",
    "#          \"Numerical Variability with confounds\" + \"&nbsp;\" * 10 +\n",
    "#          \"Numerical Variability No confounds\",\n",
    "#     showarrow=False,\n",
    "#     font=dict(size=11),\n",
    "#     align=\"center\"\n",
    "# )\n",
    "\n",
    "# fig.write_image(\"LocalMetricplot.png\", scale=3)\n",
    "\n",
    "\n",
    "# # # Remove annotations for avr_bet\n",
    "# # import plotly.io as pio\n",
    "# # pio.write_html(fig, \"LocalMetricplot.html\")\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# List of metrics\n",
    "metrics = ['degree', 'betweeness', 'eigenvec', 'clusteringcoef']\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), \n",
    "    cols=1,  \n",
    "    vertical_spacing=0.05\n",
    ")\n",
    "\n",
    "# Colors for different groups\n",
    "colors = {\n",
    "    'pd_anat_with_conf': 'rgb(34,139,34)',      # Dark Green\n",
    "    'pd_anat_no_conf': 'rgb(60,179,60)',        # Light Green\n",
    "    'pd_num_with_conf': 'rgb(8,81,156)',        # Dark Blue\n",
    "    'pd_num_no_conf': 'rgb(49,130,189)',        # Light Blue\n",
    "    'hc_anat_with_conf': 'rgb(178,34,34)',           # Dark Red\n",
    "    'hc_anat_no_conf': 'rgb(255,69,69)',             # Light Red\n",
    "    'hc_num_with_conf': 'rgb(148,0,211)',            # Dark Purple\n",
    "    'hc_num_no_conf': 'rgb(186,85,211)'              # Light Purple\n",
    "}\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # Patient group data processing\n",
    "    # With confounds - Anatomical\n",
    "    columns_to_extract_bet1 = [col for col in avrg_Wconf_anat.columns if metric in col]\n",
    "    statics_bet1 = change(avrg_Wconf_anat[columns_to_extract_bet1].iloc[0][avrg_Wconf_anat[columns_to_extract_bet1].columns[0]], avrg_Wconf_anat[columns_to_extract_bet1].columns[0])\n",
    "    bet_melted1 = statics_bet1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    \n",
    "    # With confounds - Numerical\n",
    "    columns_to_extract_with1 = [col for col in avrg_Wconf_num.columns if metric in col]\n",
    "    statics_with1 = change(avrg_Wconf_num[columns_to_extract_with1].iloc[0][avrg_Wconf_num[columns_to_extract_with1].columns[0]], avrg_Wconf_num[columns_to_extract_with1].columns[0])\n",
    "    with_melted1 = statics_with1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    \n",
    "    # No confounds - Anatomical\n",
    "    columns_to_extract_bet = [col for col in avrg_Noconf_anat.columns if metric in col]\n",
    "    statics_bet = change(avrg_Noconf_anat[columns_to_extract_bet].iloc[0][avrg_Noconf_anat[columns_to_extract_bet].columns[0]], avrg_Noconf_anat[columns_to_extract_bet].columns[0])\n",
    "    bet_melted = statics_bet.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    \n",
    "    # No confounds - Numerical\n",
    "    columns_to_extract_with = [col for col in avrg_Noconf_num.columns if metric in col]\n",
    "    statics_with = change(avrg_Noconf_num[columns_to_extract_with].iloc[0][avrg_Noconf_num[columns_to_extract_with].columns[0]], avrg_Noconf_num[columns_to_extract_with].columns[0])\n",
    "    with_melted = statics_with.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    \n",
    "    # Healthy Control group data processing\n",
    "    # With confounds - Anatomical\n",
    "    columns_to_extract_hc_bet1 = [col for col in avrghc_Wconf_anat.columns if metric in col]\n",
    "    statics_hc_bet1 = change(avrghc_Wconf_anat[columns_to_extract_hc_bet1].iloc[0][avrghc_Wconf_anat[columns_to_extract_hc_bet1].columns[0]], avrghc_Wconf_anat[columns_to_extract_hc_bet1].columns[0])\n",
    "    hc_bet_melted1 = statics_hc_bet1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    \n",
    "    # With confounds - Numerical\n",
    "    columns_to_extract_hc_with1 = [col for col in avrghc_Wconf_num.columns if metric in col]\n",
    "    statics_hc_with1 = change(avrghc_Wconf_num[columns_to_extract_hc_with1].iloc[0][avrghc_Wconf_num[columns_to_extract_hc_with1].columns[0]], avrghc_Wconf_num[columns_to_extract_hc_with1].columns[0])\n",
    "    hc_with_melted1 = statics_hc_with1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    \n",
    "    # No confounds - Anatomical\n",
    "    columns_to_extract_hc_bet = [col for col in avrghc_Noconf_anat.columns if metric in col]\n",
    "    statics_hc_bet = change(avrghc_Noconf_anat[columns_to_extract_hc_bet].iloc[0][avrghc_Noconf_anat[columns_to_extract_hc_bet].columns[0]], avrghc_Noconf_anat[columns_to_extract_hc_bet].columns[0])\n",
    "    hc_bet_melted = statics_hc_bet.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    \n",
    "    # No confounds - Numerical\n",
    "    columns_to_extract_hc_with = [col for col in avrghc_Noconf_num.columns if metric in col]\n",
    "    statics_hc_with = change(avrghc_Noconf_num[columns_to_extract_hc_with].iloc[0][avrghc_Noconf_num[columns_to_extract_hc_with].columns[0]], avrghc_Noconf_num[columns_to_extract_hc_with].columns[0])\n",
    "    hc_with_melted = statics_hc_with.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    \n",
    "    # Adjust x positions for better separation of groups\n",
    "    x_positions = {\n",
    "        'pd_anat_with_conf': 1,\n",
    "        'hc_anat_with_conf':2,\n",
    "        'pd_anat_no_conf': 3,\n",
    "        'hc_anat_no_conf':4,\n",
    "        'pd_num_with_conf': 5,\n",
    "        'hc_num_with_conf':6,\n",
    "        'pd_num_no_conf': 7,\n",
    "        'hc_num_no_conf':8\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Patient group - Anatomical with confounds\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=[x_positions['pd_anat_with_conf']] * len(bet_melted1),\n",
    "            y=bet_melted1['value'],\n",
    "            name=\"Pd Anat W/Conf\",\n",
    "            boxpoints='all',\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(color=colors['pd_anat_with_conf']),\n",
    "            width=0.5,\n",
    "            showlegend=False,  # Show legend only for the first metric\n",
    "            text=bet_melted1['region'],\n",
    "            hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>pd Anat W/Conf</extra>\"\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    \n",
    "    # Patient group - Anatomical no confounds\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=[x_positions['hc_anat_with_conf']] * len(bet_melted),\n",
    "            y=bet_melted['value'],\n",
    "            name=\"hc Anat W/Conf\",\n",
    "            boxpoints='all',\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(color=colors['hc_anat_with_conf']),\n",
    "            width=0.5,\n",
    "            showlegend=False,\n",
    "            text=bet_melted['region'],\n",
    "            hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>hc Anat W/Conf</extra>\"\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    \n",
    "    # Patient group - Numerical with confounds\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=[x_positions['pd_anat_no_conf']] * len(with_melted1),\n",
    "            y=with_melted1['value'],\n",
    "            name=\"pd Anat no/Conf\",\n",
    "            boxpoints='suspectedoutliers',\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                color=colors['pd_anat_no_conf'],\n",
    "                outliercolor=colors['pd_anat_no_conf'],\n",
    "                line=dict(outliercolor=colors['pd_anat_no_conf'], outlierwidth=2)\n",
    "            ),\n",
    "            width=0.5,\n",
    "            showlegend=False,\n",
    "            text=with_melted1['region'],\n",
    "            hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>pd Anat no/Conf</extra>\"\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    \n",
    "    # Patient group - Numerical no confounds\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=[x_positions['hc_anat_no_conf']] * len(with_melted),\n",
    "            y=with_melted['value'],\n",
    "            name=\"hc Anat No/Conf\",\n",
    "            boxpoints='suspectedoutliers',\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                color=colors['hc_anat_no_conf'],\n",
    "                outliercolor=colors['hc_anat_no_conf'],\n",
    "                line=dict(outliercolor=colors['hc_anat_no_conf'], outlierwidth=2)\n",
    "            ),\n",
    "            width=0.5,\n",
    "            showlegend=False,\n",
    "            text=with_melted['region'],\n",
    "            hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>hc Anat No/Conf</extra>\"\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    \n",
    "    # HC group - Anatomical with confounds\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=[x_positions['pd_num_with_conf']] * len(hc_bet_melted1),\n",
    "            y=hc_bet_melted1['value'],\n",
    "            name=\"pd Num W/Conf\",\n",
    "            boxpoints='all',\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(color=colors['pd_num_with_conf']),\n",
    "            width=0.5,\n",
    "            showlegend=False,\n",
    "            text=hc_bet_melted1['region'],\n",
    "            hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>pd Num W/Conf</extra>\"\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    \n",
    "    # HC group - Anatomical no confounds\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=[x_positions['hc_num_with_conf']] * len(hc_bet_melted),\n",
    "            y=hc_bet_melted['value'],\n",
    "            name=\"HC Num W/Conf\",\n",
    "            boxpoints='all',\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(color=colors['hc_num_no_conf']),\n",
    "            width=0.5,\n",
    "            showlegend=False,\n",
    "            text=hc_bet_melted['region'],\n",
    "            hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>HC Num W/Conf</extra>\"\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    \n",
    "    # HC group - Numerical with confounds\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=[x_positions['pd_num_no_conf']] * len(hc_with_melted1),\n",
    "            y=hc_with_melted1['value'],\n",
    "            name=\"pd Num No/Conf\",\n",
    "            boxpoints='suspectedoutliers',\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                color=colors['pd_num_no_conf'],\n",
    "                outliercolor=colors['pd_num_no_conf'],\n",
    "                line=dict(outliercolor=colors['pd_num_with_conf'], outlierwidth=2)\n",
    "            ),\n",
    "            width=0.5,\n",
    "            showlegend=False,\n",
    "            text=hc_with_melted1['region'],\n",
    "            hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>pd Num No/Conf</extra>\"\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    \n",
    "    # HC group - Numerical no confounds\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=[x_positions['hc_num_no_conf']] * len(hc_with_melted),\n",
    "            y=hc_with_melted['value'],\n",
    "            name=\"HC Num No/Conf\",\n",
    "            boxpoints='suspectedoutliers',\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                color=colors['hc_num_no_conf'],\n",
    "                outliercolor=colors['hc_num_no_conf'],\n",
    "                line=dict(outliercolor=colors['hc_num_no_conf'], outlierwidth=2)\n",
    "            ),\n",
    "            width=0.5,\n",
    "            showlegend=False,\n",
    "            text=hc_with_melted['region'],\n",
    "            hovertemplate=\"<b>Region:</b> %{text}<br><b>Value:</b> %{y}<br><extra>HC Num No/Conf</extra>\"\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Regional Variability (Local Graph Metric) - Pd vs. Healthy Controls\",\n",
    "    height=200 * len(metrics),  # Increased height to accommodate more data\n",
    "    width=1200,  # Increased width\n",
    "    showlegend=False,\n",
    "    # legend_title=\"Groups\",\n",
    "    # legend=dict(\n",
    "    #     orientation=\"h\",\n",
    "    #     yanchor=\"bottom\",\n",
    "    #     y=-0.15,\n",
    "    #     xanchor=\"center\",\n",
    "    #     x=0.5\n",
    "    # ),\n",
    "    margin=dict(l=50, r=50, t=80, b=150),  # Increased bottom margin for legend\n",
    "    template='plotly'\n",
    ")\n",
    "\n",
    "# Update y-axes with scientific notation format\n",
    "for row in range(1, len(metrics) + 1):\n",
    "    fig.update_yaxes(tickformat=\".1e\", row=row, col=1)\n",
    "\n",
    "# Create custom x-axis ticks\n",
    "x_ticks = list(x_positions.values())\n",
    "x_tick_labels = [\n",
    "    \"pd Anat W/Conf\", \"hc Anat No/Conf\", \n",
    "    \"pd Num W/Conf\", \"hc Num No/Conf\",\n",
    "    \"pd Anat W/Conf\", \"hc Anat No/Conf\", \n",
    "    \"pd Num W/Conf\", \"hc Num No/Conf\"\n",
    "]\n",
    "\n",
    "# Remove x-axis tick labels for all but the last subplot\n",
    "for row in range(1, len(metrics)):\n",
    "    fig.update_xaxes(showticklabels=False, row=row, col=1)\n",
    "\n",
    "# Add x-axis labels only to the bottom subplot\n",
    "fig.update_xaxes(\n",
    "    tickvals=x_ticks,\n",
    "    ticktext=x_tick_labels,\n",
    "    tickangle=45,\n",
    "    row=len(metrics),\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add annotations for metric names on the left side\n",
    "metric_labels = [\"Degree Centrality\", \"Betweeness Centrality\", \"Eigen vector Centrality\", \"Clustering coefficient\"]\n",
    "y_positions = [1.038,0.79,0.5,0.23]  # Adjusted for 4 metrics\n",
    "\n",
    "for i, (label, y_pos) in enumerate(zip(metric_labels, y_positions)):\n",
    "    fig.add_annotation(\n",
    "        text=label,\n",
    "        xref=\"paper\", yref=\"paper\",\n",
    "        x=0.0, y=y_pos,\n",
    "        showarrow=False,\n",
    "        xanchor=\"left\"\n",
    "    )\n",
    "\n",
    "# Save figure\n",
    "fig.write_image(\"LocalMetricPlot_Patients_vs_HC.png\", scale=3)\n",
    "fig.write_html(\"LocalMetricPlot_Patients_vs_HC.html\")\n",
    "\n",
    "# Show figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # List of metrics\n",
    "# metrics = ['degree', 'betweeness', 'eigenvec', 'clusteringcoef']\n",
    "\n",
    "# # Create subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=len(metrics), \n",
    "#     cols=1,  \n",
    "#     vertical_spacing=0.05   \n",
    "# )\n",
    "\n",
    "# # Function to add jitter\n",
    "# def add_jitter(x_values, scale=0.2):\n",
    "#     return x_values + np.random.uniform(-scale, scale, size=len(x_values))\n",
    "\n",
    "# for i, metric in enumerate(metrics):\n",
    "#     # Extract columns for each metric\n",
    "#     columns_to_extract_bet1 = [col for col in avrg_Wconf_anat.columns if metric in col]\n",
    "#     columns_to_extract_with1 = [col for col in avrg_Wconf_num.columns if metric in col]\n",
    "#     statics_bet1 = change(avrg_Wconf_anat[columns_to_extract_bet1].iloc[0][avrg_Wconf_anat[columns_to_extract_bet1].columns[0]],avrg_Wconf_anat[columns_to_extract_bet1].columns[0])\n",
    "#     statics_with1 = change(avrg_Wconf_num[columns_to_extract_with1].iloc[0][avrg_Wconf_num[columns_to_extract_with1].columns[0]],avrg_Wconf_num[columns_to_extract_with1].columns[0])\n",
    "\n",
    "#     columns_to_extract_bet = [col for col in avrg_Noconf_anat.columns if metric in col]\n",
    "#     columns_to_extract_with = [col for col in avrg_Noconf_num.columns if metric in col]\n",
    "#     statics_bet = change(avrg_Noconf_anat[columns_to_extract_bet].iloc[0][avrg_Noconf_anat[columns_to_extract_bet].columns[0]],avrg_Noconf_anat[columns_to_extract_bet].columns[0])\n",
    "#     statics_with = change(avrg_Noconf_num[columns_to_extract_with].iloc[0][avrg_Noconf_num[columns_to_extract_with].columns[0]],avrg_Noconf_num[columns_to_extract_with].columns[0])\n",
    "\n",
    "#     # Melt the DataFrames\n",
    "#     bet_melted1 = statics_bet1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "#     with_melted1 = statics_with1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "\n",
    "#     bet_melted = statics_bet.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "#     with_melted = statics_with.melt(id_vars=['region'], var_name='metric', value_name='value')   \n",
    "    \n",
    "#     # Assign x positions with jitter for each dataset\n",
    "#     x_bet1 = add_jitter(np.full_like(bet_melted1['value'], i * 4 + 0))\n",
    "#     x_bet = add_jitter(np.full_like(bet_melted['value'], i * 4 + 1))\n",
    "#     x_with1 = add_jitter(np.full_like(with_melted1['value'], i * 4 + 2))\n",
    "#     x_with = add_jitter(np.full_like(with_melted['value'], i * 4 + 3))\n",
    "\n",
    "#     # Plot scatter for Between Subject (confounds)\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_bet1, \n",
    "#             y=bet_melted1['value'],\n",
    "#             mode='markers',\n",
    "#             marker=dict(color='rgb(34,139,34)', size=8, opacity=0.7),\n",
    "#             name=f\"Between Subject {metric.capitalize()} (Confounds)\",\n",
    "#             text=bet_melted1['region'],\n",
    "#             hoverinfo=\"text+y\"\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "\n",
    "#     # Plot scatter for Between Subject (No confounds)\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_bet, \n",
    "#             y=bet_melted['value'],\n",
    "#             mode='markers',\n",
    "#             marker=dict(color='rgb(34,139,34)', size=8, opacity=0.7),\n",
    "#             name=f\"Between Subject {metric.capitalize()} (No Confounds)\",\n",
    "#             text=bet_melted['region'],\n",
    "#             hoverinfo=\"text+y\"\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "\n",
    "#     # Plot scatter for Within Subject (confounds)\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_with1, \n",
    "#             y=with_melted1['value'],\n",
    "#             mode='markers',\n",
    "#             marker=dict(color='rgb(8,81,156)', size=8, opacity=0.7),\n",
    "#             name=f\"Within Subject {metric.capitalize()} (Confounds)\",\n",
    "#             text=with_melted1['region'],\n",
    "#             hoverinfo=\"text+y\"\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "\n",
    "#     # Plot scatter for Within Subject (No confounds)\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=x_with, \n",
    "#             y=with_melted['value'],\n",
    "#             mode='markers',\n",
    "#             marker=dict(color='rgb(8,81,156)', size=8, opacity=0.7),\n",
    "#             name=f\"Within Subject {metric.capitalize()} (No Confounds)\",\n",
    "#             text=with_melted['region'],\n",
    "#             hoverinfo=\"text+y\"\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title=\"Anatomical and Numerical Variability (Local Graph Metric)\",\n",
    "#     height=300 * len(metrics),\n",
    "#     showlegend=True,\n",
    "#     legend_title=\"Legend\",\n",
    "#     margin=dict(l=50, r=50, t=50, b=100),\n",
    "#     template='plotly'\n",
    "# )\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' has already been extracted\n",
    "dfG = df_Noconf_num[(df_Noconf_num['acquisition'] == 'acq-RL') & \n",
    "                             (df_Noconf_num['session'] == '1')]\n",
    "\n",
    "dfGNo_num = dfG.drop(columns=['degree_(numericalVar)',\t'betweeness_(numericalVar)',\t'clusteringcoef_(numericalVar)'\t,'eigenvec_(numericalVar)' ])# Remove the unnecessary column\n",
    "\n",
    "\n",
    "\n",
    "##########################################################################################3\n",
    "# Assuming 'df' has already been extracted\n",
    "df1G= df_Wconf_num[(df_Wconf_num['acquisition'] == 'acq-RL') & \n",
    "                             (df_Wconf_num['session'] == '1')]\n",
    "\n",
    "# df1 = df1.drop(columns=['session', 'acquisition', 'subject'])\n",
    "df1GW_num = df1G.drop(columns=['degree_(numericalVar)',\t'betweeness_(numericalVar)',\t'clusteringcoef_(numericalVar)'\t,'eigenvec_(numericalVar)' ])# Remove the unnecessary column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' has already been extracted\n",
    "dfG = df_Noconf_anat[(df_Noconf_anat['acquisition'] == 'acq-RL') & \n",
    "                             (df_Noconf_anat['session'] == '1')]\n",
    "\n",
    "dfGNo_anat = dfG.drop(columns=['degree_(AnatomicalVar)'\t,'betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)'\t,'eigenvec_(AnatomicalVar)'])\n",
    "\n",
    "\n",
    "###########################################################################################3\n",
    "# Assuming 'df' has already been extracted\n",
    "df1G = df_Wconf_anat[(df_Wconf_anat['acquisition'] == 'acq-RL') & \n",
    "                             (df_Wconf_anat['session'] == '1')]\n",
    "\n",
    "df1GW_anat= df1G.drop(columns=['degree_(AnatomicalVar)'\t,'betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)'\t,'eigenvec_(AnatomicalVar)'])\n",
    "df1GW_num = df1GW_num.rename(columns={'smallworldness(numericalVar)':'smallworldness(numericalVarW)','avg_shortestPathLength(numericalVar)':'avg_shortestPathLength(numericalVarW)'})\n",
    "\n",
    "df1GW_anat = df1GW_anat.rename(columns={'smallworldness(AnatomicalVar)':'smallworldness(AnatomicalVarW)','avg_shortestPathLength(AnatomicalVar)':'avg_shortestPathLength(AnatomicalVarW)'  })\n",
    "\n",
    "df1GW_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# import pandas as pd\n",
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "# # List of metrics\n",
    "# metrics = ['smallworldness', 'avg_shortestPathLength']\n",
    "\n",
    "# # Create subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=len(metrics), \n",
    "#     cols=1,  \n",
    "#     vertical_spacing=0.05,     \n",
    "#     # subplot_titles=[\n",
    "#     #     f\"Small worldness with anatomical mean = {avr_BetweenSubject['smallworldness_(AnatomicalVar)'].mean()}\",\n",
    "#     #     f\"Avrg shortest path with anatomical mean = {avr_BetweenSubject['avg_shortestPathLength_(AnatomicalVar)'].mean()}\"\n",
    "#     # ]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# for i, metric in enumerate(metrics):\n",
    "\n",
    "#     # Extract columns for each metric\n",
    "#     columns_to_extract_bet1 = ['iteration'] + [col for col in df1GW_anat.columns if metric in col]\n",
    "#     columns_to_extract_with1 = ['subject'] + [col for col in df1GW_num.columns if metric in col]\n",
    "    \n",
    "#     statics_bet1 = df1GW_anat[columns_to_extract_bet1]\n",
    "#     statics_with1 = df1GW_num[columns_to_extract_with1]\n",
    "\n",
    "#     columns_to_extract_bet = ['iteration'] + [col for col in dfGNo_anat.columns if metric in col]\n",
    "#     columns_to_extract_with = ['subject'] + [col for col in dfGNo_num.columns if metric in col]\n",
    "    \n",
    "#     statics_bet = dfGNo_anat[columns_to_extract_bet]\n",
    "#     statics_with = dfGNo_num[columns_to_extract_with]\n",
    "#     bet_melted1 = statics_bet1.melt(id_vars=['iteration'], var_name='metric', value_name='value')\n",
    "#     with_melted1 = statics_with1.melt(id_vars=['subject'], var_name='metric', value_name='value')\n",
    "\n",
    "#     bet_melted = statics_bet.melt(id_vars=['iteration'], var_name='metric', value_name='value')\n",
    "#     with_melted = statics_with.melt(id_vars=['subject'], var_name='metric', value_name='value')  \n",
    "#     # Plot box plots for Between Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=bet_melted1['metric'], \n",
    "#             y=bet_melted1['value'],\n",
    "#             showlegend=False,\n",
    "#             name=f\"Between Subject With confounds{metric.capitalize()}\",\n",
    "#             boxpoints='all',  # Add stripplot-like points\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(34,139,34)'),\n",
    "#             width=0.2\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "#      # Plot box plots for Between Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=bet_melted['metric'], \n",
    "#             y=bet_melted['value'],\n",
    "#             showlegend=False,\n",
    "#             name=f\"Between Subject {metric.capitalize()}\",\n",
    "#             boxpoints='all',  # Add stripplot-like points\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(34,139,34)'),\n",
    "#             width=0.2\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )   \n",
    "#     # Plot box plots for Within Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=with_melted1['metric'], \n",
    "#             y=with_melted1['value'],\n",
    "#             showlegend=False,\n",
    "#             width=0.2,  # Smaller box width,\n",
    "#             name=f\"Within Subject With confounds{metric.capitalize()}\",\n",
    "#             boxpoints='suspectedoutliers', # only suspected outliers\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0.1,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(8,81,156)',\n",
    "#                         outliercolor='rgb(8,81,156)',\n",
    "#                         line=dict(\n",
    "#                             outliercolor='rgb(8,81,156)',\n",
    "#                             outlierwidth=2)),\n",
    "            \n",
    "#             text=with_melted1['subject'],  # Hover text showing subject ID\n",
    "#             hovertemplate=(\n",
    "#                 \"<b>region:</b> %{text}<br>\"  # Display subject ID\n",
    "#                 \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "#                 \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "#                 \"<extra></extra>\"              # Remove default extra info\n",
    "#         )\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "#     # Plot box plots for Within Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=with_melted['metric'], \n",
    "#             y=with_melted['value'],\n",
    "#             showlegend=False,\n",
    "#             width=0.2,  # Smaller box width,\n",
    "#             name=f\"Within Subject {metric.capitalize()}\",\n",
    "#             boxpoints='suspectedoutliers', # only suspected outliers\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0.1,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(8,81,156)',\n",
    "#                         outliercolor='rgb(8,81,156)',\n",
    "#                         line=dict(\n",
    "#                             outliercolor='rgb(8,81,156)',\n",
    "#                             outlierwidth=2)),\n",
    "            \n",
    "#             text=with_melted['subject'],  # Hover text showing subject ID\n",
    "#             hovertemplate=(\n",
    "#                 \"<b>region:</b> %{text}<br>\"  # Display subject ID\n",
    "#                 \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "#                 \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "#                 \"<extra></extra>\"              # Remove default extra info\n",
    "#         )\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title=\"Anatomical and Numerical Variability (Global Graph Metric)\",\n",
    "#     height=300 * len(metrics),\n",
    "#     showlegend=True,\n",
    "#     legend_title=\"Legend\",\n",
    "#     margin=dict(l=50, r=50, t=50, b=100),\n",
    "#     template='plotly'\n",
    "# )\n",
    "\n",
    "# # Iterate through rows to remove x-axis tick labels for all subplots\n",
    "# for row in range(1, len(metrics) + 1):\n",
    "#     fig.update_xaxes(showticklabels=False, row=row, col=1)\n",
    "\n",
    "# fig.update_xaxes(\n",
    "#     title_text=\"Anatomical Variability with confounds\" + \"&nbsp;\" *40 +  \"Anatomical Variability No confounds\" + \"&nbsp;\" *40  + \"Numerical Variability with confounds\" + \"&nbsp;\" *40 + \"Numerical Variability No confounds\",\n",
    "#     row=len(metrics),\n",
    "#     col=1,\n",
    "#     side='bottom'\n",
    "# )\n",
    "# # Add section titles for anatomical and numerical data\n",
    "# fig.add_annotation(\n",
    "#     text=\"Smallworldness\",\n",
    "#     xref=\"paper\", yref=\"paper\",\n",
    "#     x=0.0, y=1.035,  # Centered horizontally above the first two rows\n",
    "#     showarrow=False,\n",
    "# )\n",
    "\n",
    "# fig.add_annotation(\n",
    "#     text=\"Average shortest path\",\n",
    "#     xref=\"paper\", yref=\"paper\",\n",
    "#     x=0.0, y=0.50 , # Centered horizontally above the last two rows\n",
    "#     showarrow=False,\n",
    "# )\n",
    "# import plotly.io as pio\n",
    "# pio.write_html(fig, \"GlobalMetricplot.html\")\n",
    "# # Remove annotations for avr_bet\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdofavrg_with(Results_table) :\n",
    "   # Initialize DataFrames\n",
    "    stdofavr_WithinSubject = pd.DataFrame()  # Std over averaged across runs\n",
    "    avr_WithinSubject = pd.DataFrame()  # Average across runs\n",
    "\n",
    "    # Get unique subjects from the Results_table\n",
    "    subjects =np.unique(Results_table['subject'])\n",
    "\n",
    "\n",
    "    i=0\n",
    "    # Loop over each subject\n",
    "    for subj in subjects: \n",
    "        # Filter the DataFrame for each subject and drop unnecessary columns once\n",
    "        filtered_df = Results_table[Results_table['subject'] == subj]\n",
    "        \n",
    "        # Loop over each unique session and acquisition combination\n",
    "        for session in np.unique(filtered_df['session']):\n",
    "            for acquisition in np.unique(filtered_df['acquisition']):\n",
    "                if acquisition not in ['acq-RLsplit1', 'acq-LRsplit1']:\n",
    "                    avr_degree_centrality, avr_betweeness_centrality, avr_eigen_centrality, avr_clustering_coef = [], [], [], []\n",
    "                    std_degree_centrality, std_betweeness_centrality, std_eigen_centrality, std_clustering_coef = [], [], [], []\n",
    "                    smallworldness, avg_shortestPathLength = [], []                # Loop over each repetition to generate correlation matrices\n",
    "                    for rep in range(1, 11):\n",
    "                        # Filter for each specific repetition\n",
    "                        filtered_rows = filtered_df[\n",
    "                            (filtered_df['session'] == session) &\n",
    "                            (filtered_df['acquisition'] == acquisition) &\n",
    "                            (filtered_df['repetition'] == f'rep-{rep}')\n",
    "                        ]\n",
    "                        if filtered_rows.empty:\n",
    "                            continue\n",
    "                        degree_values = list(filtered_rows['degree_centralities'].values[0].values())\n",
    "                        betweeness_values = list(filtered_rows['betweenness_centralities'].values[0].values())\n",
    "                        eigen_values = list(filtered_rows['eigenvector_centralities'].values[0].values())\n",
    "                        clustering_values = list(filtered_rows['clustering_coefficients'].values[0].values())\n",
    "                        # Append averages for each iteration\n",
    "                        avr_degree_centrality.append(np.mean(degree_values))\n",
    "                        avr_betweeness_centrality.append(np.mean(betweeness_values))\n",
    "                        avr_eigen_centrality.append(np.mean(eigen_values))\n",
    "                        avr_clustering_coef.append(np.mean(clustering_values))\n",
    "                        # Append standard deviations for each\n",
    "                        std_degree_centrality.append(np.std(degree_values))\n",
    "                        std_betweeness_centrality.append(np.std(betweeness_values))\n",
    "                        std_eigen_centrality.append(np.std(eigen_values))\n",
    "                        std_clustering_coef.append(np.std(clustering_values))\n",
    "                        smallworldness.append(filtered_rows['small_worldness'])\n",
    "                        avg_shortestPathLength.append(filtered_rows['avg_shortest_path_length'])\n",
    "                                # Calculate and store statistics for each subject after all iterations\n",
    "                    if not avr_degree_centrality:\n",
    "                        continue\n",
    "                    stdofavr_WithinSubject.loc[i, 'degree_(numericalVar)'] = np.std(avr_degree_centrality)\n",
    "                    stdofavr_WithinSubject.loc[i, 'betweeness_(numericalVar)'] = np.std(avr_betweeness_centrality)\n",
    "                    stdofavr_WithinSubject.loc[i, 'clusteringcoef_(numericalVar)'] = np.std(avr_clustering_coef)\n",
    "                    stdofavr_WithinSubject.loc[i, 'eigenvec_(numericalVar)'] = np.std(avr_eigen_centrality)\n",
    "                    stdofavr_WithinSubject.loc[i, 'smallworldness(numericalVar)'] = np.std(smallworldness)\n",
    "                    stdofavr_WithinSubject.loc[i, 'avg_shortestPathLength(numericalVar)'] = np.std(avg_shortestPathLength)\n",
    "\n",
    "                    stdofavr_WithinSubject.loc[i, 'subject']=subj\n",
    "                    stdofavr_WithinSubject.loc[i, 'session']= session\n",
    "                    stdofavr_WithinSubject.loc[i, 'acquisition']= acquisition\n",
    "                    \n",
    "                    avr_WithinSubject.loc[i, 'degree_(numericalVar)'] = np.mean(avr_degree_centrality)\n",
    "                    avr_WithinSubject.loc[i, 'betweeness_(numericalVar)'] = np.mean(avr_betweeness_centrality)\n",
    "                    avr_WithinSubject.loc[i, 'clusteringcoef_(numericalVar)'] = np.mean(avr_clustering_coef)\n",
    "                    avr_WithinSubject.loc[i, 'eigenvec_(numericalVar)'] = np.mean(avr_eigen_centrality)\n",
    "                    avr_WithinSubject.loc[i, 'smallworldness(numericalVar)'] = np.mean(smallworldness)\n",
    "                    avr_WithinSubject.loc[i, 'avg_shortestPathLength(numericalVar)'] = np.mean(avg_shortestPathLength)\n",
    "                    avr_WithinSubject.loc[i, 'subject']=subj\n",
    "                    avr_WithinSubject.loc[i, 'session']= session\n",
    "                    avr_WithinSubject.loc[i, 'acquisition']= acquisition\n",
    "                    \n",
    "                    i=i+1\n",
    "    stdofavr_WithinSubject=stdofavr_WithinSubject.dropna()\n",
    "    avr_WithinSubject=avr_WithinSubject.dropna()\n",
    "    return stdofavr_WithinSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdofavrg_bet(Results_table):\n",
    "    # Initialize DataFrames for between-subject analysis\n",
    "    stdofavr_BetweenSubject = pd.DataFrame()  # Std over averaged across subjects\n",
    "    avr_BetweenSubject = pd.DataFrame()  # Average across subjects\n",
    "\n",
    "\n",
    "    # Get unique subjects from the Results_table\n",
    "    subjects = np.unique(Results_table['subject'])\n",
    "\n",
    "    # Initialize a list to hold data entries for the final DataFrame\n",
    "    subject_data = []\n",
    "    i=0\n",
    "    # Loop over each repetition\n",
    "    for rep in range(1, 11):\n",
    "        # Filter DataFrame for the current repetition and drop unnecessary columns\n",
    "        filtered_df = Results_table[Results_table['repetition'] == f'rep-{rep}']\n",
    "\n",
    "        # Loop over each session\n",
    "        for session in np.unique(filtered_df['session']):\n",
    "            # Loop over each acquisition type\n",
    "            for acquisition in np.unique(filtered_df['acquisition']):\n",
    "                # Skip unwanted acquisitions\n",
    "                if acquisition in ['acq-RLsplit1', 'acq-LRsplit1']:\n",
    "                    continue\n",
    "                \n",
    "                # Initialize lists to store results for each iteration across subjects\n",
    "                acrosSub_avr_degree_centrality = []\n",
    "                acrosSub_avr_betweeness_centrality = []\n",
    "                acrosSub_avr_eigen_centrality = []\n",
    "                acrosSub_avr_clustering_coef = []\n",
    "                acrosSub_std_degree_centrality = []\n",
    "                acrosSub_std_betweeness_centrality = []\n",
    "                acrosSub_std_eigen_centrality = []\n",
    "                acrosSub_std_clustering_coef = []\n",
    "                acrosSub_smallworld=[]\n",
    "                acrosSub_avrgShortestPathLength=[]\n",
    "                \n",
    "                # Loop over each subject\n",
    "                for subj in subjects:\n",
    "                    # Filter rows for the specific subject, session, and acquisition\n",
    "                    filtered_rows = filtered_df[\n",
    "                        (filtered_df['session'] == session) &\n",
    "                        (filtered_df['acquisition'] == acquisition) &\n",
    "                        (filtered_df['subject'] == subj)\n",
    "                    ]\n",
    "\n",
    "                    if filtered_rows.empty:\n",
    "                        continue\n",
    "                    degree_values = list(filtered_rows['degree_centralities'].values[0].values())\n",
    "                    betweeness_values = list(filtered_rows['betweenness_centralities'].values[0].values())\n",
    "                    eigen_values = list(filtered_rows['eigenvector_centralities'].values[0].values())\n",
    "                    clustering_values = list(filtered_rows['clustering_coefficients'].values[0].values())\n",
    "\n",
    "                    # Append averages for each subject\n",
    "                    acrosSub_avr_degree_centrality.append(np.mean(degree_values))\n",
    "                    acrosSub_avr_betweeness_centrality.append(np.mean(betweeness_values))\n",
    "                    acrosSub_avr_eigen_centrality.append(np.mean(eigen_values))\n",
    "                    acrosSub_avr_clustering_coef.append(np.mean(clustering_values))\n",
    "\n",
    "                    # Append standard deviations for each subject\n",
    "                    acrosSub_std_degree_centrality.append(np.std(degree_values))\n",
    "                    acrosSub_std_betweeness_centrality.append(np.std(betweeness_values))\n",
    "                    acrosSub_std_eigen_centrality.append(np.std(eigen_values))\n",
    "                    acrosSub_std_clustering_coef.append(np.std(clustering_values))\n",
    "                    acrosSub_smallworld.append(filtered_rows['small_worldness'])\n",
    "                    acrosSub_avrgShortestPathLength.append(filtered_rows['avg_shortest_path_length'])\n",
    "                # Calculate and store statistics for each iteration across subjects\n",
    "                if len(acrosSub_avr_degree_centrality)<2:\n",
    "                    continue\n",
    "                if rep==8 and session=='1' and acquisition=='acq-RL':\n",
    "                     print(np.std(acrosSub_avr_degree_centrality))\n",
    "                     print(acrosSub_avr_degree_centrality)\n",
    "                stdofavr_BetweenSubject.loc[i, 'degree_(AnatomicalVar)'] = np.std(acrosSub_avr_degree_centrality)\n",
    "                stdofavr_BetweenSubject.loc[i, 'betweeness_(AnatomicalVar)'] = np.std(acrosSub_avr_betweeness_centrality)\n",
    "                stdofavr_BetweenSubject.loc[i, 'clusteringcoef_(AnatomicalVar)'] = np.std(acrosSub_avr_clustering_coef)\n",
    "                stdofavr_BetweenSubject.loc[i, 'eigenvec_(AnatomicalVar)'] = np.std(acrosSub_avr_eigen_centrality)\n",
    "                stdofavr_BetweenSubject.loc[i, 'smallworldness_(AnatomicalVar)'] = np.std(acrosSub_smallworld)\n",
    "                stdofavr_BetweenSubject.loc[i, 'avg_shortestPathLength_(AnatomicalVar)'] = np.std(acrosSub_avrgShortestPathLength)\n",
    "            \n",
    "                stdofavr_BetweenSubject.loc[i, 'iter']=f'iter_{rep}'\n",
    "                stdofavr_BetweenSubject.loc[i, 'session']= session\n",
    "                stdofavr_BetweenSubject.loc[i, 'acquisition']= acquisition\n",
    "\n",
    "                avr_BetweenSubject.loc[i, 'degree_(AnatomicalVar)'] = np.mean(acrosSub_avr_degree_centrality)\n",
    "                avr_BetweenSubject.loc[i, 'betweeness_(AnatomicalVar)'] = np.mean(acrosSub_avr_betweeness_centrality)\n",
    "                avr_BetweenSubject.loc[i, 'clusteringcoef_(AnatomicalVar)'] = np.mean(acrosSub_avr_clustering_coef)\n",
    "                avr_BetweenSubject.loc[i, 'eigenvec_(AnatomicalVar)'] = np.mean(acrosSub_avr_eigen_centrality)\n",
    "                avr_BetweenSubject.loc[i, 'smallworldness_(AnatomicalVar)'] = np.mean(acrosSub_smallworld)\n",
    "                avr_BetweenSubject.loc[i, 'avg_shortestPathLength_(AnatomicalVar)'] = np.mean(acrosSub_avrgShortestPathLength)\n",
    "\n",
    "                avr_BetweenSubject.loc[i, 'iter']=f'iter_{rep}'\n",
    "                avr_BetweenSubject.loc[i, 'session']= session\n",
    "                avr_BetweenSubject.loc[i, 'acquisition']= acquisition\n",
    "\n",
    "                i=i+1\n",
    "    stdofavr_BetweenSubject=stdofavr_BetweenSubject.dropna()\n",
    "    avr_BetweenSubject=avr_BetweenSubject.dropna()\n",
    "    return stdofavr_BetweenSubject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdofavr_WithinSubject= stdofavrg_with(dfN)\n",
    "stdofavr_BetweenSubject=stdofavrg_bet(dfN)\n",
    "df_stdofavr_WithinSubject= stdofavrg_with(dfW)\n",
    "df_stdofavr_BetweenSubject=stdofavrg_bet(dfW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame\n",
    "s = dfN[(dfN['repetition'] == 'rep-8') & \n",
    "        (dfN['acquisition'] == 'acq-RL') & \n",
    "        (dfN['session'] == '1')]['degree_centralities']\n",
    "\n",
    "# Compute mean of each dictionary row\n",
    "row_means = s.apply(lambda d: pd.Series(d).mean())\n",
    "\n",
    "# Compute standard deviation of those means\n",
    "overall_std = row_means.std()\n",
    "\n",
    "print(\"Standard deviation of row means:\", overall_std)\n",
    "row_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_stdofavr_WithinSubjecthc=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/NumGmetricstd_WConf_batchhc.pkl')\n",
    "df_stdofavr_BetweenSubjecthc=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/AnatGmetricstd_WConf_batchhc.pkl')\n",
    "df_stdofavr_WithinSubjecthcN=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/NumGmetricstd_NoConf_batchhc.pkl')\n",
    "df_stdofavr_BetweenSubjecthcN=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/AnatGmetricstd_NoConf_batchhc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdofavr_BetweenSubject[(stdofavr_BetweenSubject['session']=='1') & (stdofavr_BetweenSubject['acquisition']=='acq-RL')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdofavr_WithinSubject1 = stdofavr_WithinSubject[\n",
    "    (stdofavr_WithinSubject['session'] == '1') & \n",
    "    (stdofavr_WithinSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness(numericalVar)','avg_shortestPathLength(numericalVar)'])\n",
    "\n",
    "# Rename specific columns\n",
    "stdofavr_WithinSubject1 = stdofavr_WithinSubject1.rename(columns={'degree_(numericalVar)': 'degree_(numericalVar1)', 'betweeness_(numericalVar)': 'betweeness_(numericalVar1)' , 'clusteringcoef_(numericalVar)':'clusteringcoef_(numericalVar1)'   , 'eigenvec_(numericalVar)':'eigenvec_(numericalVar1)'  })\n",
    "\n",
    "stdofavr_BetweenSubject1 = stdofavr_BetweenSubject[\n",
    "    (stdofavr_BetweenSubject['session'] == '1') & \n",
    "    (stdofavr_BetweenSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness_(AnatomicalVar)','avg_shortestPathLength_(AnatomicalVar)'])\n",
    "\n",
    "stdofavr_BetweenSubject1 = stdofavr_BetweenSubject1.rename(columns={'degree_(AnatomicalVar)': 'degree_(AnatomicalVar1)', 'betweeness_(AnatomicalVar)': 'betweeness_(AnatomicalVar1)' , 'clusteringcoef_(AnatomicalVar)':'clusteringcoef_(AnatomicalVar1)'   , 'eigenvec_(AnatomicalVar)':'eigenvec_(AnatomicalVar1)'  })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdofavr_WithinSubjectWconf = df_stdofavr_WithinSubject[\n",
    "    (df_stdofavr_WithinSubject['session'] == '1') & \n",
    "    (df_stdofavr_WithinSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness(numericalVar)','avg_shortestPathLength(numericalVar)'])\n",
    "stdofavr_BetweenSubjectWconf = df_stdofavr_BetweenSubject[\n",
    "    (df_stdofavr_BetweenSubject['session'] == '1') & \n",
    "    (df_stdofavr_BetweenSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness_(AnatomicalVar)','avg_shortestPathLength_(AnatomicalVar)'])\n",
    "\n",
    "stdofavr_WithinSubjectWconf = stdofavr_WithinSubjectWconf.rename(columns={'degree_(numericalVar)': 'degree_(numericalVarW)', 'betweeness_(numericalVar)': 'betweeness_(numericalVarW)' , 'clusteringcoef_(numericalVar)':'clusteringcoef_(numericalVarW)'   , 'eigenvec_(numericalVar)':'eigenvec_(numericalVarW)'  })\n",
    "stdofavr_BetweenSubjectWconf = stdofavr_BetweenSubjectWconf.rename(columns={'degree_(AnatomicalVar)': 'degree_(AnatomicalVarW)', 'betweeness_(AnatomicalVar)': 'betweeness_(AnatomicalVarW)' , 'clusteringcoef_(AnatomicalVar)':'clusteringcoef_(AnatomicalVarW)'   , 'eigenvec_(AnatomicalVar)':'eigenvec_(AnatomicalVarW)'  })\n",
    "# ####################3\n",
    "stdofavr_WithinSubjectWconfhc = df_stdofavr_WithinSubjecthc[\n",
    "    (df_stdofavr_WithinSubjecthc['session'] == '1') & \n",
    "    (df_stdofavr_WithinSubjecthc['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness(numericalVar)','avg_shortestPathLength(numericalVar)'])\n",
    "stdofavr_BetweenSubjectWconfhc = df_stdofavr_BetweenSubjecthc[\n",
    "    (df_stdofavr_BetweenSubjecthc['session'] == '1') & \n",
    "    (df_stdofavr_BetweenSubjecthc['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness_(AnatomicalVar)','avg_shortestPathLength_(AnatomicalVar)'])\n",
    "\n",
    "stdofavr_WithinSubjectWconfhc= stdofavr_WithinSubjectWconfhc.rename(columns={'degree_(numericalVar)': 'degree_(numericalVarWhc)', 'betweeness_(numericalVar)': 'betweeness_(numericalVarWhc)' , 'clusteringcoef_(numericalVar)':'clusteringcoef_(numericalVarWhc)'   , 'eigenvec_(numericalVar)':'eigenvec_(numericalVarWhc)'  })\n",
    "stdofavr_BetweenSubjectWconfhc = stdofavr_BetweenSubjectWconfhc.rename(columns={'degree_(AnatomicalVar)': 'degree_(AnatomicalVarWhc)', 'betweeness_(AnatomicalVar)': 'betweeness_(AnatomicalVarWhc)' , 'clusteringcoef_(AnatomicalVar)':'clusteringcoef_(AnatomicalVarWhc)'   , 'eigenvec_(AnatomicalVar)':'eigenvec_(AnatomicalVarWhc)'  })\n",
    "################################################\n",
    "stdofavr_WithinSubjectWconfhcN = df_stdofavr_WithinSubjecthcN[\n",
    "    (df_stdofavr_WithinSubjecthcN['session'] == '1') & \n",
    "    (df_stdofavr_WithinSubjecthcN['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness(numericalVar)','avg_shortestPathLength(numericalVar)'])\n",
    "stdofavr_BetweenSubjectWconfhcN = df_stdofavr_BetweenSubjecthcN[\n",
    "    (df_stdofavr_BetweenSubjecthcN['session'] == '1') & \n",
    "    (df_stdofavr_BetweenSubjecthcN['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness_(AnatomicalVar)','avg_shortestPathLength_(AnatomicalVar)'])\n",
    "\n",
    "stdofavr_WithinSubjectWconfhcN= stdofavr_WithinSubjectWconfhcN.rename(columns={'degree_(numericalVar)': 'degree_(numericalVarWhcN)', 'betweeness_(numericalVar)': 'betweeness_(numericalVarWhcN)' , 'clusteringcoef_(numericalVar)':'clusteringcoef_(numericalVarWhcN)'   , 'eigenvec_(numericalVar)':'eigenvec_(numericalVarWhcN)'  })\n",
    "stdofavr_BetweenSubjectWconfhcN = stdofavr_BetweenSubjectWconfhcN.rename(columns={'degree_(AnatomicalVar)': 'degree_(AnatomicalVarWhcN)', 'betweeness_(AnatomicalVar)': 'betweeness_(AnatomicalVarWhcN)' , 'clusteringcoef_(AnatomicalVar)':'clusteringcoef_(AnatomicalVarWhcN)'   , 'eigenvec_(AnatomicalVar)':'eigenvec_(AnatomicalVarWhcN)'  })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.graph_objects as go\n",
    "# import plotly.express as px\n",
    "# import pandas as pd\n",
    "\n",
    "# # List of metrics\n",
    "# metrics = ['degree', 'betweeness', 'eigenvec', 'clusteringcoef']\n",
    "# stdofavr_BetweenSubjectWconf\n",
    "# # Create subplots\n",
    "# from plotly.subplots import make_subplots\n",
    "# fig = make_subplots(\n",
    "#     rows=len(metrics), \n",
    "#     cols=1,  \n",
    "#     vertical_spacing=0.05,     \n",
    "#     subplot_titles=[\n",
    "#         f\"Degree Centrality with anatomical mean = {avr_BetweenSubject['degree_(AnatomicalVar)'].mean()}\",\n",
    "#         f\"Betweeness Centrality with anatomical mean = {avr_BetweenSubject['betweeness_(AnatomicalVar)'].mean()}\",\n",
    "#         f\"Eigen Vector Centrality with anatomical mean = {avr_BetweenSubject['eigenvec_(AnatomicalVar)'].mean()}\",\n",
    "#         f\"Clustering Coefficient with anatomical mean = {avr_BetweenSubject['clusteringcoef_(AnatomicalVar)'].mean()}\"\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# for i, metric in enumerate(metrics):\n",
    "#     # Extract columns for each metric\n",
    "#     columns_to_extract_bet1 = ['iter'] + [col for col in stdofavr_BetweenSubjectWconf .columns if metric in col]\n",
    "#     columns_to_extract_with1 = ['subject'] + [col for col in stdofavr_WithinSubjectWconf .columns if metric in col]\n",
    "    \n",
    "#     statics_bet1 = stdofavr_BetweenSubjectWconf[columns_to_extract_bet1]\n",
    "#     statics_with1 = stdofavr_WithinSubjectWconf[columns_to_extract_with1]\n",
    "\n",
    "#     columns_to_extract_bet = ['iter'] + [col for col in stdofavr_BetweenSubject1.columns if metric in col]\n",
    "#     columns_to_extract_with = ['subject'] + [col for col in stdofavr_WithinSubject1.columns if metric in col]\n",
    "    \n",
    "#     statics_bet = stdofavr_BetweenSubject1[columns_to_extract_bet]\n",
    "#     statics_with = stdofavr_WithinSubject1[columns_to_extract_with]\n",
    "#     # print(statics_bet)\n",
    "#     # # Calculate standard deviations\n",
    "#     # std_devs_bet = statics_bet.drop(['iter'], axis=1).std()\n",
    "#     # std_devs_with = statics_with.drop(['subject'], axis=1).std()\n",
    "    \n",
    "#     # statics_bet = pd.concat([statics_bet, pd.DataFrame([std_devs_bet], columns=std_devs_bet.index)], ignore_index=True)\n",
    "#     # statics_bet.at[statics_bet.index[-1], 'iter'] = 'overall_std'\n",
    "\n",
    "#     # statics_with = pd.concat([statics_with, pd.DataFrame([std_devs_with], columns=std_devs_with.index)], ignore_index=True)\n",
    "#     # statics_with.at[statics_with.index[-1], 'subject'] = 'overall_std'\n",
    "    \n",
    "#     # Melt the DataFrames\n",
    "#     bet_melted1 = statics_bet1.melt(id_vars=['iter'], var_name='metric', value_name='value')\n",
    "#     with_melted1 = statics_with1.melt(id_vars=['subject'], var_name='metric', value_name='value')\n",
    "\n",
    "#     bet_melted = statics_bet.melt(id_vars=['iter'], var_name='metric', value_name='value')\n",
    "#     with_melted = statics_with.melt(id_vars=['subject'], var_name='metric', value_name='value')   \n",
    "#     # Plot box plots for Between Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=bet_melted1['metric'], \n",
    "#             y=bet_melted1['value'],\n",
    "#             showlegend=False,\n",
    "#             name=f\"Between Subject {metric.capitalize()}\",\n",
    "#             boxpoints='all',  # Add stripplot-like points\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(34,139,34)'),\n",
    "#             width=0.2\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "#         # Plot box plots for Between Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=bet_melted['metric'], \n",
    "#             y=bet_melted['value'],\n",
    "#             showlegend=False,\n",
    "#             name=f\"Between Subject {metric.capitalize()}\",\n",
    "#             boxpoints='all',  # Add stripplot-like points\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(34,139,34)'),\n",
    "#             width=0.2\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "#     # Plot box plots for Within Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=with_melted1['metric'], \n",
    "#             y=with_melted1['value'],\n",
    "#             showlegend=False,\n",
    "#             width=0.2,  # Smaller box width,\n",
    "#             name=f\"Within Subject {metric.capitalize()}\",\n",
    "#             boxpoints='suspectedoutliers', # only suspected outliers\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0.1,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(8,81,156)',\n",
    "#                         outliercolor='rgb(8,81,156)',\n",
    "#                         line=dict(\n",
    "#                             outliercolor='rgb(8,81,156)',\n",
    "#                             outlierwidth=2)),\n",
    "            \n",
    "#             text=with_melted1['subject'],  # Hover text showing subject ID\n",
    "#             hovertemplate=(\n",
    "#                 \"<b>Subject:</b> %{text}<br>\"  # Display subject ID\n",
    "#                 \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "#                 \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "#                 \"<extra></extra>\"              # Remove default extra info\n",
    "#         )\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "#         # Plot box plots for Within Subject\n",
    "#     fig.add_trace(\n",
    "#         go.Box(\n",
    "#             x=with_melted['metric'], \n",
    "#             y=with_melted['value'],\n",
    "#             showlegend=False,\n",
    "#             width=0.2,  # Smaller box width,\n",
    "#             name=f\"Within Subject {metric.capitalize()}\",\n",
    "#             boxpoints='suspectedoutliers', # only suspected outliers\n",
    "#             jitter=0.3,\n",
    "#             pointpos=0.1,\n",
    "#             marker=dict(\n",
    "#                         color='rgb(8,81,156)',\n",
    "#                         outliercolor='rgb(8,81,156)',\n",
    "#                         line=dict(\n",
    "#                             outliercolor='rgb(8,81,156)',\n",
    "#                             outlierwidth=2)),\n",
    "            \n",
    "#             text=with_melted['subject'],  # Hover text showing subject ID\n",
    "#             hovertemplate=(\n",
    "#                 \"<b>Subject:</b> %{text}<br>\"  # Display subject ID\n",
    "#                 \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "#                 \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "#                 \"<extra></extra>\"              # Remove default extra info\n",
    "#         )\n",
    "#         ),\n",
    "#         row=i + 1, col=1\n",
    "#     )\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title=\"Metrics with Anatomical and Numerical Variability\",\n",
    "#     height=150 * len(metrics),\n",
    "#     width=900,\n",
    "#     showlegend=True,\n",
    "#     legend_title=\"Legend\",\n",
    "#     margin=dict(l=50, r=50, t=50, b=100)\n",
    "# )\n",
    "\n",
    "# # Iterate through rows to remove x-axis tick labels for all subplots\n",
    "# for row in range(1, len(metrics) + 1):\n",
    "#     fig.update_xaxes(showticklabels=False, row=row, col=1)\n",
    "\n",
    "# fig.update_xaxes(\n",
    "#     title_text=\"Anatomical Variability with confounds\" + \"&nbsp;\" *10 +  \"Anatomical Variability No confounds\" + \"&nbsp;\" *10  + \"Numerical Variability with confounds\" + \"&nbsp;\" *10 + \"Numerical Variability No confounds\",\n",
    "#     row=len(metrics),\n",
    "#     col=1,\n",
    "#     side='bottom',\n",
    "#     title_font=dict(size=11)  # Adjust the font size here\n",
    "# )\n",
    "# # Remove annotations for avr_bet\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# List of metrics\n",
    "metrics = ['degree', 'betweeness', 'eigenvec', 'clusteringcoef']\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), \n",
    "    cols=1,  \n",
    "    vertical_spacing=0.05,     \n",
    "    # subplot_titles=[\n",
    "    #     f\"Degree Centrality with anatomical mean = {avr_BetweenSubject['degree_(AnatomicalVar)'].mean()}\",\n",
    "    #     f\"Betweeness Centrality with anatomical mean = {avr_BetweenSubject['betweeness_(AnatomicalVar)'].mean()}\",\n",
    "    #     f\"Eigen Vector Centrality with anatomical mean = {avr_BetweenSubject['eigenvec_(AnatomicalVar)'].mean()}\",\n",
    "    #     f\"Clustering Coefficient with anatomical mean = {avr_BetweenSubject['clusteringcoef_(AnatomicalVar)'].mean()}\"\n",
    "    # ]\n",
    ")\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # Extract columns\n",
    "    datasets = [\n",
    "        ('BetweenSubjectWconf', stdofavr_BetweenSubjectWconf),\n",
    "        ('BetweenSubject1', stdofavr_BetweenSubject1),\n",
    "        ('BetweenSubjectWconfhc', stdofavr_BetweenSubjectWconfhc),\n",
    "        ('BetweenSubjectWconfhcN', stdofavr_BetweenSubjectWconfhcN),\n",
    "        ('WithinSubjectWconf', stdofavr_WithinSubjectWconf),\n",
    "        ('WithinSubject1', stdofavr_WithinSubject1),\n",
    "        ('WithinSubjectWconfhc', stdofavr_WithinSubjectWconfhc),\n",
    "        ('WithinSubjectWconfhcN', stdofavr_WithinSubjectWconfhcN)\n",
    "    ]\n",
    "    \n",
    "    colors = ['rgb(34,139,34)', 'rgb(8,81,156)', 'rgb(255,69,0)', 'rgb(75,0,130)']\n",
    "    \n",
    "    for j, (name, df) in enumerate(datasets):\n",
    "        id_var = 'iter' if 'Between' in name else 'subject'\n",
    "        columns_to_extract = [id_var] + [col for col in df.columns if metric in col]\n",
    "        statics = df[columns_to_extract]\n",
    "        melted = statics.melt(id_vars=[id_var], var_name='metric', value_name='value')\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Box(\n",
    "                x=melted['metric'],\n",
    "                y=melted['value'],\n",
    "                showlegend=(i == 0),\n",
    "                name=f\"{name} {metric.capitalize()}\",\n",
    "                boxpoints='suspectedoutliers' if 'Within' in name else 'all',\n",
    "                jitter=0.3,\n",
    "                pointpos=0,\n",
    "                marker=dict(color=colors[j % len(colors)]),\n",
    "                width=0.2\n",
    "            ),\n",
    "            row=i + 1, col=1\n",
    "        )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Metrics with Anatomical and Numerical Variability\",\n",
    "    height=200 * len(metrics),\n",
    "    width=1300,\n",
    "    showlegend=True,\n",
    "    legend_title=\"Legend\",\n",
    "    margin=dict(l=50, r=50, t=50, b=100)\n",
    ")\n",
    "\n",
    "# Hide x-axis labels except for the last row\n",
    "for row in range(1, len(metrics)+1):\n",
    "    fig.update_xaxes(showticklabels=False, row=row, col=1)\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Wconf__anat_PPMI\" + \"\\xa0\" * 10 + \"Noconf_anat_PPMI\"  + \"\\xa0\" * 10 + \"Wconf__anat_HC\" + \"\\xa0\" * 10 + \"Noconf__anat_HC\" + \"\\xa0\" * 10 + \n",
    "    \"Wconf__num_PPMI\" + \"\\xa0\" * 10 + \"Noconf_num_PPMI\"  + \"\\xa0\" * 10 + \"Wconf__num_HC\" + \"\\xa0\" * 10 + \"Noconf__num_HC\"\n",
    "               ,\n",
    "    row=len(metrics),\n",
    "    col=1,\n",
    "    side='bottom',\n",
    "    title_font=dict(size=11)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalMettric_WithinSubject = stdofavr_WithinSubject[\n",
    "    (stdofavr_WithinSubject['session'] == '1') & \n",
    "    (stdofavr_WithinSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition',\t'degree_(numericalVar)','betweeness_(numericalVar)','clusteringcoef_(numericalVar)','eigenvec_(numericalVar)'])\n",
    "\n",
    "\n",
    "globalMettric_BetweenSubject= stdofavr_BetweenSubject[\n",
    "    (stdofavr_BetweenSubject['session'] == '1') & \n",
    "    (stdofavr_BetweenSubject['acquisition'] == 'acq-RL')].drop(columns=['session', 'acquisition','degree_(AnatomicalVar)','betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)','eigenvec_(AnatomicalVar)'])\n",
    "\n",
    "\n",
    "globalMettric_WithinSubjectWconf = df_stdofavr_WithinSubject[\n",
    "    (df_stdofavr_WithinSubject['session'] == '1') & \n",
    "    (df_stdofavr_WithinSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition',\t'degree_(numericalVar)','betweeness_(numericalVar)','clusteringcoef_(numericalVar)','eigenvec_(numericalVar)'])\n",
    "\n",
    "\n",
    "globalMettric_BetweenSubjectWconf= df_stdofavr_BetweenSubject[\n",
    "    (df_stdofavr_BetweenSubject['session'] == '1') & \n",
    "    (df_stdofavr_BetweenSubject['acquisition'] == 'acq-RL')].drop(columns=['session', 'acquisition','degree_(AnatomicalVar)','betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)','eigenvec_(AnatomicalVar)'])\n",
    "globalMettric_WithinSubjectWconf = globalMettric_WithinSubjectWconf.rename(columns={'smallworldness(numericalVar)': 'smallworldness(numericalVarW)', 'avg_shortestPathLength(numericalVar)': 'avg_shortestPathLength(numericalVarW)'   })\n",
    "globalMettric_BetweenSubjectWconf = globalMettric_BetweenSubjectWconf.rename(columns={'smallworldness_(AnatomicalVar)': 'smallworldness_(AnatomicalVarW)', 'avg_shortestPathLength_(AnatomicalVar)': 'avg_shortestPathLength_(AnatomicalVarW)' })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# List of metrics\n",
    "metrics = ['smallworldness', 'avg_shortestPathLength']\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), \n",
    "    cols=1,  \n",
    "    vertical_spacing=0.1,     \n",
    "    subplot_titles=[\n",
    "        f\"Small worldness with anatomical mean = {avr_BetweenSubject['smallworldness_(AnatomicalVar)'].mean()}\",\n",
    "        f\"Avrg shortest path with anatomical mean = {avr_BetweenSubject['avg_shortestPathLength_(AnatomicalVar)'].mean()}\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "\n",
    "        # Extract columns for each metric\n",
    "    columns_to_extract_bet1 = ['iter'] + [col for col in globalMettric_BetweenSubjectWconf.columns if metric in col]\n",
    "    columns_to_extract_with1 = ['subject'] + [col for col in globalMettric_WithinSubjectWconf.columns if metric in col]\n",
    "    \n",
    "    statics_bet1 = globalMettric_BetweenSubjectWconf[columns_to_extract_bet1]\n",
    "    statics_with1 = globalMettric_WithinSubjectWconf[columns_to_extract_with1]\n",
    "    # Extract columns for each metric\n",
    "    columns_to_extract_bet = ['iter'] + [col for col in globalMettric_BetweenSubject.columns if metric in col]\n",
    "    columns_to_extract_with = ['subject'] + [col for col in globalMettric_WithinSubject.columns if metric in col]\n",
    "    \n",
    "    statics_bet = globalMettric_BetweenSubject[columns_to_extract_bet]\n",
    "    statics_with = globalMettric_WithinSubject[columns_to_extract_with]\n",
    "    \n",
    "    # # Calculate standard deviations\n",
    "    # std_devs_bet = statics_bet.drop(['iter'], axis=1).std()\n",
    "    # std_devs_with = statics_with.drop(['subject'], axis=1).std()\n",
    "    \n",
    "    # statics_bet = pd.concat([statics_bet, pd.DataFrame([std_devs_bet], columns=std_devs_bet.index)], ignore_index=True)\n",
    "    # statics_bet.at[statics_bet.index[-1], 'iter'] = 'overall_std'\n",
    "\n",
    "    # statics_with = pd.concat([statics_with, pd.DataFrame([std_devs_with], columns=std_devs_with.index)], ignore_index=True)\n",
    "    # statics_with.at[statics_with.index[-1], 'subject'] = 'overall_std'\n",
    "    \n",
    "    # Melt the DataFrames\n",
    "    bet_melted1 = statics_bet1.melt(id_vars=['iter'], var_name='metric', value_name='value')\n",
    "    with_melted1 = statics_with1.melt(id_vars=['subject'], var_name='metric', value_name='value')\n",
    "    # Melt the DataFrames\n",
    "    bet_melted = statics_bet.melt(id_vars=['iter'], var_name='metric', value_name='value')\n",
    "    with_melted = statics_with.melt(id_vars=['subject'], var_name='metric', value_name='value')\n",
    "    # Plot box plots for Between Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=bet_melted1['metric'], \n",
    "            y=bet_melted1['value'],\n",
    "            showlegend=False,\n",
    "            name=f\"Between Subject With confounds{metric.capitalize()}\",\n",
    "            boxpoints='all',  # Add stripplot-like points\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                        color='rgb(34,139,34)'),\n",
    "            width=0.2\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "     # Plot box plots for Between Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=bet_melted['metric'], \n",
    "            y=bet_melted['value'],\n",
    "            showlegend=False,\n",
    "            name=f\"Between Subject {metric.capitalize()}\",\n",
    "            boxpoints='all',  # Add stripplot-like points\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                        color='rgb(34,139,34)'),\n",
    "            width=0.2\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )   \n",
    "    # Plot box plots for Within Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=with_melted1['metric'], \n",
    "            y=with_melted1['value'],\n",
    "            showlegend=False,\n",
    "            width=0.2,  # Smaller box width,\n",
    "            name=f\"Within Subject With confounds{metric.capitalize()}\",\n",
    "            boxpoints='suspectedoutliers', # only suspected outliers\n",
    "            jitter=0.3,\n",
    "            pointpos=0.1,\n",
    "            marker=dict(\n",
    "                        color='rgb(8,81,156)',\n",
    "                        outliercolor='rgb(8,81,156)',\n",
    "                        line=dict(\n",
    "                            outliercolor='rgb(8,81,156)',\n",
    "                            outlierwidth=2)),\n",
    "            \n",
    "            text=with_melted1['subject'],  # Hover text showing subject ID\n",
    "            hovertemplate=(\n",
    "                \"<b>Subject:</b> %{text}<br>\"  # Display subject ID\n",
    "                \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "                \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "                \"<extra></extra>\"              # Remove default extra info\n",
    "        )\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    # Plot box plots for Within Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=with_melted['metric'], \n",
    "            y=with_melted['value'],\n",
    "            showlegend=False,\n",
    "            width=0.2,  # Smaller box width,\n",
    "            name=f\"Within Subject {metric.capitalize()}\",\n",
    "            boxpoints='suspectedoutliers', # only suspected outliers\n",
    "            jitter=0.3,\n",
    "            pointpos=0.1,\n",
    "            marker=dict(\n",
    "                        color='rgb(8,81,156)',\n",
    "                        outliercolor='rgb(8,81,156)',\n",
    "                        line=dict(\n",
    "                            outliercolor='rgb(8,81,156)',\n",
    "                            outlierwidth=2)),\n",
    "            \n",
    "            text=with_melted['subject'],  # Hover text showing subject ID\n",
    "            hovertemplate=(\n",
    "                \"<b>Subject:</b> %{text}<br>\"  # Display subject ID\n",
    "                \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "                \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "                \"<extra></extra>\"              # Remove default extra info\n",
    "        )\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Metrics with Anatomical and Numerical Variability\",\n",
    "    height=200* len(metrics),\n",
    "    width=900,\n",
    "    showlegend=True,\n",
    "    legend_title=\"Legend\",\n",
    "    margin=dict(l=50, r=50, t=50, b=100)\n",
    ")\n",
    "\n",
    "# Iterate through rows to remove x-axis tick labels for all subplots\n",
    "for row in range(1, len(metrics) + 1):\n",
    "    fig.update_xaxes(showticklabels=False, row=row, col=1)\n",
    "\n",
    "fig.update_xaxes(\n",
    "    #title_text=\"Anatomical Variability with confounds\" + \"&nbsp;\" *10 +  \"Anatomical Variability No confounds\" + \"&nbsp;\" *10  + \"Numerical Variability with confounds\" + \"&nbsp;\" *10 + \"Numerical Variability No confounds\",\n",
    "    row=len(metrics),\n",
    "    col=1,\n",
    "    title_font=dict(size=11)  # Adjust the font size here\n",
    "\n",
    ")\n",
    "\n",
    "# Remove annotations for avr_bet\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['subject_id', 'session','acquisition','rep', 'repp', 'region_label', 'dice_coef']\n",
    "\n",
    "# Load the CSV file into a DataFrame with specified column names\n",
    "df_ROI_atlasinT1W = pd.read_csv('/home/ubuntu/Desktop/Thesis/overlap/allbatches/dice_reg_inverse.csv', header=None, names=column_names)\n",
    "df_ROI_atlasinT1W = df_ROI_atlasinT1W.drop(0).reset_index(drop=True)\n",
    "df_ROI_atlasinT1W\n",
    "df_ROI_atlasinT1W['region_label'] = df_ROI_atlasinT1W['region_label'].astype('int64')\n",
    "df_ROI_atlasinT1W['session'] = df_ROI_atlasinT1W['session'].astype('int64')\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame with specified column names\n",
    "df_ROI_atlasinT1W2Bold = pd.read_csv('/home/ubuntu/Desktop/Thesis/overlap/allbatches/dice_coreg_inverse.csv', header=None, names=column_names)\n",
    "# df1 = pd.read_csv('/home/ubuntu/Desktop/Thesis/overlap/batch2/dice_coreg_inverseadd.csv', header=None, names=column_names)\n",
    "# Drop the first row (column names) from the second DataFrame\n",
    "# df1 = df1[1:].reset_index(drop=True)\n",
    "# df_ROI_atlasinT1W2Bold=pd.concat([df, df1], ignore_index=True)\n",
    "df_ROI_atlasinT1W2Bold = df_ROI_atlasinT1W2Bold.drop(0).reset_index(drop=True)\n",
    "df_ROI_atlasinT1W2Bold['region_label'] = df_ROI_atlasinT1W2Bold['region_label'].astype('int64')\n",
    "df_ROI_atlasinT1W2Bold['session'] = df_ROI_atlasinT1W2Bold['session'].astype('int64')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_stackmatrix_OfDice(result_df):\n",
    "    num_sub =len(result_df['subject_id'].unique()) # number of subjects\n",
    "    num_regions = 100  # number of regions\n",
    "    num_MCApairs = 45  # number of dice coefficient combinations\n",
    "    # Initialize stack matrix to hold the data for each subject\n",
    "    stack_matrix = np.empty((num_sub, num_regions, num_MCApairs))\n",
    "    index_to_subject = {}\n",
    "\n",
    "    for index, sub in enumerate(result_df['subject_id'].unique()[:num_sub]):\n",
    "        Dice_coef = np.zeros((num_regions, num_MCApairs))\n",
    "        # Store the mapping of index to subject ID\n",
    "        index_to_subject[index] = sub  \n",
    "        # Filter the DataFrame for the current subject\n",
    "        subject_data = result_df[result_df['subject_id'] == sub]\n",
    "        # print(sub, subject_data)\n",
    "        for region in range(1, num_regions + 1):\n",
    "            # Get the dice coefficients for the current region and subject\n",
    "            region_data = subject_data[subject_data['region_label'] == region]['dice_coef'].values.tolist()[0]\n",
    "            # Ensure we have the expected number of coefficients (45) for each region\n",
    "            if len(region_data) == num_MCApairs:\n",
    "                Dice_coef[region - 1, :] = region_data\n",
    "            else:\n",
    "                print(f\"Warning: Region {region} for subject {sub} has {len(region_data)} values instead of {num_MCApairs}.\")\n",
    "        \n",
    "        # Assign the computed Dice_coef matrix for the current subject to stack_matrix\n",
    "        stack_matrix[index] = Dice_coef\n",
    "    return stack_matrix,index_to_subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_atlasinT1W=df_ROI_atlasinT1W[(df_ROI_atlasinT1W['session']==1) & (df_ROI_atlasinT1W['acquisition']=='acq-RL')] \n",
    "result_df_atlasinT1W=result_df_atlasinT1W.drop(['session', 'acquisition'], axis=1)\n",
    "# Group the DataFrame by subject ID and aggregate the region_label and dice_coef into lists\n",
    "result_df_atlasinT1W = result_df_atlasinT1W.groupby(['subject_id', 'region_label']).agg({\n",
    "\n",
    "    'dice_coef': list\n",
    "}).reset_index()\n",
    "\n",
    "result_df_ROI_atlasinT1W2Bold=df_ROI_atlasinT1W2Bold[(df_ROI_atlasinT1W2Bold['session']==1) & (df_ROI_atlasinT1W2Bold['acquisition']=='acq-RL')] \n",
    "# Group the DataFrame by subject ID and aggregate the region_label and dice_coef into lists\n",
    "result_df_ROI_atlasinT1W2Bold = result_df_ROI_atlasinT1W2Bold.groupby(['subject_id', 'region_label']).agg({\n",
    "\n",
    "    'dice_coef': list\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Schaefer atlas NIfTI file\n",
    "atlas_path = \"/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_res-01_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.nii.gz\"\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "# Load the TSV file with atlas labels\n",
    "tsv_file_path = \"/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.tsv\"\n",
    "atlas_df = pd.read_csv(tsv_file_path, delimiter='\\t')\n",
    "\n",
    "# Extract labels\n",
    "atlas_labels = atlas_df['name'].tolist()\n",
    "\n",
    "# Get unique region labels (excluding background if label = 0)\n",
    "unique_labels = np.unique(atlas_data)\n",
    "unique_labels = unique_labels[unique_labels > 0]  # Remove background (0)\n",
    "\n",
    "# Compute voxel count per region\n",
    "region_sizes = {int(label): np.sum(atlas_data == label) for label in unique_labels}\n",
    "\n",
    "# Get voxel size in mm (multiply dimensions)\n",
    "voxel_volume = np.prod(atlas_img.header.get_zooms())  # Voxel size in mm\n",
    "\n",
    "# Convert voxel count to mm\n",
    "region_sizes_mm3 = {label: count * voxel_volume for label, count in region_sizes.items()}\n",
    "\n",
    "# Create a DataFrame for clarity\n",
    "region_size_df = pd.DataFrame({\n",
    "    'Region': atlas_labels,\n",
    "    'Label': list(region_sizes.keys()),\n",
    "    'Voxel_Count': list(region_sizes.values()),\n",
    "    'Size_mm3': list(region_sizes_mm3.values())\n",
    "})\n",
    "\n",
    "# Display region sizes\n",
    "print(region_size_df)\n",
    "\n",
    "\n",
    "# Sort regions by size (largest to smallest)\n",
    "region_size_df_sorted = region_size_df.sort_values(by=\"Size_mm3\", ascending=False)\n",
    "\n",
    "# Display sorted region sizes\n",
    "print(region_size_df_sorted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select one acusition for subject\n",
    "stack_matrix_T1,indexT=create_stackmatrix_OfDice(result_df_atlasinT1W)\n",
    "stack_matrix_bold,indexB=create_stackmatrix_OfDice(result_df_ROI_atlasinT1W2Bold)\n",
    "\n",
    "Numerical_min_T1=  np.min(stack_matrix_T1,axis=2)\n",
    "numerical_min_bold=np.min(stack_matrix_bold,axis=2)\n",
    "min_T1=np.mean(Numerical_min_T1,axis=0)\n",
    "min_bold=np.mean(numerical_min_bold,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing worse subjects\n",
    "# Assuming stack_matrix is your (112, 100, 45) matrix\n",
    "average_per_subject = np.mean(stack_matrix_T1, axis=(1, 2))  # Shape will be (112,)\n",
    "top_min_indices = np.argsort(average_per_subject)[:10]\n",
    "print(top_min_indices)\n",
    "average_per_subjectb = np.mean(stack_matrix_bold, axis=(1, 2))  # Shape will be (112,)\n",
    "btop_min_indices = np.argsort(average_per_subjectb)[:10]\n",
    "print(btop_min_indices)\n",
    "\n",
    "filtered_indexT = {k: v for k, v in indexT.items() if k not in top_min_indices}\n",
    "filtered_indexB = {k: v for k, v in indexB.items() if k not in btop_min_indices}\n",
    "\n",
    "stack_matrix_T1_filtered = np.delete(stack_matrix_T1, top_min_indices, axis=0)\n",
    "stack_matrix_bold_filtered = np.delete(stack_matrix_bold, btop_min_indices, axis=0)\n",
    "Numerical_min_T1_filtered=  np.min(stack_matrix_T1_filtered,axis=2)\n",
    "numerical_min_bold_filtered=np.min(stack_matrix_bold_filtered,axis=2)\n",
    "min_T1_filtered=np.mean(Numerical_min_T1_filtered,axis=0)\n",
    "min_bold_filtered=np.mean(numerical_min_bold_filtered,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stack_matrix_bold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, datasets, image\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "def Return_Metric_Img(metric_matrix):\n",
    "    # Load an example atlas, e.g., MNI152 template\n",
    "    # atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100, resolution_mm=2)\n",
    "\n",
    "    # # Assume `metric_values` is a list or array of values for the 100 regions\n",
    "    metric_values = metric_matrix # Replace with your actual metric values\n",
    "\n",
    "    # # Load the brain atlas and get the atlas regions\n",
    "    # atlas_img = nib.load(atlas.maps)\n",
    "    # Load your atlas NIfTI file\n",
    "    atlas='/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_res-01_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.nii.gz'\n",
    "    # Define the path to your TSV file\n",
    "    tsv_file_path = \"/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.tsv\"\n",
    "    atlas_img=nib.load(atlas)\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    # Create an empty image with the same shape as the atlas\n",
    "    metric_img_data = np.zeros(atlas_img.shape)\n",
    "\n",
    "    # Assign metric values to the atlas regions\n",
    "    for i, region in enumerate(np.unique(atlas_img.get_fdata())):\n",
    "        if region != 0:  # Ignore the background (region 0)\n",
    "            metric_img_data[atlas_img.get_fdata() == region] = metric_values[i-1]  # i-1 to skip background\n",
    "\n",
    "    # Create a new NIfTI image with metric values\n",
    "    metric_img = nib.Nifti1Image(metric_img_data, atlas_img.affine)\n",
    "    return metric_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Assuming you have two different metrics as images\n",
    "metric_img1 = Return_Metric_Img(min_T1)\n",
    "metric_img2 = Return_Metric_Img(min_bold)\n",
    "# vmin = min(np.min(min_T1), np.min(min_bold))\n",
    "vmin=0.85\n",
    "vmax = max(np.max(min_T1), np.max(min_bold))\n",
    "\n",
    "print(np.min(min_T1), np.min(min_bold), np.max(min_T1), np.max(min_bold))\n",
    "# Create a subplot with two figures (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 12))\n",
    "\n",
    "# Set color range and colormap\n",
    "vmax = 1\n",
    "cmap = \"Reds_r\"\n",
    "\n",
    "# Plot the first metric with an individual colorbar\n",
    "plotting.plot_stat_map(\n",
    "    metric_img1,\n",
    "    bg_img=datasets.load_mni152_template(),\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(10, 0, 0),\n",
    "    title='average dice scores of Atlas_InT1W ',\n",
    "    cmap=cmap,\n",
    "    axes=axes[0],\n",
    "    vmin=0.85, #np.min(min_T1),\n",
    "    vmax=vmax,\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "# Plot the second metric with an individual colorbar\n",
    "plotting.plot_stat_map(\n",
    "    metric_img2,\n",
    "    bg_img=datasets.load_mni152_template(),\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(10, 0, 0),\n",
    "    title='average dice scores of Atlas_InBold',\n",
    "    cmap=cmap,\n",
    "    axes=axes[1],\n",
    "    vmin=0.85, #np.min(min_bold),\n",
    "    vmax=vmax,\n",
    "    colorbar=False\n",
    ")\n",
    "# Add a common colorbar for both plots with extended tick marks\n",
    "cbar_ax = fig.add_axes([1, 0.25, 0.02, 0.5])\n",
    "norm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "# Generate custom tick marks across the color range\n",
    "ticks = np.linspace(vmin, vmax, 15)  # Adjust the number of ticks here for finer granularity\n",
    "cbar = fig.colorbar(norm, cax=cbar_ax, label='Dice scores', ticks=ticks)\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Define the text and wrap it to a specific width\n",
    "text = (\"Figure 1: Dice scores were computed for each region across each pair of MCA runs. The minimum Dice scores were identified and then the averages were calculated across subjects for each region. The figure shows the average Dice scores for the registered atlas in subject space (top) and BOLD space (bottom).\")\n",
    "wrapped_text = \"\\n\".join(textwrap.wrap(text, width=180))  # Adjust the width as needed\n",
    "\n",
    "# Add the wrapped text to the figure\n",
    "plt.figtext(0.5, -0.05, wrapped_text, ha=\"center\", fontsize=15,  wrap=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Assuming you have two different metrics as images\n",
    "metric_img1_filtered = Return_Metric_Img(min_T1_filtered)\n",
    "metric_img2_filtered = Return_Metric_Img(min_bold_filtered)\n",
    "# vmin = min(np.min(min_T1_filtered), np.min(min_bold_filtered))\n",
    "# vmax = max(np.max(min_T1_filtered), np.max(min_bold_filtered))\n",
    "# vmin = min(np.min(min_T1), np.min(min_bold))\n",
    "vmin=0.89\n",
    "vmax = max(np.max(min_T1), np.max(min_bold))\n",
    "print(np.min(min_T1_filtered), np.min(min_bold_filtered), np.max(min_T1_filtered), np.max(min_bold_filtered))\n",
    "# Create a subplot with two figures (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 12))\n",
    "\n",
    "# Set color range and colormap\n",
    "vmax = 1\n",
    "cmap = \"Reds_r\"\n",
    "\n",
    "# Plot the first metric with an individual colorbar\n",
    "plotting.plot_stat_map(\n",
    "    metric_img1_filtered,\n",
    "    bg_img=datasets.load_mni152_template(),\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(10, 0, 0),\n",
    "    title='average dice scores of Atlas_InT1W ',\n",
    "    cmap=cmap,\n",
    "    axes=axes[0],\n",
    "    vmin=0.89, #np.min(min_T1),\n",
    "    vmax=vmax,\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "# Plot the second metric with an individual colorbar\n",
    "plotting.plot_stat_map(\n",
    "    metric_img2_filtered,\n",
    "    bg_img=datasets.load_mni152_template(),\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(10, 0, 0),\n",
    "    title='average dice scores of Atlas_InBold',\n",
    "    cmap=cmap,\n",
    "    axes=axes[1],\n",
    "    vmin=0.89, #np.min(min_bold),\n",
    "    vmax=vmax,\n",
    "    colorbar=False\n",
    ")\n",
    "# Add a common colorbar for both plots with extended tick marks\n",
    "cbar_ax = fig.add_axes([1, 0.25, 0.02, 0.5])\n",
    "norm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "# Generate custom tick marks across the color range\n",
    "ticks = np.linspace(vmin, vmax, 15)  # Adjust the number of ticks here for finer granularity\n",
    "cbar = fig.colorbar(norm, cax=cbar_ax, label='Dice scores', ticks=ticks)\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Define the text and wrap it to a specific width\n",
    "text = (\"Figure 1: Dice scores were computed for each region across each pair of MCA runs. The minimum Dice scores were identified and then the averages were calculated across subjects for each region. The figure shows the average Dice scores for the registered atlas in subject space (top) and BOLD space (bottom).\")\n",
    "wrapped_text = \"\\n\".join(textwrap.wrap(text, width=180))  # Adjust the width as needed\n",
    "\n",
    "# Add the wrapped text to the figure\n",
    "plt.figtext(0.5, -0.05, wrapped_text, ha=\"center\", fontsize=15,  wrap=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Numerical Variability:  \\forall r \\in \\left[ 1 \\hspace{0.1cm}, R \\right],  \\hspace{0.5cm} \n",
    "\\Delta_{r} = \\text{mean}_{s \\in \\left[ 1 \\hspace{0.1cm}, S \\right]} \\left( \\min_{(p,p^{'}),(p^{''},p^{'''}) \\in \\left[ 1 \\hspace{0.1cm}, P \\right]} \n",
    "\\hspace{0.2cm} D \\left( V_{r,s,(p,p^{'})}, V_{r,s,(p^{''},p^{'''})} \\right) \\right)\n",
    "\\newline\n",
    "\n",
    "\\hspace{0.5cm}R:\\text{Number of Regions}, \\hspace{0.1cm} S: \\text{Number of Subjects}, \\hspace{0.2cm} \\text{and} \\hspace{0.2cm} P: \\text{Number of MCA pairs}\\,(p,p^{'})\n",
    "\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting based on region size\n",
    "avrofMCApairs_T1=np.mean(stack_matrix_T1,axis=2)\n",
    "avrofMCApairs_bold=np.mean(stack_matrix_bold,axis=2)\n",
    "print(avrofMCApairs_T1.shape)\n",
    "\n",
    "# Step 1: Get sorted region indices based on size\n",
    "sorted_indices = region_size_df_sorted['Label'].index.values  # Get indices after sorting\n",
    "sorted_region=region_size_df_sorted['Region'].values\n",
    "print((sorted_region[0]))\n",
    "# Step 2: Apply sorting to the columns of the matrices\n",
    "sorted_avrofMCApairs_T1 = avrofMCApairs_T1[:, sorted_indices]  # Reorder columns\n",
    "sorted_avrofMCApairs_bold = avrofMCApairs_bold[:, sorted_indices]  # Reorder columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = min(sorted_avrofMCApairs_T1.min(),sorted_avrofMCApairs_bold.min())\n",
    "vmax = max(sorted_avrofMCApairs_T1.max(), sorted_avrofMCApairs_bold.max())\n",
    "# Convert index dictionaries to ordered lists of subject IDs\n",
    "indexT_labels = [indexT[i] for i in range(len(indexT))]  # Extract subject IDs from indexT dictionary\n",
    "indexB_labels = [indexB[i] for i in range(len(indexB))]  # Extract subject IDs from indexB dictionary\n",
    "sorted_region=region_size_df_sorted['Region'].values\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sorted_avrofMCApairs_T1,\n",
    "        y=indexT_labels,  # Use extracted subject IDs for BOLD\n",
    "        x=sorted_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"Average\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sorted_avrofMCApairs_bold,\n",
    "        y=indexB_labels,  # Use extracted subject IDs for BOLD\n",
    "        x=sorted_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of average of MCApairs_T1 and average of MCApairs_bold (Shared Color Map)\",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Remove 5 worse subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting after removing worse subjects based on region size\n",
    "avrofMCApairs_T1_filtered=np.mean(stack_matrix_T1_filtered,axis=2)\n",
    "avrofMCApairs_bold_filtered=np.mean(stack_matrix_bold_filtered,axis=2)\n",
    "\n",
    "# Step 1: Get sorted region indices based on size\n",
    "sorted_indices = region_size_df_sorted['Label'].index.values  # Get indices after sorting\n",
    "sorted_region=region_size_df_sorted['Region'].values\n",
    "print((sorted_region[1]))\n",
    "# Step 2: Apply sorting to the columns of the matrices\n",
    "\n",
    "sorted_avrofMCApairs_T1_filtered = avrofMCApairs_T1_filtered[:, sorted_indices]  # Reorder columns\n",
    "sorted_avrofMCApairs_bold_filtered= avrofMCApairs_bold_filtered[:, sorted_indices]  # Reorder columns\n",
    "\n",
    "# Now sorted_avrofMCApairs_T1 and sorted_avrofMCApairs_bold have regions sorted by increasing size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = min(sorted_avrofMCApairs_T1_filtered.min(), sorted_avrofMCApairs_bold_filtered.min())\n",
    "vmax = max(sorted_avrofMCApairs_T1_filtered.max(), sorted_avrofMCApairs_bold_filtered.max())\n",
    "# Convert index dictionaries to ordered lists of subject IDs\n",
    "indexT_labels_filtered = [filtered_indexT[i] for i in filtered_indexT.keys()]  # Extract subject IDs from indexT dictionary\n",
    "indexB_labels_filtered = [filtered_indexB[i] for i in filtered_indexB.keys()]  # Extract subject IDs from indexB dictionary\n",
    "sorted_region=region_size_df_sorted['Region'].values\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sorted_avrofMCApairs_T1_filtered,\n",
    "        y=indexT_labels_filtered,\n",
    "        x=sorted_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"rainbow_r\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sorted_avrofMCApairs_bold_filtered,\n",
    "        y=indexB_labels_filtered,\n",
    "        x=sorted_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of average dice scores of MCApairs_T1 and average of MCApairs_bold across sorted region by size after removing 10 worse subjects from both T1 and Bold\",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting based on region size and average across subjects values\n",
    "import numpy as np\n",
    "\n",
    "# Compute mean across subjects (axis=0)\n",
    "mean_values = np.mean(sorted_avrofMCApairs_T1, axis=0)  \n",
    "mean_valuesb = np.mean(sorted_avrofMCApairs_bold, axis=0)  \n",
    "\n",
    "# Get sorted indices based on mean values\n",
    "sortedavr_indices = np.argsort(mean_values).astype(int)  \n",
    "sortedavr_indicesb = np.argsort(mean_valuesb).astype(int)  \n",
    "\n",
    "# Convert region labels to NumPy array and reorder\n",
    "sorted_region = np.array(region_size_df_sorted['Region'].values)  \n",
    "sortedavr_region = sorted_region[sortedavr_indices]  \n",
    "sortedavr_regionb = sorted_region[sortedavr_indices]  \n",
    "\n",
    "# Reorder matrix columns (regions)\n",
    "sortedavr_avrofMCApairs_T1= sorted_avrofMCApairs_T1[:, sortedavr_indices]\n",
    "sortedavr_avrofMCApairs_bold_T1= sorted_avrofMCApairs_bold[:, sortedavr_indices]\n",
    "sortedavr_avrofMCApairs_bold= sorted_avrofMCApairs_bold[:, sortedavr_indicesb]\n",
    "\n",
    "# Compute mean across regions for each subject (axis=1)\n",
    "Smean_values = np.mean(sortedavr_avrofMCApairs_T1, axis=1)  \n",
    "Smean_valuesb = np.mean(sortedavr_avrofMCApairs_bold, axis=1)  \n",
    "\n",
    "# Get sorted indices for subjects (convert to integer type)\n",
    "Ssortedavr_indices = np.argsort(Smean_values).astype(int)  \n",
    "Ssortedavr_indicesb = np.argsort(Smean_valuesb).astype(int)  \n",
    "\n",
    "# Convert subject labels to NumPy array\n",
    "indexT_labels= np.array([indexT[i] for i in indexT.keys()])  \n",
    "indexB_labels= np.array([indexB[i] for i in indexB.keys()])  \n",
    "\n",
    "# Reorder subject labels **(For reference, NOT for indexing)**\n",
    "sorted_indexT_labels = indexT_labels[Ssortedavr_indices]  \n",
    "sorted_indexB_labels= indexB_labels[Ssortedavr_indicesb]  \n",
    "\n",
    "# sortedavr_stdofMCApairs_T1 = sortedavr_stdofMCApairs_T1[Ssortedavr_indices, :]  \n",
    "# sortedavr_stdofMCApairs_bold= sortedavr_stdofMCApairs_bold[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_T1= sortedavr_avrofMCApairs_T1[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_bold_T1= sortedavr_avrofMCApairs_bold_T1[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_bold= sortedavr_avrofMCApairs_bold[Ssortedavr_indicesb, :]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = 0.85 # min(sortedavr_avrofMCApairs_T1.min(), sortedavr_avrofMCApairs_bold.min())\n",
    "vmax = max(sortedavr_avrofMCApairs_T1.max(), sortedavr_avrofMCApairs_bold_T1.max())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_T1,\n",
    "        y=sorted_indexT_labels,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"rainbow_r\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_bold_T1,\n",
    "        y=sorted_indexT_labels,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of the AVerage of MCA pairs_T1 and Average of MCApairs_bold sorted based on the average dice scores across subject and across regions of T1 \",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = 0.85 # min(sortedavr_avrofMCApairs_T1.min(), sortedavr_avrofMCApairs_bold.min())\n",
    "vmax = max(sortedavr_avrofMCApairs_T1.max(), sortedavr_avrofMCApairs_bold.max())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_T1,\n",
    "        y=sorted_indexT_labels,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"rainbow_r\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_bold,\n",
    "        y=sorted_indexB_labels,\n",
    "        x=sortedavr_regionb,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of the AVerage of MCA pairs_T1 and Average of MCApairs_bold sorted based on the average dice scores across subject and across regions of T1 and bold separatedly \",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute mean across subjects (axis=0)\n",
    "mean_values = np.mean(sorted_avrofMCApairs_T1_filtered, axis=0)  \n",
    "mean_valuesb = np.mean(sorted_avrofMCApairs_bold_filtered, axis=0)  \n",
    "\n",
    "# Get sorted indices based on mean values\n",
    "sortedavr_indices = np.argsort(mean_values).astype(int)  \n",
    "sortedavr_indicesb = np.argsort(mean_valuesb).astype(int)  \n",
    "\n",
    "# Convert region labels to NumPy array and reorder\n",
    "sorted_region = np.array(region_size_df_sorted['Region'].values)  \n",
    "sortedavr_region = sorted_region[sortedavr_indices]  \n",
    "sortedavr_regionb = sorted_region[sortedavr_indicesb]  \n",
    "\n",
    "# Reorder matrix columns (regions)\n",
    "\n",
    "sortedavr_avrofMCApairs_T1_filtered = sorted_avrofMCApairs_T1_filtered[:, sortedavr_indices]\n",
    "sortedavr_avrofMCApairs_bold_T1_filtered = sorted_avrofMCApairs_bold_filtered[:, sortedavr_indices]\n",
    "sortedavr_avrofMCApairs_bold_filtered = sorted_avrofMCApairs_bold_filtered[:, sortedavr_indicesb]\n",
    "\n",
    "# Compute mean across regions for each subject (axis=1)\n",
    "Smean_values = np.mean(sortedavr_avrofMCApairs_T1_filtered, axis=1)  \n",
    "Smean_valuesb = np.mean(sortedavr_avrofMCApairs_bold_filtered, axis=1)  \n",
    "\n",
    "# Get sorted indices for subjects (convert to integer type)\n",
    "Ssortedavr_indices = np.argsort(Smean_values).astype(int)  \n",
    "Ssortedavr_indicesb = np.argsort(Smean_valuesb).astype(int)  \n",
    "\n",
    "# Convert subject labels to NumPy array\n",
    "indexT_labels_filtered = np.array([filtered_indexT[i] for i in filtered_indexT.keys()])  \n",
    "indexB_labels_filtered = np.array([filtered_indexB[i] for i in filtered_indexB.keys()])  \n",
    "\n",
    "# Reorder subject labels **(For reference, NOT for indexing)**\n",
    "sorted_indexT_labels_filtered = indexT_labels_filtered[Ssortedavr_indices]  \n",
    "sorted_indexB_labels_filtered = indexB_labels_filtered[Ssortedavr_indicesb]  \n",
    "\n",
    "\n",
    "sortedavr_avrofMCApairs_T1_filtered = sortedavr_avrofMCApairs_T1_filtered[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_bold_T1_filtered = sortedavr_avrofMCApairs_bold_T1_filtered[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_bold_filtered = sortedavr_avrofMCApairs_bold_filtered[Ssortedavr_indicesb, :]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = 0.85 # min(sortedavr_avrofMCApairs_T1.min(), sortedavr_avrofMCApairs_bold.min())\n",
    "vmax = max(sortedavr_avrofMCApairs_T1_filtered.max(), sortedavr_avrofMCApairs_bold_T1_filtered.max())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_T1_filtered,\n",
    "        y=sorted_indexT_labels_filtered,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"rainbow_r\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_bold_T1_filtered,\n",
    "        y=sorted_indexT_labels_filtered,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of the AVerage of MCA pairs_T1 and Average of MCApairs_bold sorted based on the average dice scores across subject and across regions of T1 \",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = 0.85 #min(sortedavr_avrofMCApairs_T1_filtered.min(), sortedavr_avrofMCApairs_bold_filtered.min())\n",
    "vmax = max(sortedavr_avrofMCApairs_T1_filtered.max(), sortedavr_avrofMCApairs_bold_filtered.max())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_T1_filtered,\n",
    "        y=sorted_indexT_labels_filtered,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"Average\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_bold_filtered,\n",
    "        y=sorted_indexB_labels_filtered,\n",
    "        x=sortedavr_regionb,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of the AVerage of MCA pairs_T1 and Average of MCApairs_bold sorted based on the average dice scores across subject and across regions of T1 and bold separatedly\",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_matrix_T11=create_stackmatrix_OfDice(result_df_atlasinT1W1)\n",
    "stack_matrix_bold1=create_stackmatrix_OfDice(result_df_ROI_atlasinT1W2Bold1)\n",
    "\n",
    "Numerical_min_T11=  np.min(stack_matrix_T11,axis=2)\n",
    "numerical_min_bold1=np.min(stack_matrix_bold1,axis=2)\n",
    "min_T11=np.mean(Numerical_min_T11,axis=0)\n",
    "min_bold1=np.mean(numerical_min_bold1,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- $$\n",
    "Numerical Variability:  \\forall r \\in \\left[ 1 \\hspace{0.1cm}, R \\right],  \\hspace{0.5cm} \n",
    "\\Delta_{r} = \\text{mean}_{s \\in \\left[ 1 \\hspace{0.1cm}, S \\right]} \\left( \\min_{(p,p^{'}),(p^{''},p^{'''}) \\in \\left[ 1 \\hspace{0.1cm}, P \\right]} \n",
    "\\hspace{0.2cm} D \\left( V_{r,s,(p,p^{'})}, V_{r,s,(p^{''},p^{'''})} \\right) \\right)\n",
    "\\newline\n",
    "Anatomical Variability:  \\forall r \\in \\left[ 1 \\hspace{0.1cm}, R \\right], \\hspace{0.5cm} \\Delta^{'}_{r} = \\text{mean}_{(p,p^{'}) \\in \\left[ 1 \\hspace{0.1cm}, P \\right]} \\left( \\min_{(s,s^{'}) \\in \\left[ 1 \\hspace{0.1cm}, S \\right]} \n",
    "\\hspace{0.2cm} D \\left( V_{r,s,(p,p^{'})}, V_{r,s^{'},(p,p^{'})} \\right) \\right)\\\\\n",
    "\\newline\n",
    "\\hspace{0.5cm}R:\\text{Number of Regions}, \\hspace{0.1cm} S: \\text{Number of Subjects}, \\hspace{0.2cm} \\text{and} \\hspace{0.2cm} P: \\text{Number of MCA pairs}\\,(p,p^{'})\n",
    "\n",
    "\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
