{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U kaleido\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import significantdigits as sd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_tableNoConf=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/IEEE/Results_tableNoConf_IEEE.pkl')\n",
    "Results_tableWConf=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/IEEE/Results_tableWConf_IEEE.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_Noconf_MCA=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/Result_tableNoConf_batch1.pkl')\n",
    "Result_Wconf_MCA=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/Result_tableWConf_batch1.pkl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_metric(df, mode):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    if mode == 'IEEE':\n",
    "        repetition = 2\n",
    "        suffix = '(IEEE)'\n",
    "    else:\n",
    "        repetition = 11\n",
    "        suffix = '(MCA)'\n",
    "\n",
    "    columns = [\n",
    "        f'degree_{suffix}', f'betweeness_{suffix}',\n",
    "        f'clusteringcoef_{suffix}', f'eigenvec_{suffix}',\n",
    "        f'smallworldness{suffix}', f'avg_shortestPathLength{suffix}'\n",
    "        # 'subject', 'session', 'acquisition','reptition'\n",
    "    ]\n",
    "\n",
    "    metric = pd.DataFrame(columns=columns)\n",
    "    metric = metric.astype({col: 'object' for col in columns if col not in ['subject', 'session', 'acquisition']})\n",
    "\n",
    "    subjects = np.unique(df['subject'])\n",
    "    i = 0\n",
    "\n",
    "    for subj in subjects:\n",
    "        filtered_df = df[df['subject'] == subj]\n",
    "\n",
    "        for session in np.unique(filtered_df['session']):\n",
    "            for acquisition in np.unique(filtered_df['acquisition']):\n",
    "                if acquisition not in ['acq-RLsplit1', 'acq-LRsplit1']:\n",
    "\n",
    "                    for rep in range(1, repetition):\n",
    "                        filtered_rows = filtered_df[\n",
    "                            (filtered_df['session'] == session) &\n",
    "                            (filtered_df['acquisition'] == acquisition) &\n",
    "                            (filtered_df['repetition'] == f'rep-{rep}')\n",
    "                        ]\n",
    "                        if filtered_rows.empty:\n",
    "                            continue\n",
    "\n",
    "                        degree_values = list(filtered_rows['degree_centralities'].values[0].values())\n",
    "                        betweeness_values = list(filtered_rows['betweenness_centralities'].values[0].values())\n",
    "                        eigen_values = list(filtered_rows['eigenvector_centralities'].values[0].values())\n",
    "                        clustering_values = list(filtered_rows['clustering_coefficients'].values[0].values())\n",
    "                        smallworld_values = filtered_rows['small_worldness'].values[0]\n",
    "                        shortestpath_values = filtered_rows['avg_shortest_path_length'].values[0]\n",
    "\n",
    "\n",
    "                        row_data = {\n",
    "                            f'degree_{suffix}': degree_values,\n",
    "                            f'betweeness_{suffix}': betweeness_values,\n",
    "                            f'clusteringcoef_{suffix}': clustering_values,\n",
    "                            f'eigenvec_{suffix}': eigen_values,\n",
    "                            f'smallworldness{suffix}': smallworld_values,\n",
    "                            f'avg_shortestPathLength{suffix}': shortestpath_values\n",
    "                            # 'subject': subj,\n",
    "                            # 'session': session,\n",
    "                            # 'acquisition': acquisition,\n",
    "                            # 'repetition':rep\n",
    "                        }\n",
    "\n",
    "                        metric.loc[i] = row_data\n",
    "                        i += 1\n",
    "\n",
    "    return metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resulthc_Noconf_MCA=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/batchhc/Results_tableNoConf.pkl')\n",
    "Resulthc_Wconf_MCA=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/batchhc/Results_tableWConf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_Wconf_ie=collect_metric(Results_tableWConf,mode='IEEE')\n",
    "metric_Noconf_ie=collect_metric(Results_tableNoConf,mode='IEEE')\n",
    "metric_Noconf_MCA=collect_metric(Result_Noconf_MCA,mode='MCA')\n",
    "metric_Wconf_MCA=collect_metric(Result_Wconf_MCA,mode='MCA')\n",
    "metrichc_Noconf_MCA=collect_metric(Resulthc_Noconf_MCA,mode='MCA')\n",
    "metrichc_Wconf_MCA=collect_metric(Resulthc_Wconf_MCA,mode='MCA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose df is your original DataFrame\n",
    "def stacked_metrics(df):\n",
    "    # For the columns with lists (first 4 columns), flatten all lists\n",
    "    # flattened = {\n",
    "    #     col: sum(df[col].tolist(), [])  # flattens list of lists into one list\n",
    "    #     for col in df.columns[:4]       # assuming first 4 columns are list-valued\n",
    "    # }\n",
    "\n",
    "    # # For the scalar columns, concatenate all values into a list\n",
    "    # for col in df.columns[4:]:\n",
    "    #     flattened[col] = df[col].tolist()\n",
    "\n",
    "    # # Convert to DataFrame with one row\n",
    "    # stacked_df = pd.DataFrame([flattened])\n",
    "    # Suppose df is your original DataFrame\n",
    "\n",
    "    # Define list-type metric columns and scalar metric columns\n",
    "    list_columns = df.columns[:4]   # First 4: lists\n",
    "    scalar_columns = df.columns[4:]  # Last 2: scalars\n",
    "\n",
    "    # Stack list-columns into one list of lists\n",
    "    stacked_data = {\n",
    "        col: df[col].tolist() for col in list_columns\n",
    "    }\n",
    "\n",
    "    # Flatten scalar columns into one list\n",
    "    for col in scalar_columns:\n",
    "        stacked_data[col] = df[col].values.tolist()\n",
    "\n",
    "    return stacked_data\n",
    "\n",
    "stackmetric_Wconf_ie=stacked_metrics(metric_Wconf_ie)\n",
    "stackmetric_Noconf_ie=stacked_metrics(metric_Noconf_ie)\n",
    "stackmetric_Noconf_MCA=stacked_metrics(metric_Noconf_MCA)\n",
    "stackmetric_Wconf_MCA=stacked_metrics(metric_Wconf_MCA)\n",
    "stackmetrichc_Noconf_MCA=stacked_metrics(metrichc_Noconf_MCA)\n",
    "stackmetrichc_Wconf_MCA=stacked_metrics(metrichc_Wconf_MCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg_arr_noie=   np.array(stackmetric_Noconf_ie['degree_(IEEE)'])\n",
    "bet_arr_noie=   np.array(stackmetric_Noconf_ie['betweeness_(IEEE)'])\n",
    "clu_arr_noie=   np.array(stackmetric_Noconf_ie['clusteringcoef_(IEEE)'])\n",
    "eigen_arr_noie= np.array(stackmetric_Noconf_ie['eigenvec_(IEEE)'])\n",
    "small_arr_noie= np.array(stackmetric_Noconf_ie['smallworldness(IEEE)'])\n",
    "short_arr_noie= np.array(stackmetric_Noconf_ie['avg_shortestPathLength(IEEE)'])\n",
    "\n",
    "deg_arr_cie=   np.array(stackmetric_Wconf_ie['degree_(IEEE)'])\n",
    "bet_arr_cie=   np.array(stackmetric_Wconf_ie['betweeness_(IEEE)'])\n",
    "clu_arr_cie=   np.array(stackmetric_Wconf_ie['clusteringcoef_(IEEE)'])\n",
    "eigen_arr_cie= np.array(stackmetric_Wconf_ie['eigenvec_(IEEE)'])\n",
    "Results_tableWConf\n",
    "small_arr_cie= np.array(stackmetric_Wconf_ie['smallworldness(IEEE)'])\n",
    "short_arr_cie= np.array(stackmetric_Wconf_ie['avg_shortestPathLength(IEEE)'])\n",
    "\n",
    "deg_arr_cMCA=   np.array(stackmetric_Wconf_MCA['degree_(MCA)'])\n",
    "bet_arr_cMCA=   np.array(stackmetric_Wconf_MCA['betweeness_(MCA)'])\n",
    "clu_arr_cMCA=   np.array(stackmetric_Wconf_MCA['clusteringcoef_(MCA)'])\n",
    "eigen_arr_cMCA= np.array(stackmetric_Wconf_MCA['eigenvec_(MCA)'])\n",
    "small_arr_cMCA= np.array(stackmetric_Wconf_MCA['smallworldness(MCA)'])\n",
    "short_arr_cMCA= np.array(stackmetric_Wconf_MCA['avg_shortestPathLength(MCA)'])\n",
    "\n",
    "deg_arr_noMCA=   np.array(stackmetric_Noconf_MCA['degree_(MCA)'])\n",
    "bet_arr_noMCA=   np.array(stackmetric_Noconf_MCA['betweeness_(MCA)'])\n",
    "clu_arr_noMCA=   np.array(stackmetric_Noconf_MCA['clusteringcoef_(MCA)'])\n",
    "eigen_arr_noMCA= np.array(stackmetric_Noconf_MCA['eigenvec_(MCA)'])\n",
    "small_arr_noMCA= np.array(stackmetric_Noconf_MCA['smallworldness(MCA)'])\n",
    "short_arr_noMCA= np.array(stackmetric_Noconf_MCA['avg_shortestPathLength(MCA)'])\n",
    "\n",
    "deg_arrhc_cMCA=   np.array(stackmetrichc_Wconf_MCA['degree_(MCA)'])\n",
    "bet_arrhc_cMCA=   np.array(stackmetrichc_Wconf_MCA['betweeness_(MCA)'])\n",
    "clu_arrhc_cMCA=   np.array(stackmetrichc_Wconf_MCA['clusteringcoef_(MCA)'])\n",
    "eigen_arrhc_cMCA= np.array(stackmetrichc_Wconf_MCA['eigenvec_(MCA)'])\n",
    "small_arrhc_cMCA= np.array(stackmetrichc_Wconf_MCA['smallworldness(MCA)'])\n",
    "short_arrhc_cMCA= np.array(stackmetrichc_Wconf_MCA['avg_shortestPathLength(MCA)'])\n",
    "\n",
    "deg_arhcr_noMCA=   np.array(stackmetrichc_Noconf_MCA['degree_(MCA)'])\n",
    "bet_arrhc_noMCA=   np.array(stackmetrichc_Noconf_MCA['betweeness_(MCA)'])\n",
    "clu_arrhc_noMCA=   np.array(stackmetrichc_Noconf_MCA['clusteringcoef_(MCA)'])\n",
    "eigen_arrhc_noMCA= np.array(stackmetrichc_Noconf_MCA['eigenvec_(MCA)'])\n",
    "small_arrhc_noMCA= np.array(stackmetrichc_Noconf_MCA['smallworldness(MCA)'])\n",
    "short_arrhc_noMCA= np.array(stackmetrichc_Noconf_MCA['avg_shortestPathLength(MCA)'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_metricacross_MCA(df):\n",
    "    \"\"\"\n",
    "    Calculate standard deviation of network metrics across repetitions for each subject, session, and acquisition.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        Input dataframe containing network metrics for different subjects, sessions, acquisitions and repetitions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with standard deviations of metrics for each subject, session, and acquisition\n",
    "    \"\"\"\n",
    "    # Define columns for metrics\n",
    "    metric_columns = [\n",
    "        'degree_(numericalVar)', 'betweeness_(numericalVar)', \n",
    "        'clusteringcoef_(numericalVar)', 'eigenvec_(numericalVar)', \n",
    "        'smallworldness(numericalVar)', 'avg_shortestPathLength(numericalVar)'\n",
    "    ]\n",
    "    \n",
    "    # Define info columns\n",
    "    info_columns = ['subject', 'session', 'acquisition']\n",
    "    \n",
    "    # Pre-filter to exclude unwanted acquisitions for efficiency\n",
    "    df_filtered = df[~df['acquisition'].isin(['acq-RLsplit1', 'acq-LRsplit1'])]\n",
    "    \n",
    "    # Create empty list to collect results\n",
    "    results = []\n",
    "    \n",
    "    # Group by subject, session, acquisition to avoid nested loops\n",
    "    for (subj, session, acquisition), group in df_filtered.groupby(['subject', 'session', 'acquisition']):\n",
    "        # Collections for each metric across repetitions\n",
    "        all_metrics = {\n",
    "            'degree': [], 'betweeness': [], 'eigen': [], \n",
    "            'clustering': [], 'smallworld': [], 'shortestpath': []\n",
    "        }\n",
    "        \n",
    "        # Collect metrics for all repetitions\n",
    "        for _, row in group.iterrows():\n",
    "            rep_num = row.get('repetition')\n",
    "            if not isinstance(rep_num, str) or not rep_num.startswith('rep-'):\n",
    "                continue\n",
    "                \n",
    "            # Only process if we have the necessary data\n",
    "            if (isinstance(row.get('degree_centralities'), dict) and \n",
    "                isinstance(row.get('betweenness_centralities'), dict) and\n",
    "                isinstance(row.get('eigenvector_centralities'), dict) and\n",
    "                isinstance(row.get('clustering_coefficients'), dict)):\n",
    "                \n",
    "                # Extract metrics\n",
    "                all_metrics['degree'].append(list(row['degree_centralities'].values()))\n",
    "                all_metrics['betweeness'].append(list(row['betweenness_centralities'].values()))\n",
    "                all_metrics['eigen'].append(list(row['eigenvector_centralities'].values()))\n",
    "                all_metrics['clustering'].append(list(row['clustering_coefficients'].values()))\n",
    "                \n",
    "                # Add scalar metrics if they exist\n",
    "                if 'small_worldness' in row:\n",
    "                    all_metrics['smallworld'].append(row['small_worldness'])\n",
    "                if 'avg_shortest_path_length' in row:\n",
    "                    all_metrics['shortestpath'].append(row['avg_shortest_path_length'])\n",
    "        \n",
    "        # Only calculate stats if we have data\n",
    "        if not all_metrics['degree']:\n",
    "            continue\n",
    "            \n",
    "        # Calculate standard deviations\n",
    "        result_row = {\n",
    "            'subject': subj,\n",
    "            'session': session,\n",
    "            'acquisition': acquisition,\n",
    "            'degree_(numericalVar)': all_metrics['degree'],\n",
    "            'betweeness_(numericalVar)': all_metrics['betweeness'],\n",
    "            'eigenvec_(numericalVar)': all_metrics['eigen'],\n",
    "            'clusteringcoef_(numericalVar)': all_metrics['clustering']\n",
    "        }\n",
    "        \n",
    "        # Add scalar metrics if available\n",
    "        if all_metrics['smallworld']:\n",
    "            result_row['smallworldness(numericalVar)'] =all_metrics['smallworld']\n",
    "        if all_metrics['shortestpath']:\n",
    "            result_row['avg_shortestPathLength(numericalVar)'] = all_metrics['shortestpath']\n",
    "            \n",
    "        results.append(result_row)\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    collectedmetric = pd.DataFrame(results)\n",
    "    \n",
    "    # Ensure proper data types for object columns\n",
    "    for col in metric_columns:\n",
    "        if col in collectedmetric.columns:\n",
    "            collectedmetric[col] = collectedmetric[col].astype('object')\n",
    "    \n",
    "    return collectedmetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_Noconf=collect_metricacross_MCA(Results_tableNoConf)\n",
    "collect_Wconf=collect_metricacross_MCA(Results_tableWConf)\n",
    "m = collect_Noconf[\n",
    "    (collect_Noconf['session'] == '1') &\n",
    "    (collect_Noconf['subject'] == 'sub-102419') &\n",
    "    (collect_Noconf['acquisition'] == 'acq-RL')]['degree_(numericalVar)'].values[0]#[0][0]\n",
    "np.array(m).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_mcaNoconf=collect_metricacross_MCA(Result_Noconf_MCA)\n",
    "collect_MCAWconf=collect_metricacross_MCA(Result_Wconf_MCA)\n",
    "collecthc_mcaNoconf=collect_metricacross_MCA(Resulthc_Noconf_MCA)\n",
    "collecthc_MCAWconf=collect_metricacross_MCA(Resulthc_Wconf_MCA)\n",
    "m = collect_mcaNoconf[\n",
    "    (collect_mcaNoconf['session'] == '1') &\n",
    "    (collect_mcaNoconf['subject'] == 'sub-102419') &\n",
    "    (collect_mcaNoconf['acquisition'] == 'acq-RL')\n",
    "]['degree_(numericalVar)'].values[0]\n",
    "# print(m)\n",
    "# # Unwrap the first (and only) element\n",
    "# m = m[0]  # Now m should be shape (10, 100)\n",
    "\n",
    "m = np.array(m)\n",
    "print(m.shape) \n",
    "# len(m)\n",
    "# collect_mcaNoconf[collect_mcaNoconf['session']=='1']\n",
    "# collect_mcaNoconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test(mca,ieee):\n",
    "    ieee=ieee[0]\n",
    "    c1=sum(1 for val in mca if val < ieee)\n",
    "    c2=sum(1 for val in mca if val > ieee)\n",
    "    c_equal = sum(1 for val in mca if val == ieee)\n",
    "    n_sample=10\n",
    "\n",
    "    if (c1 == 0 and c2 == 10) or (c1 == 10 and c2 == 0):\n",
    "        C = 0\n",
    "    if c1==c2==0:\n",
    "        C=10\n",
    "    else:\n",
    "        C = max(c1, c2, c_equal)\n",
    "    \n",
    "    p = (C + 1) / (n_sample +1)\n",
    "    return C,p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_test(mca,ieee):\n",
    "#     ieee=ieee[0]\n",
    "#     c1=sum(1 for val in mca if val < ieee)\n",
    "#     c2=sum(1 for val in mca if val >= ieee)\n",
    "#     if (all(val == ieee for val in mca)):\n",
    "#         C=10\n",
    "#     else:\n",
    "#         C=max(c1,c2)\n",
    "#     p=(C+1)/11\n",
    "#     return C,p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create an empty list to store results\n",
    "results_list = []\n",
    "\n",
    "# Define the metrics we want to process\n",
    "metrics = [\n",
    "    'degree_(numericalVar)',\n",
    "    'betweeness_(numericalVar)',\n",
    "    'eigenvec_(numericalVar)',\n",
    "    'clusteringcoef_(numericalVar)'\n",
    "]\n",
    "\n",
    "collect_mcaNoconf = collect_mcaNoconf[collect_mcaNoconf['session'] == '1']\n",
    "\n",
    "for subj in collect_mcaNoconf['subject'].unique():\n",
    "    subj_df = collect_mcaNoconf[collect_mcaNoconf['subject'] == subj]\n",
    "    \n",
    "    for acq in subj_df['acquisition'].unique():    \n",
    "        acq_df = subj_df[subj_df['acquisition'] == acq]\n",
    "        \n",
    "        # Check if IEEE data is available for this subject and acquisition\n",
    "        ieee_query = collect_Noconf[(collect_Noconf['session']=='1') & \n",
    "                                   (collect_Noconf['subject']==subj) & \n",
    "                                   (collect_Noconf['acquisition']==acq)]\n",
    "        \n",
    "        if len(ieee_query) == 0:\n",
    "            print(f\"No IEEE data available for subject {subj}, acquisition {acq}\")\n",
    "            continue\n",
    "        \n",
    "        # Process each metric\n",
    "        for metric in metrics:\n",
    "            if metric not in acq_df.columns:\n",
    "                print(f\"Metric {metric} not found in MCA data for subject {subj}, acquisition {acq}\")\n",
    "                continue\n",
    "                \n",
    "            if metric not in ieee_query.columns:\n",
    "                print(f\"Metric {metric} not found in IEEE data for subject {subj}, acquisition {acq}\")\n",
    "                continue\n",
    "            \n",
    "            # Get MCA values\n",
    "            try:\n",
    "                mca = np.array(acq_df[metric].values[0])\n",
    "            except (IndexError, TypeError) as e:\n",
    "                print(f\"Error accessing MCA {metric} data for subject {subj}, acquisition {acq}: {e}\")\n",
    "                continue\n",
    "                \n",
    "            # Get IEEE values\n",
    "            try:\n",
    "                ieee = ieee_query[metric].values[0][0]\n",
    "            except (IndexError, TypeError) as e:\n",
    "                print(f\"Error accessing IEEE {metric} data for subject {subj}, acquisition {acq}: {e}\")\n",
    "                continue\n",
    "            \n",
    "            # Process each region\n",
    "            for region in range(100):\n",
    "                try:\n",
    "                    val_mca = mca[:, region]  # Extract values across 10 runs for one region\n",
    "                except IndexError:\n",
    "                    print(f\"Region {region} not available in MCA {metric} data for subject {subj}, acquisition {acq}\")\n",
    "                    continue\n",
    "                \n",
    "                # Check if this region exists in IEEE data\n",
    "                if region < len(ieee):\n",
    "                    val_ieee = [ieee[region]]\n",
    "                    \n",
    "                    # Use the compute_test function\n",
    "                    try:\n",
    "                        c,p_value = compute_test(val_mca, val_ieee)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error computing test for {metric}, region {region}, subject {subj}, acquisition {acq}: {e}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Store results in dictionary\n",
    "                    result_row = {\n",
    "                        'subject': subj,\n",
    "                        'acquisition': acq,\n",
    "                        'metric': metric,\n",
    "                        'region': region,\n",
    "                        'val_mca': val_mca.tolist(),  # Convert numpy array to list for DataFrame storage\n",
    "                        'val_ieee': val_ieee,\n",
    "                        'p_value': p_value,\n",
    "                        'c_value':c\n",
    "                    }\n",
    "                    \n",
    "                    # Append to results list\n",
    "                    results_list.append(result_row)\n",
    "                else:\n",
    "                    print(f\"Region {region} not available in IEEE {metric} data for subject {subj}, acquisition {acq}\")\n",
    "                    continue\n",
    "\n",
    "# Create DataFrame from results list\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Optionally, save the DataFrame to CSV\n",
    "# results_df.to_csv('metric_comparison_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# # Assuming results_df is already created from previous code\n",
    "# # First make sure the DataFrame is sorted appropriately\n",
    "# results_df = results_df.sort_values(by=['subject', 'acquisition', 'metric'])\n",
    "\n",
    "# # Set your significance threshold\n",
    "# alpha_threshold = 0.01\n",
    "\n",
    "# # Create lists to store the corrected results\n",
    "# subjects_list = []\n",
    "# acquisitions_list = []\n",
    "# metrics_list = []\n",
    "# significant_regions_list = []\n",
    "# correction_methods = ['fdr_bh', 'bonferroni', 'holm']\n",
    "\n",
    "# # Process each subject, acquisition, and metric combination separately\n",
    "# for subj in results_df['subject'].unique():\n",
    "#     for acq in results_df[results_df['subject'] == subj]['acquisition'].unique():\n",
    "#         for metric in results_df[(results_df['subject'] == subj) & \n",
    "#                                (results_df['acquisition'] == acq)]['metric'].unique():\n",
    "            \n",
    "#             # Get p-values for this combination\n",
    "#             subset = results_df[(results_df['subject'] == subj) & \n",
    "#                               (results_df['acquisition'] == acq) & \n",
    "#                               (results_df['metric'] == metric)]\n",
    "            \n",
    "#             if len(subset) == 0:\n",
    "#                 continue\n",
    "                \n",
    "#             # Extract p-values\n",
    "#             p_values = subset['p_value'].values\n",
    "#             # Apply multiple comparison corrections\n",
    "#             results_dict = {}\n",
    "#             # for method in correction_methods:\n",
    "#             # Apply correction\n",
    "#             reject, pvals_corrected, _, _ = multipletests(p_values, alpha=alpha_threshold, method='bonferroni')\n",
    "            \n",
    "#             # Find significant regions after correction\n",
    "#             significant_regions = subset.loc[reject, 'region'].tolist()\n",
    "            \n",
    "#             results_dict[f'bonferroni_significant'] = significant_regions\n",
    "#             results_dict[f'bonferroni_count'] = len(significant_regions)\n",
    "            \n",
    "#             # Update the main DataFrame with corrected p-values\n",
    "#             results_df.loc[subset.index, f'p_corrected_bonferroni'] = pvals_corrected\n",
    "#             results_df.loc[subset.index, f'significant_bonferroni'] = reject\n",
    "            \n",
    "#             # Store summary results\n",
    "#             subjects_list.append(subj)\n",
    "#             acquisitions_list.append(acq)\n",
    "#             metrics_list.append(metric)\n",
    "#             significant_regions_list.append(results_dict)\n",
    "\n",
    "# # Create a summary DataFrame\n",
    "# summary_df = pd.DataFrame({\n",
    "#     'subject': subjects_list,\n",
    "#     'acquisition': acquisitions_list,\n",
    "#     'metric': metrics_list,\n",
    "#     'correction_results': significant_regions_list\n",
    "# })\n",
    "\n",
    "# # # Example: Display the number of significant regions using bonferroni correction\n",
    "# # print(\"Summary of significant regions (bonferroni correction):\")\n",
    "# # for _, row in summary_df.iterrows():\n",
    "# #     print(f\"Subject: {row['subject']}, Acquisition: {row['acquisition']}, \"\n",
    "# #           f\"Metric: {row['metric']}, Significant regions: {row['correction_results']['bonferroni_count']}\")\n",
    "\n",
    "# # Update the main results DataFrame with a significance flag (using FDR correction)\n",
    "# results_df['is_significant'] = results_df['p_corrected_bonferroni'] < alpha_threshold\n",
    "\n",
    "# # Optional: Save the resultimport numpy as np\n",
    "\n",
    "# # results_df.to_csv('corrected_results.csv', index=False)\n",
    "# # summary_df.to_csv('significance_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['metric'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_Deg=results_df[(results_df['metric']=='degree_(numericalVar)') & (results_df['acquisition']=='acq-RL') &(results_df['region']==0)]\n",
    "res_df_Deg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = res_df_Deg.apply(lambda row: row['val_mca'] + row['val_ieee'], axis=1)\n",
    "matrix_df = pd.DataFrame(df.tolist())\n",
    "matrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(seed=18)\n",
    "from scipy.stats import friedmanchisquare\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "result=[]\n",
    "for region in range(100):\n",
    "    res_df_Deg=results_df[(results_df['metric']=='degree_(numericalVar)') & (results_df['acquisition']=='acq-RL') &(results_df['region']==region)]\n",
    "    df = res_df_Deg.apply(lambda row: row['val_mca'] + row['val_ieee'], axis=1)\n",
    "    matrix_df = pd.DataFrame(df.tolist())\n",
    "    res = friedmanchisquare(*[matrix_df[col] for col in matrix_df.columns])\n",
    "    result.append(res.pvalue)\n",
    "# Convert to array for easier processing\n",
    "pvals = np.array(result)\n",
    "\n",
    "# Apply FDR correction\n",
    "rej, pvals_corrected, _, _ = multipletests(pvals, alpha=0.05, method='fdr_bh')\n",
    "significant_regions = [i for i, p in enumerate(pvals_corrected) if p < 0.05]\n",
    "print(\"Significant regions (p < 0.05):\", significant_regions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df_Deg=results_df[(results_df['metric']=='degree_(numericalVar)') & (results_df['acquisition']=='acq-RL') &(results_df['region']==95)]\n",
    "df = res_df_Deg.apply(lambda row: row['val_mca'] + row['val_ieee'], axis=1)\n",
    "matrix = pd.DataFrame(df.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# df is your DataFrame with shape (100 subjects, 11 runs)\n",
    "# Each column should represent a run\n",
    "\n",
    "# Perform Nemenyi post-hoc test\n",
    "nemenyi_result = sp.posthoc_nemenyi_friedman(matrix)\n",
    "\n",
    "# Show the result\n",
    "# import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(nemenyi_result, annot=True, cmap='viridis', fmt=\".3f\")\n",
    "plt.title(\"Nemenyi Post-hoc Test: Pairwise Comparisons\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "# def plot_dist(df1,df2,df3,df4,metric):\n",
    "#     # Labels for conditions and colors\n",
    "#     data_list = [\n",
    "#         (df1, \"IEEE Confound\", \"blue\"),\n",
    "#         (df2, \"IEEE No Confound\", \"green\"),\n",
    "#         (df3, \"MCA Confound\", \"red\"),\n",
    "#         (df4, \"MCA No Confound\", \"orange\"),\n",
    "#     ]\n",
    "\n",
    "#     fig = go.Figure()\n",
    "\n",
    "#     for arr, label, color in data_list:\n",
    "#         for region in range(arr.shape[1]):\n",
    "#             fig.add_trace(\n",
    "#                 go.Box(\n",
    "#                     y=arr[:, region],\n",
    "#                     x=[f\"Region {region+1}\"] * arr.shape[0],\n",
    "#                     name=label,\n",
    "#                     boxpoints=\"outliers\",\n",
    "#                     marker_color=color,\n",
    "#                     legendgroup=label,\n",
    "#                     showlegend=(region == 0),  # Show legend only once per group\n",
    "#                     line=dict(width=1),\n",
    "#                     opacity=0.7\n",
    "#                 )\n",
    "#             )\n",
    "\n",
    "#     # Layout adjustments\n",
    "#     fig.update_layout(\n",
    "#         title=f\"Distribution of {metric} Centrality Across 100 Regions\",\n",
    "#         xaxis_title=\"Brain Region\",\n",
    "#         yaxis_title=f\"{metric} Centrality\",\n",
    "#         boxmode=\"group\",\n",
    "#         showlegend=True,\n",
    "#         width=1800,\n",
    "#         height=600\n",
    "#     )\n",
    "\n",
    "#     fig.show()\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Example arrays (replace with real data)\n",
    "# deg_arr_cie = ...\n",
    "# deg_arr_noie = ...\n",
    "# deg_arr_cMCA = ...\n",
    "# deg_arr_noMCA = ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def plot_dist(df1, df2, df3, df4, metric):\n",
    "    num_regions = df1.shape[1]\n",
    "    regions_per_plot = 25\n",
    "    num_subplots = num_regions // regions_per_plot\n",
    "\n",
    "\n",
    "    fig = make_subplots(rows=num_subplots, cols=1, shared_yaxes=True,\n",
    "                        # subplot_titles=[f\"Regions {i*25+1}–{(i+1)*25}\" for i in range(num_subplots)],\n",
    "                        vertical_spacing=0.08)  # Increased spacing for labels\n",
    "\n",
    "    for region in range(num_regions):\n",
    "        subplot_row = region // regions_per_plot + 1\n",
    "        region_in_subplot = region % regions_per_plot\n",
    "        \n",
    "        # Position adjustment for proper overlay\n",
    "        pos_conf = region_in_subplot - 0.2  # Position for Confound group\n",
    "        pos_no_conf = region_in_subplot + 0.2  # Position for No Confound group\n",
    "        \n",
    "        # CONFOUND GROUP - LEFT SIDE\n",
    "        # -------------------------------\n",
    "        # MCA Confound – box\n",
    "        fig.add_trace(go.Box(\n",
    "            y=df3[:, region],\n",
    "            x=[pos_conf] * len(df3[:, region]),\n",
    "            name=\"MCA Confound\" if region == 0 else None,\n",
    "            marker_color=\"red\",\n",
    "            boxpoints=False,\n",
    "            legendgroup=\"MCA Confound\",\n",
    "            showlegend=(region == 0),\n",
    "            line=dict(width=1),\n",
    "            opacity=0.7,\n",
    "            hoverinfo=\"y+name\"\n",
    "        ), row=subplot_row, col=1)\n",
    "\n",
    "        # IEEE Confound – dots overlayed on MCA Confound box\n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=df1[:, region],\n",
    "            x=[pos_conf] * len(df1[:, region]),\n",
    "            name=\"IEEE Confound\" if region == 0 else None,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                color=\"blue\",\n",
    "                opacity=0.5\n",
    "            ),\n",
    "            legendgroup=\"IEEE Confound\",\n",
    "            showlegend=(region == 0),\n",
    "            hoverinfo=\"y+name\"\n",
    "        ), row=subplot_row, col=1)\n",
    "        \n",
    "        # NO CONFOUND GROUP - RIGHT SIDE\n",
    "        # -------------------------------\n",
    "        # MCA No Confound – box\n",
    "        fig.add_trace(go.Box(\n",
    "            y=df4[:, region],\n",
    "            x=[pos_no_conf] * len(df4[:, region]),\n",
    "            name=\"MCA No Confound\" if region == 0 else None,\n",
    "            marker_color=\"orange\",\n",
    "            boxpoints=False,\n",
    "            legendgroup=\"MCA No Confound\",\n",
    "            showlegend=(region == 0),\n",
    "            line=dict(width=1),\n",
    "            opacity=0.7,\n",
    "            hoverinfo=\"y+name\"\n",
    "        ), row=subplot_row, col=1)\n",
    "\n",
    "        # IEEE No Confound – dots overlayed on MCA No Confound box\n",
    "        fig.add_trace(go.Scatter(\n",
    "            y=df2[:, region],\n",
    "            x=[pos_no_conf] * len(df2[:, region]),\n",
    "            name=\"IEEE No Confound\" if region == 0 else None,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                color=\"green\",\n",
    "                opacity=0.5\n",
    "            ),\n",
    "            legendgroup=\"IEEE No Confound\",\n",
    "            showlegend=(region == 0),\n",
    "            hoverinfo=\"y+name\"\n",
    "        ), row=subplot_row, col=1)\n",
    "\n",
    "    # Update layout and axes\n",
    "    fig.update_layout(\n",
    "        title=f\"Distribution of {metric} Centrality Across {num_regions} Regions\",\n",
    "        height=200 * num_subplots,  # Increased height for label visibility\n",
    "        width=2000,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        margin=dict(b=100)  # Add bottom margin for labels\n",
    "    )\n",
    "    \n",
    "    # Set appropriate x-axis tick positions and labels for EVERY row\n",
    "    for i in range(1, num_subplots + 1):\n",
    "        # Calculate region range for this subplot\n",
    "        start_region = regions_per_plot * (i-1)\n",
    "        end_region = min(regions_per_plot * i, num_regions)\n",
    "        \n",
    "        # Update x-axis with tick values and labels\n",
    "        fig.update_xaxes(\n",
    "            row=i, \n",
    "            col=1,\n",
    "            tickmode='array',\n",
    "            tickvals=list(range(regions_per_plot)),  # Positions within the subplot\n",
    "            ticktext=[f\"R{j+1}\" for j in range(start_region, end_region)],  # Actual region labels\n",
    "            tickangle=45,  # Angled labels for better readability on ALL rows\n",
    "            showticklabels=True,  # Show labels on ALL rows\n",
    "            title_text=\"Brain Region\" if i == num_subplots else \"\"  # Only show axis title on last row\n",
    "        )\n",
    "        \n",
    "        # Set y-axis title only on the first row, but show tick labels on all\n",
    "        fig.update_yaxes(\n",
    "            title_text=f\"{metric} Centrality\" if i == 1 else \"\", \n",
    "            row=i, \n",
    "            col=1\n",
    "        )\n",
    "\n",
    "    # Add subtle grid lines for better readability\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(200,200,200,0.2)')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(200,200,200,0.2)')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist(deg_arr_cie,deg_arr_noie,deg_arr_cMCA,deg_arr_noMCA,'Degree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist(bet_arr_cie,bet_arr_noie,bet_arr_cMCA,bet_arr_noMCA,'Betweeness')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist(clu_arr_cie,clu_arr_noie,clu_arr_cMCA,clu_arr_noMCA,'ClustringCoefficient')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist(eigen_arr_cie,eigen_arr_noie,eigen_arr_cMCA,eigen_arr_noMCA,'EigenVector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist1(small_arr_cie,small_arr_noie,small_arr_cMCA,small_arr_noMCA,\"SmallWorldness\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist(short_arr_cie,short_arr_noie,short_arr_cMCA,short_arr_noMCA,'Average_shortestPath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def plot_dist1(df1, df2, df3, df4, metric):\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Position adjustments for proper overlay\n",
    "    pos_conf = 0  # Position for Confound group\n",
    "    pos_no_conf = 1  # Position for No Confound group\n",
    "    \n",
    "    # CONFOUND GROUP - LEFT SIDE\n",
    "    # -------------------------------\n",
    "    # MCA Confound – box\n",
    "    fig.add_trace(go.Box(\n",
    "        y=df3,\n",
    "        x=[pos_conf] * len(df3),\n",
    "        name=\"MCA Confound\",\n",
    "        marker_color=\"red\",\n",
    "        boxpoints=False,\n",
    "        line=dict(width=1),\n",
    "        opacity=0.7,\n",
    "        hoverinfo=\"y+name\"\n",
    "    ))\n",
    "\n",
    "    # IEEE Confound – dots overlayed on MCA Confound box\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=df1,\n",
    "        x=[pos_conf] * len(df1),\n",
    "        name=\"IEEE Confound\",\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=\"blue\",\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        hoverinfo=\"y+name\"\n",
    "    ))\n",
    "    \n",
    "    # NO CONFOUND GROUP - RIGHT SIDE\n",
    "    # -------------------------------\n",
    "    # MCA No Confound – box\n",
    "    fig.add_trace(go.Box(\n",
    "        y=df4,\n",
    "        x=[pos_no_conf] * len(df4),\n",
    "        name=\"MCA No Confound\",\n",
    "        marker_color=\"orange\",\n",
    "        boxpoints=False,\n",
    "        line=dict(width=1),\n",
    "        opacity=0.7,\n",
    "        hoverinfo=\"y+name\"\n",
    "    ))\n",
    "\n",
    "    # IEEE No Confound – dots overlayed on MCA No Confound box\n",
    "    fig.add_trace(go.Scatter(\n",
    "        y=df2,\n",
    "        x=[pos_no_conf] * len(df2),\n",
    "        name=\"IEEE No Confound\",\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            size=5,\n",
    "            color=\"green\",\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        hoverinfo=\"y+name\"\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Distribution of {metric}\",\n",
    "        height=600,\n",
    "        width=800,\n",
    "        showlegend=True,\n",
    "        legend=dict(\n",
    "            orientation=\"h\",\n",
    "            yanchor=\"bottom\",\n",
    "            y=1.02,\n",
    "            xanchor=\"center\",\n",
    "            x=0.5\n",
    "        ),\n",
    "        xaxis=dict(\n",
    "            tickmode='array',\n",
    "            tickvals=[pos_conf, pos_no_conf],\n",
    "            ticktext=[\"Confound\", \"No Confound\"],\n",
    "            title_text=\"Group\"\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title_text=f\"{metric} Value\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add subtle grid lines\n",
    "    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='rgba(200,200,200,0.2)')\n",
    "    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='rgba(200,200,200,0.2)')\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ECDF for each sample\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)  # Sort data\n",
    "    y = np.arange(1, len(x) + 1) / len(x)  # Compute cumulative probability\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import plotly.graph_objects as go\n",
    "# def plot_metric_cdf(df1,df2,df3,df4,metric):\n",
    "#     # Create a Plotly figure\n",
    "#     fig = go.Figure()\n",
    "#     if metric =='SmallWorldness' or metric == 'Average_shortestPath':\n",
    "#         num=1\n",
    "#         df1 = df1.reshape(-1, 1)  # shape becomes (70, 1)\n",
    "#         df2 = df2.reshape(-1, 1)  # shape becomes (70, 1)\n",
    "#         df3 = df3.reshape(-1, 1)  # shape becomes (70, 1)\n",
    "#         df4 = df4.reshape(-1, 1)  # shape becomes (70, 1)\n",
    "\n",
    "#     else:\n",
    "#         num=100\n",
    "#     # Add all ECDFs with transparency for each region\n",
    "#     for idx in range(num):\n",
    "#         xIE_wconf, yIE_wconf = ecdf(df1[:, idx])    # Green\n",
    "#         xIE_noconf, yIE_noconf = ecdf(df2[:, idx]) # Red\n",
    "#         xMCA_wconf, yMCA_wconf = ecdf(df3[:, idx]) # Blue\n",
    "#         xMCA_noconf, yMCA_noconf = ecdf(df4[:, idx]) # Orange\n",
    "\n",
    "#         # Green\n",
    "#         fig.add_trace(go.Scatter(\n",
    "#             x=xIE_wconf, y=yIE_wconf, mode='lines', showlegend=False,\n",
    "#             line=dict(color='rgba(0, 128, 0, 0.02)'), hoverinfo='skip',dash='dash'\n",
    "#         ))\n",
    "\n",
    "#         # Red\n",
    "#         fig.add_trace(go.Scatter(\n",
    "#             x=xIE_noconf, y=yIE_noconf, mode='lines', showlegend=False,\n",
    "#             line=dict(color='rgba(255, 0, 0, 0.02)'), hoverinfo='skip',dash='dash'\n",
    "#         ))\n",
    "\n",
    "#         # Blue\n",
    "#         fig.add_trace(go.Scatter(\n",
    "#             x=xMCA_wconf, y=yMCA_wconf, mode='lines', showlegend=False,\n",
    "#             line=dict(color='rgba(0, 0, 255, 0.02)'), hoverinfo='skip'\n",
    "#         ))\n",
    "\n",
    "#         # Orange\n",
    "#         fig.add_trace(go.Scatter(\n",
    "#             x=xMCA_noconf, y=yMCA_noconf, mode='lines', showlegend=False,\n",
    "#             line=dict(color='rgba(255, 165, 0, 0.02)'), hoverinfo='skip'\n",
    "#         ))\n",
    "\n",
    "#     # Add legend traces\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=[None], y=[None], mode='lines', name='With Confound IEEE',\n",
    "#         line=dict(color='rgb(0, 128, 0)', width=2)\n",
    "#     ))\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=[None], y=[None], mode='lines', name='No Confound IEEE',\n",
    "#         line=dict(color='rgb(255, 0, 0)', width=2)\n",
    "#     ))\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=[None], y=[None], mode='lines', name='With Confound MCA',\n",
    "#         line=dict(color='rgb(0, 0, 255)', width=2)\n",
    "#     ))\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=[None], y=[None], mode='lines', name='No Confound MCA',\n",
    "#         line=dict(color='rgb(255, 165, 0)', width=2)\n",
    "#     ))\n",
    "\n",
    "#     # Update layout\n",
    "#     fig.update_layout(\n",
    "#         title=f\"ECDF of {metric} Centrality values Across All 100 Regions\",\n",
    "#         xaxis_title=f\"{metric} Centrality values\",\n",
    "#         yaxis_title=\"Cumulative Probability\",\n",
    "#         legend_title=\"Condition\",\n",
    "#         template=\"plotly_white\"\n",
    "#     )\n",
    "\n",
    "#     # Show the figure\n",
    "#     fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def ecdf(data):\n",
    "    \"\"\"Compute x, y values for ECDF plot.\"\"\"\n",
    "    x = np.sort(data)\n",
    "    y = np.arange(1, len(x)+1) / len(x)\n",
    "    return x, y\n",
    "\n",
    "def plot_metric_cdf(df1, df2, df3, df4, metric):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    if metric == 'SmallWorldness' or metric == 'Average_shortestPath':\n",
    "        num = 1\n",
    "        df1 = df1.reshape(-1, 1)\n",
    "        df2 = df2.reshape(-1, 1)\n",
    "        df3 = df3.reshape(-1, 1)\n",
    "        df4 = df4.reshape(-1, 1)\n",
    "    else:\n",
    "        num = 100\n",
    "\n",
    "    # --- Plot faint ECDFs for each node ---\n",
    "    for idx in range(num):\n",
    "        xIE_wconf, yIE_wconf = ecdf(df1[:, idx])\n",
    "        xIE_noconf, yIE_noconf = ecdf(df2[:, idx])\n",
    "        xMCA_wconf, yMCA_wconf = ecdf(df3[:, idx])\n",
    "        xMCA_noconf, yMCA_noconf = ecdf(df4[:, idx])\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=xIE_wconf, y=yIE_wconf, mode='lines', showlegend=False,\n",
    "                                 line=dict(color='rgba(27, 158, 119, 0.1)', width=0.5), hoverinfo='skip'))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=xIE_noconf, y=yIE_noconf, mode='lines', showlegend=False,\n",
    "                                 line=dict(color='rgba(217, 95, 2, 0.1)', width=0.5), hoverinfo='skip'))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=xMCA_wconf, y=yMCA_wconf, mode='lines', showlegend=False,\n",
    "                                 line=dict(color='rgba(117, 112, 179, 0.1)', width=0.5), hoverinfo='skip'))\n",
    "\n",
    "        fig.add_trace(go.Scatter(x=xMCA_noconf, y=yMCA_noconf, mode='lines', showlegend=False,\n",
    "                                 line=dict(color='rgba(231, 41, 138, 0.1)', width=0.5), hoverinfo='skip'))\n",
    "\n",
    "    # --- Plot mean ECDF for each group ---\n",
    "    def add_mean_ecdf(data, color, name):\n",
    "        x_sorted = np.sort(data, axis=0)         # shape: (70, num)\n",
    "        x_mean = np.mean(x_sorted, axis=1)\n",
    "        y_vals = np.linspace(0, 1, len(x_mean))\n",
    "        fig.add_trace(go.Scatter(x=x_mean, y=y_vals, mode='lines', name=name,\n",
    "                                 line=dict(color=color, width=2.5)))\n",
    "\n",
    "    add_mean_ecdf(df1, 'rgb(27, 158, 119)', 'With Confound IEEE')\n",
    "    add_mean_ecdf(df2, 'rgb(217, 95, 2)',   'No Confound IEEE')\n",
    "    add_mean_ecdf(df3, 'rgb(117, 112, 179)', 'With Confound MCA')\n",
    "    add_mean_ecdf(df4, 'rgb(231, 41, 138)',  'No Confound MCA')\n",
    "\n",
    "    # --- Layout ---\n",
    "    fig.update_layout(\n",
    "        title=f\"ECDF of {metric} Centrality values Across All {num if num > 1 else 'Subjects'}\",\n",
    "        xaxis_title=f\"{metric} Values\",\n",
    "        yaxis_title=\"Cumulative Probability\",\n",
    "        legend_title=\"Condition\",\n",
    "        template=\"plotly_white\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_cdf(deg_arr_cie,deg_arr_noie,deg_arr_cMCA,deg_arr_noMCA,'Degree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metric_cdf(deg_arr_cie,deg_arr_noie,deg_arr_cMCA,deg_arr_noMCA,'Degree')\n",
    "plot_metric_cdf(bet_arr_cie,bet_arr_noie,bet_arr_cMCA,bet_arr_noMCA,'Betweeness')\n",
    "plot_metric_cdf(clu_arr_cie,clu_arr_noie,clu_arr_cMCA,clu_arr_noMCA,'ClustringCoefficient')\n",
    "plot_metric_cdf(eigen_arr_cie,eigen_arr_noie,eigen_arr_cMCA,eigen_arr_noMCA,'EigenVector')\n",
    "plot_metric_cdf(small_arr_cie,small_arr_noie,small_arr_cMCA,small_arr_noMCA,\"SmallWorldness\")\n",
    "plot_metric_cdf(short_arr_cie,short_arr_noie,short_arr_cMCA,short_arr_noMCA,'Average_shortestPath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a subset to plot (e.g., first 10 ECDFs)\n",
    "num_to_plot = 10 # Adjust as needed\n",
    "selected_indices = [3724,2584,3550]\n",
    "\n",
    "# Get a list of colors from Plotly's qualitative palette\n",
    "colors = px.colors.qualitative.Plotly\n",
    "\n",
    "# Create a Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add ECDF traces for selected elements\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    color = colors[i % len(colors)]  # Cycle through colors\n",
    "    x_wconf, y_wconf = ecdf(subj_valwconfsig_matrix[idx])\n",
    "    x_noconf, y_noconf = ecdf(subj_valNoconfsig_matrix[idx])\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=x_wconf, y=y_wconf, mode='lines', name=f'With Confound {idx}', line=dict(color=color)))\n",
    "    fig.add_trace(go.Scatter(x=x_noconf, y=y_noconf, mode='lines', name=f'No Confound {idx}', line=dict(color=color, dash='dash')))\n",
    "\n",
    "# Update layout for better readability\n",
    "fig.update_layout(\n",
    "    title=\"ECDF of Significant digits  Values across all 4949 Regions pairs\",\n",
    "    xaxis_title=\" Significant digits Value\",\n",
    "    yaxis_title=\"Cumulative Probability\",\n",
    "    legend_title=\"Legend\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Show the figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' has already been extracted\n",
    "df = df_Noconf_num[(df_Noconf_num['acquisition'] == 'acq-RL') & \n",
    "                             (df_Noconf_num['session'] == '1')]\n",
    "\n",
    "df = df.drop(columns=['session', 'acquisition', 'subject' # Remove the unnecessary column\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a new DataFrame for the final stacked result\n",
    "stacked_row = {}\n",
    "\n",
    "# Loop through each column\n",
    "for col in df.columns:\n",
    "    # If column contains matrices or arrays, stack them vertically\n",
    "    stacked_row[col] = np.vstack(df[col].values)\n",
    "\n",
    "# Convert the dictionary to a DataFrame with one row\n",
    "stacked_df = pd.DataFrame([stacked_row])\n",
    "# Create a DataFrame to store the averaged values\n",
    "avrg_Noconf_num = pd.DataFrame({\n",
    "    col: [np.mean(stacked_df.iloc[0][col], axis=0)] for col in ['degree_(numericalVar)',\t'betweeness_(numericalVar)',\t'clusteringcoef_(numericalVar)' , 'eigenvec_(numericalVar)']\n",
    "})\n",
    "# Add 'small_worldness' and 'avg_shortest_path_length' to avrg_Wconf_anat\n",
    "avrg_Noconf_num['smallworldness(numericalVar)'] =stacked_df['smallworldness(numericalVar)']\n",
    "avrg_Noconf_num['avg_shortestPathLength(numericalVar)'] = stacked_df['avg_shortestPathLength(numericalVar)']\n",
    "\n",
    "##########################################################################################3\n",
    "# Assuming 'df' has already been extracted\n",
    "df1= df_Wconf_num[(df_Wconf_num['acquisition'] == 'acq-RL') & \n",
    "                             (df_Wconf_num['session'] == '1')]\n",
    "\n",
    "df1 = df1.drop(columns=['session', 'acquisition', 'subject'])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a new DataFrame for the final stacked result\n",
    "stacked_row = {}\n",
    "\n",
    "# Loop through each column\n",
    "for col in df1.columns:\n",
    "    # If column contains matrices or arrays, stack them vertically\n",
    "    stacked_row[col] = np.vstack(df1[col].values)\n",
    "\n",
    "# Convert the dictionary to a DataFrame with one row\n",
    "stacked_df1 = pd.DataFrame([stacked_row])\n",
    "# Create a DataFrame to store the averaged values\n",
    "avrg_Wconf_num = pd.DataFrame({\n",
    "    col: [np.mean(stacked_df1.iloc[0][col], axis=0)] for col in ['degree_(numericalVar)',\t'betweeness_(numericalVar)',\t'clusteringcoef_(numericalVar)' , 'eigenvec_(numericalVar)']\n",
    "})\n",
    "# Add 'small_worldness' and 'avg_shortest_path_length' to avrg_Wconf_anat\n",
    "avrg_Wconf_num['smallworldness(numericalVar)'] =stacked_df1['smallworldness(numericalVar)']\n",
    "avrg_Wconf_num['avg_shortestPathLength(numericalVar)'] = stacked_df1['avg_shortestPathLength(numericalVar)']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'df' has already been extracted\n",
    "df = df_Noconf_anat[(df_Noconf_anat['acquisition'] == 'acq-RL') & \n",
    "                             (df_Noconf_anat['session'] == '1')&(df_Noconf_anat['iteration']==\"iter_1\")]\n",
    "\n",
    "df = df.drop(columns=['session', 'acquisition', 'iteration'])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a new DataFrame for the final stacked result\n",
    "stacked_row1= {}\n",
    "\n",
    "# Loop through each column\n",
    "for col in df.columns:\n",
    "    # If column contains matrices or arrays, stack them vertically\n",
    "    stacked_row1[col] = np.vstack(df[col].values)\n",
    "\n",
    "# Convert the dictionary to a DataFrame with one row\n",
    "stacked_df = pd.DataFrame([stacked_row1])\n",
    "# Create a DataFrame to store the averaged values\n",
    "avrg_Noconf_anat = pd.DataFrame({\n",
    "    col: [np.mean(stacked_df.iloc[0][col], axis=0)] for col in ['degree_(AnatomicalVar)',\t'betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)' , 'eigenvec_(AnatomicalVar)']\n",
    "})\n",
    "# Add 'small_worldness' and 'avg_shortest_path_length' to avrg_Wconf_anat\n",
    "avrg_Noconf_anat['smallworldness(AnatomicalVar)'] =stacked_df['smallworldness(AnatomicalVar)']\n",
    "avrg_Noconf_anat['avg_shortestPathLength(AnatomicalVar)'] = stacked_df['avg_shortestPathLength(AnatomicalVar)']\n",
    "###########################################################################################3\n",
    "# Assuming 'df' has already been extracted\n",
    "df1 = df_Wconf_anat[(df_Wconf_anat['acquisition'] == 'acq-RL') & \n",
    "                             (df_Wconf_anat['session'] == '1')& (df_Wconf_anat['iteration']==\"iter_1\")]\n",
    "\n",
    "df1 = df1.drop(columns=['session', 'acquisition', 'iteration'])\n",
    "\n",
    "\n",
    "\n",
    "# Initialize a new DataFrame for the final stacked result\n",
    "stacked_row1= {}\n",
    "\n",
    "# Loop through each column\n",
    "for col in df1.columns:\n",
    "    # If column contains matrices or arrays, stack them vertically\n",
    "    stacked_row1[col] = np.vstack(df1[col].values)\n",
    "\n",
    "# Convert the dictionary to a DataFrame with one row\n",
    "stacked_df1 = pd.DataFrame([stacked_row1])\n",
    "# Create a DataFrame to store the averaged values(df_Noconf_anat['iteration']==\"iter_1\")\n",
    "# Create a DataFrame to store the averaged values\n",
    "avrg_Wconf_anat = pd.DataFrame({\n",
    "    col: [np.mean(stacked_df1.iloc[0][col], axis=0)] for col in ['degree_(AnatomicalVar)',\t'betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)' , 'eigenvec_(AnatomicalVar)']\n",
    "})\n",
    "# Add 'small_worldness' and 'avg_shortest_path_length' to avrg_Wconf_anat\n",
    "avrg_Wconf_anat['smallworldness(AnatomicalVar)'] =stacked_df1['smallworldness(AnatomicalVar)']\n",
    "avrg_Wconf_anat['avg_shortestPathLength(AnatomicalVar)'] = stacked_df1['avg_shortestPathLength(AnatomicalVar)']\n",
    "\n",
    "avrg_Wconf_num = avrg_Wconf_num.rename(columns={'degree_(numericalVar)': 'degree_(numericalVarW)', 'betweeness_(numericalVar)': 'betweeness_(numericalVarW)' , 'clusteringcoef_(numericalVar)':'clusteringcoef_(numericalVarW)'   , 'eigenvec_(numericalVar)':'eigenvec_(numericalVarW)'  ,'smallworldness(numericalVar)':'smallworldness(numericalVarW)','avg_shortestPathLength(numericalVar)':'avg_shortestPathLength(numericalVarW)'\n",
    "\n",
    "\t})\n",
    "avrg_Wconf_anat = avrg_Wconf_anat.rename(columns={'degree_(AnatomicalVar)': 'degree_(AnatomicalVarW)', 'betweeness_(AnatomicalVar)': 'betweeness_(AnatomicalVarW)' , 'clusteringcoef_(AnatomicalVar)':'clusteringcoef_(AnatomicalVarW)'   , 'eigenvec_(AnatomicalVar)':'eigenvec_(AnatomicalVarW)','smallworldness(AnatomicalVar)':'smallworldness(AnatomicalVarW)','avg_shortestPathLength(AnatomicalVar)':'avg_shortestPathLength(AnatomicalVarW)'  })\n",
    "\n",
    "# stacked_df1\n",
    "# np.array(stacked_df1.iloc[0]['smallworldness(AnatomicalVar)']).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change(arr,col):\n",
    "    # Create the 'region' column with labels\n",
    "    \n",
    "    regions = [f\"region_{i+1}\" for i in range(len(arr))]\n",
    "\n",
    "    # Create the DataFrame\n",
    "    df = pd.DataFrame({'region': regions, f'{col}': arr})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# List of metrics\n",
    "metrics = ['degree', 'betweeness', 'eigenvec', 'clusteringcoef']\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), \n",
    "    cols=1,  \n",
    "    vertical_spacing=0.05   \n",
    "    # subplot_titles=[\n",
    "    #     f\"Degree Centrality with anatomical mean = {avr_BetweenSubject['degree_(AnatomicalVar)'].mean()}\",\n",
    "    #     f\"Betweeness Centrality with anatomical mean = {avr_BetweenSubject['betweeness_(AnatomicalVar)'].mean()}\",\n",
    "    #     f\"Eigen Vector Centrality with anatomical mean = {avr_BetweenSubject['eigenvec_(AnatomicalVar)'].mean()}\",\n",
    "    #     f\"Clustering Coefficient with anatomical mean = {avr_BetweenSubject['clusteringcoef_(AnatomicalVar)'].mean()}\"\n",
    "    # ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # Extract columns for each metric\n",
    "    columns_to_extract_bet1 =  [col for col in avrg_Wconf_anat.columns if metric in col]\n",
    "    columns_to_extract_with1 = [col for col in avrg_Wconf_num.columns if metric in col]\n",
    "    statics_bet1 = change(avrg_Wconf_anat[columns_to_extract_bet1].iloc[0][avrg_Wconf_anat[columns_to_extract_bet1].columns[0]],avrg_Wconf_anat[columns_to_extract_bet1].columns[0])\n",
    "    statics_with1 = change(avrg_Wconf_num[columns_to_extract_with1].iloc[0][avrg_Wconf_num[columns_to_extract_with1].columns[0]],avrg_Wconf_num[columns_to_extract_with1].columns[0])\n",
    "\n",
    "    columns_to_extract_bet = [col for col in avrg_Noconf_anat.columns if metric in col]\n",
    "    columns_to_extract_with =  [col for col in avrg_Noconf_num.columns if metric in col]\n",
    "    statics_bet = change(avrg_Noconf_anat[columns_to_extract_bet].iloc[0][avrg_Noconf_anat[columns_to_extract_bet].columns[0]],avrg_Noconf_anat[columns_to_extract_bet].columns[0])\n",
    "    statics_with = change(avrg_Noconf_num[columns_to_extract_with].iloc[0][avrg_Noconf_num[columns_to_extract_with].columns[0]],avrg_Noconf_num[columns_to_extract_with].columns[0])\n",
    "    \n",
    "    # Melt the DataFrames\n",
    "    bet_melted1 = statics_bet1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    with_melted1 = statics_with1.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "\n",
    "    bet_melted = statics_bet.melt(id_vars=['region'], var_name='metric', value_name='value')\n",
    "    with_melted = statics_with.melt(id_vars=['region'], var_name='metric', value_name='value')   \n",
    "    # Plot box plots for Between Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=bet_melted1['metric'], \n",
    "            y=bet_melted1['value'],\n",
    "            showlegend=False,\n",
    "            name=f\"Between Subject {metric.capitalize()}\",\n",
    "            boxpoints='all',  # Add stripplot-like points\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                        color='rgb(34,139,34)'),\n",
    "            width=0.2\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "        # Plot box plots for Between Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=bet_melted['metric'], \n",
    "            y=bet_melted['value'],\n",
    "            showlegend=False,\n",
    "            name=f\"Between Subject {metric.capitalize()}\",\n",
    "            boxpoints='all',  # Add stripplot-like points\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                        color='rgb(34,139,34)'),\n",
    "            width=0.2\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    # Plot box plots for Within Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=with_melted1['metric'], \n",
    "            y=with_melted1['value'],\n",
    "            showlegend=False,\n",
    "            width=0.2,  # Smaller box width,\n",
    "            name=f\"Within Subject {metric.capitalize()}\",\n",
    "            boxpoints='suspectedoutliers', # only suspected outliers\n",
    "            jitter=0.3,\n",
    "            pointpos=0.1,\n",
    "            marker=dict(\n",
    "                        color='rgb(8,81,156)',\n",
    "                        outliercolor='rgb(8,81,156)',\n",
    "                        line=dict(\n",
    "                            outliercolor='rgb(8,81,156)',\n",
    "                            outlierwidth=2)),\n",
    "            \n",
    "            text=with_melted1['region'],  # Hover text showing subject ID\n",
    "            hovertemplate=(\n",
    "                \"<b>region:</b> %{text}<br>\"  # Display subject ID\n",
    "                \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "                \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "                \"<extra></extra>\"              # Remove default extra info\n",
    "        )\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "        # Plot box plots for Within Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=with_melted['metric'], \n",
    "            y=with_melted['value'],\n",
    "            showlegend=False,\n",
    "            width=0.2,  # Smaller box width,\n",
    "            name=f\"Within Subject {metric.capitalize()}\",\n",
    "            boxpoints='suspectedoutliers', # only suspected outliers\n",
    "            jitter=0.3,\n",
    "            pointpos=0.1,\n",
    "            marker=dict(\n",
    "                        color='rgb(8,81,156)',\n",
    "                        outliercolor='rgb(8,81,156)',\n",
    "                        line=dict(\n",
    "                            outliercolor='rgb(8,81,156)',\n",
    "                            outlierwidth=2)),\n",
    "            \n",
    "            text=with_melted['region'],  # Hover text showing subject ID\n",
    "            hovertemplate=(\n",
    "                \"<b>region:</b> %{text}<br>\"  # Display subject ID\n",
    "                \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "                \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "                \"<extra></extra>\"              # Remove default extra info\n",
    "        )\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Anatomical and Numerical Variability (Local Graph Metric)\",\n",
    "    height=300 * len(metrics),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Legend\",\n",
    "    margin=dict(l=50, r=50, t=50, b=100),\n",
    "    template='plotly'\n",
    ")\n",
    "\n",
    "# Iterate through rows to remove x-axis tick labels for all subplots\n",
    "for row in range(1, len(metrics) + 1):\n",
    "    fig.update_xaxes(showticklabels=False, row=row, col=1)\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Anatomical Variability with confounds\" + \"&nbsp;\" *65 +  \"Anatomical Variability No confounds\" + \"&nbsp;\" *65  + \"Numerical Variability with confounds\" + \"&nbsp;\" *65 + \"Numerical Variability No confounds\",\n",
    "    row=len(metrics),\n",
    "    col=1,\n",
    "    side='bottom'\n",
    ")\n",
    "# Add section titles for anatomical and numerical data\n",
    "fig.add_annotation(\n",
    "    text=\"Degree Centrality\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=1.018,  # Centered horizontally above the first two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=\"Betweeness Centrality\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.77 , # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "fig.add_annotation(\n",
    "    text=\"Eigen vector Centrality\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.5,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "    text=\"Clustering coefficient\",\n",
    "    xref=\"paper\", yref=\"paper\",\n",
    "    x=0.0, y=0.23,  # Centered horizontally above the last two rows\n",
    "    showarrow=False,\n",
    ")\n",
    "\n",
    "\n",
    "# # Remove annotations for avr_bet\n",
    "# import plotly.io as pio\n",
    "# pio.write_html(fig, \"LocalMetricplot.html\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdofavr_WithinSubject=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/NumGmetricstd_NoConf_batchhc.pkl')\n",
    "avr_WithinSubject=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/NumGmetricavr_NoConf_batchhc.pkl')\n",
    "stdofavr_BetweenSubject=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/AnatGmetricstd_NoConf_batchhc.pkl')\n",
    "avr_BetweenSubject=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/AnatGmetricavr_NoConf_batchhc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdofavr_WithinSubject1 = stdofavr_WithinSubject[\n",
    "    (stdofavr_WithinSubject['session'] == '1') & \n",
    "    (stdofavr_WithinSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness(numericalVar)','avg_shortestPathLength(numericalVar)'])\n",
    "\n",
    "# Rename specific columns\n",
    "stdofavr_WithinSubject1 = stdofavr_WithinSubject1.rename(columns={'degree_(numericalVar)': 'degree_(numericalVar1)', 'betweeness_(numericalVar)': 'betweeness_(numericalVar1)' , 'clusteringcoef_(numericalVar)':'clusteringcoef_(numericalVar1)'   , 'eigenvec_(numericalVar)':'eigenvec_(numericalVar1)'  })\n",
    "\n",
    "stdofavr_BetweenSubject1 = stdofavr_BetweenSubject[\n",
    "    (stdofavr_BetweenSubject['session'] == '1') & \n",
    "    (stdofavr_BetweenSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness_(AnatomicalVar)','avg_shortestPathLength_(AnatomicalVar)'])\n",
    "\n",
    "stdofavr_BetweenSubject1 = stdofavr_BetweenSubject1.rename(columns={'degree_(AnatomicalVar)': 'degree_(AnatomicalVar1)', 'betweeness_(AnatomicalVar)': 'betweeness_(AnatomicalVar1)' , 'clusteringcoef_(AnatomicalVar)':'clusteringcoef_(AnatomicalVar1)'   , 'eigenvec_(AnatomicalVar)':'eigenvec_(AnatomicalVar1)'  })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stdofavr_WithinSubject=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/NumGmetricstd_WConf_batchhc.pkl')\n",
    "df_stdofavr_BetweenSubject=pd.read_pickle('/home/ubuntu/Desktop/Thesis/overlap/allbatches/AnatGmetricstd_WConf_batchhc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stdofavr_WithinSubjectWconf = df_stdofavr_WithinSubject[\n",
    "    (df_stdofavr_WithinSubject['session'] == '1') & \n",
    "    (df_stdofavr_WithinSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness(numericalVar)','avg_shortestPathLength(numericalVar)'])\n",
    "stdofavr_BetweenSubjectWconf = df_stdofavr_BetweenSubject[\n",
    "    (df_stdofavr_BetweenSubject['session'] == '1') & \n",
    "    (df_stdofavr_BetweenSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition','smallworldness_(AnatomicalVar)','avg_shortestPathLength_(AnatomicalVar)'])\n",
    "\n",
    "stdofavr_WithinSubjectWconf = stdofavr_WithinSubjectWconf.rename(columns={'degree_(numericalVar)': 'degree_(numericalVarW)', 'betweeness_(numericalVar)': 'betweeness_(numericalVarW)' , 'clusteringcoef_(numericalVar)':'clusteringcoef_(numericalVarW)'   , 'eigenvec_(numericalVar)':'eigenvec_(numericalVarW)'  })\n",
    "stdofavr_BetweenSubjectWconf = stdofavr_BetweenSubjectWconf.rename(columns={'degree_(AnatomicalVar)': 'degree_(AnatomicalVarW)', 'betweeness_(AnatomicalVar)': 'betweeness_(AnatomicalVarW)' , 'clusteringcoef_(AnatomicalVar)':'clusteringcoef_(AnatomicalVarW)'   , 'eigenvec_(AnatomicalVar)':'eigenvec_(AnatomicalVarW)'  })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# List of metrics\n",
    "metrics = ['degree', 'betweeness', 'eigenvec', 'clusteringcoef']\n",
    "stdofavr_BetweenSubjectWconf\n",
    "# Create subplots\n",
    "from plotly.subplots import make_subplots\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), \n",
    "    cols=1,  \n",
    "    vertical_spacing=0.05,     \n",
    "    subplot_titles=[\n",
    "        f\"Degree Centrality with anatomical mean = {avr_BetweenSubject['degree_(AnatomicalVar)'].mean()}\",\n",
    "        f\"Betweeness Centrality with anatomical mean = {avr_BetweenSubject['betweeness_(AnatomicalVar)'].mean()}\",\n",
    "        f\"Eigen Vector Centrality with anatomical mean = {avr_BetweenSubject['eigenvec_(AnatomicalVar)'].mean()}\",\n",
    "        f\"Clustering Coefficient with anatomical mean = {avr_BetweenSubject['clusteringcoef_(AnatomicalVar)'].mean()}\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    # Extract columns for each metric\n",
    "    columns_to_extract_bet1 = ['iter'] + [col for col in stdofavr_BetweenSubjectWconf .columns if metric in col]\n",
    "    columns_to_extract_with1 = ['subject'] + [col for col in stdofavr_WithinSubjectWconf .columns if metric in col]\n",
    "    \n",
    "    statics_bet1 = stdofavr_BetweenSubjectWconf[columns_to_extract_bet1]\n",
    "    statics_with1 = stdofavr_WithinSubjectWconf[columns_to_extract_with1]\n",
    "\n",
    "    columns_to_extract_bet = ['iter'] + [col for col in stdofavr_BetweenSubject1.columns if metric in col]\n",
    "    columns_to_extract_with = ['subject'] + [col for col in stdofavr_WithinSubject1.columns if metric in col]\n",
    "    \n",
    "    statics_bet = stdofavr_BetweenSubject1[columns_to_extract_bet]\n",
    "    statics_with = stdofavr_WithinSubject1[columns_to_extract_with]\n",
    "    # print(statics_bet)\n",
    "    # # Calculate standard deviations\n",
    "    # std_devs_bet = statics_bet.drop(['iter'], axis=1).std()\n",
    "    # std_devs_with = statics_with.drop(['subject'], axis=1).std()\n",
    "    \n",
    "    # statics_bet = pd.concat([statics_bet, pd.DataFrame([std_devs_bet], columns=std_devs_bet.index)], ignore_index=True)\n",
    "    # statics_bet.at[statics_bet.index[-1], 'iter'] = 'overall_std'\n",
    "\n",
    "    # statics_with = pd.concat([statics_with, pd.DataFrame([std_devs_with], columns=std_devs_with.index)], ignore_index=True)\n",
    "    # statics_with.at[statics_with.index[-1], 'subject'] = 'overall_std'\n",
    "    \n",
    "    # Melt the DataFrames\n",
    "    bet_melted1 = statics_bet1.melt(id_vars=['iter'], var_name='metric', value_name='value')\n",
    "    with_melted1 = statics_with1.melt(id_vars=['subject'], var_name='metric', value_name='value')\n",
    "\n",
    "    bet_melted = statics_bet.melt(id_vars=['iter'], var_name='metric', value_name='value')\n",
    "    with_melted = statics_with.melt(id_vars=['subject'], var_name='metric', value_name='value')   \n",
    "    # Plot box plots for Between Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=bet_melted1['metric'], \n",
    "            y=bet_melted1['value'],\n",
    "            showlegend=False,\n",
    "            name=f\"Between Subject {metric.capitalize()}\",\n",
    "            boxpoints='all',  # Add stripplot-like points\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                        color='rgb(34,139,34)'),\n",
    "            width=0.2\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "        # Plot box plots for Between Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=bet_melted['metric'], \n",
    "            y=bet_melted['value'],\n",
    "            showlegend=False,\n",
    "            name=f\"Between Subject {metric.capitalize()}\",\n",
    "            boxpoints='all',  # Add stripplot-like points\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                        color='rgb(34,139,34)'),\n",
    "            width=0.2\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    # Plot box plots for Within Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=with_melted1['metric'], \n",
    "            y=with_melted1['value'],\n",
    "            showlegend=False,\n",
    "            width=0.2,  # Smaller box width,\n",
    "            name=f\"Within Subject {metric.capitalize()}\",\n",
    "            boxpoints='suspectedoutliers', # only suspected outliers\n",
    "            jitter=0.3,\n",
    "            pointpos=0.1,\n",
    "            marker=dict(\n",
    "                        color='rgb(8,81,156)',\n",
    "                        outliercolor='rgb(8,81,156)',\n",
    "                        line=dict(\n",
    "                            outliercolor='rgb(8,81,156)',\n",
    "                            outlierwidth=2)),\n",
    "            \n",
    "            text=with_melted1['subject'],  # Hover text showing subject ID\n",
    "            hovertemplate=(\n",
    "                \"<b>Subject:</b> %{text}<br>\"  # Display subject ID\n",
    "                \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "                \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "                \"<extra></extra>\"              # Remove default extra info\n",
    "        )\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "        # Plot box plots for Within Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=with_melted['metric'], \n",
    "            y=with_melted['value'],\n",
    "            showlegend=False,\n",
    "            width=0.2,  # Smaller box width,\n",
    "            name=f\"Within Subject {metric.capitalize()}\",\n",
    "            boxpoints='suspectedoutliers', # only suspected outliers\n",
    "            jitter=0.3,\n",
    "            pointpos=0.1,\n",
    "            marker=dict(\n",
    "                        color='rgb(8,81,156)',\n",
    "                        outliercolor='rgb(8,81,156)',\n",
    "                        line=dict(\n",
    "                            outliercolor='rgb(8,81,156)',\n",
    "                            outlierwidth=2)),\n",
    "            \n",
    "            text=with_melted['subject'],  # Hover text showing subject ID\n",
    "            hovertemplate=(\n",
    "                \"<b>Subject:</b> %{text}<br>\"  # Display subject ID\n",
    "                \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "                \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "                \"<extra></extra>\"              # Remove default extra info\n",
    "        )\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Graph metrics with Anatomical and Numerical Variability\",\n",
    "    height=300 * len(metrics),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Legend\",\n",
    "    margin=dict(l=50, r=50, t=50, b=100)\n",
    ")\n",
    "\n",
    "# Iterate through rows to remove x-axis tick labels for all subplots\n",
    "for row in range(1, len(metrics) + 1):\n",
    "    fig.update_xaxes(showticklabels=False, row=row, col=1)\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Anatomical Variability with confounds\" + \"&nbsp;\" *40 +  \"Anatomical Variability No confounds\" + \"&nbsp;\" *40  + \"Numerical Variability with confounds\" + \"&nbsp;\" *40 + \"Numerical Variability No confounds\",\n",
    "    row=len(metrics),\n",
    "    col=1\n",
    ")\n",
    "# Remove annotations for avr_bet\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalMettric_WithinSubject = stdofavr_WithinSubject[\n",
    "    (stdofavr_WithinSubject['session'] == '1') & \n",
    "    (stdofavr_WithinSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition',\t'degree_(numericalVar)','betweeness_(numericalVar)','clusteringcoef_(numericalVar)','eigenvec_(numericalVar)'])\n",
    "\n",
    "\n",
    "globalMettric_BetweenSubject= stdofavr_BetweenSubject[\n",
    "    (stdofavr_BetweenSubject['session'] == '1') & \n",
    "    (stdofavr_BetweenSubject['acquisition'] == 'acq-RL')].drop(columns=['session', 'acquisition','degree_(AnatomicalVar)','betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)','eigenvec_(AnatomicalVar)'])\n",
    "\n",
    "\n",
    "globalMettric_WithinSubjectWconf = df_stdofavr_WithinSubject[\n",
    "    (df_stdofavr_WithinSubject['session'] == '1') & \n",
    "    (df_stdofavr_WithinSubject['acquisition'] == 'acq-RL')\n",
    "].drop(columns=['session', 'acquisition',\t'degree_(numericalVar)','betweeness_(numericalVar)','clusteringcoef_(numericalVar)','eigenvec_(numericalVar)'])\n",
    "\n",
    "\n",
    "globalMettric_BetweenSubjectWconf= df_stdofavr_BetweenSubject[\n",
    "    (df_stdofavr_BetweenSubject['session'] == '1') & \n",
    "    (df_stdofavr_BetweenSubject['acquisition'] == 'acq-RL')].drop(columns=['session', 'acquisition','degree_(AnatomicalVar)','betweeness_(AnatomicalVar)',\t'clusteringcoef_(AnatomicalVar)','eigenvec_(AnatomicalVar)'])\n",
    "globalMettric_WithinSubjectWconf = globalMettric_WithinSubjectWconf.rename(columns={'smallworldness(numericalVar)': 'smallworldness(numericalVarW)', 'avg_shortestPathLength(numericalVar)': 'avg_shortestPathLength(numericalVarW)'   })\n",
    "globalMettric_BetweenSubjectWconf = globalMettric_BetweenSubjectWconf.rename(columns={'smallworldness_(AnatomicalVar)': 'smallworldness_(AnatomicalVarW)', 'avg_shortestPathLength_(AnatomicalVar)': 'avg_shortestPathLength_(AnatomicalVarW)' })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# List of metrics\n",
    "metrics = ['smallworldness', 'avg_shortestPathLength']\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=len(metrics), \n",
    "    cols=1,  \n",
    "    vertical_spacing=0.05,     \n",
    "    subplot_titles=[\n",
    "        f\"Small worldness with anatomical mean = {avr_BetweenSubject['smallworldness_(AnatomicalVar)'].mean()}\",\n",
    "        f\"Avrg shortest path with anatomical mean = {avr_BetweenSubject['avg_shortestPathLength_(AnatomicalVar)'].mean()}\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "\n",
    "        # Extract columns for each metric\n",
    "    columns_to_extract_bet1 = ['iter'] + [col for col in globalMettric_BetweenSubjectWconf.columns if metric in col]\n",
    "    columns_to_extract_with1 = ['subject'] + [col for col in globalMettric_WithinSubjectWconf.columns if metric in col]\n",
    "    \n",
    "    statics_bet1 = globalMettric_BetweenSubjectWconf[columns_to_extract_bet1]\n",
    "    statics_with1 = globalMettric_WithinSubjectWconf[columns_to_extract_with1]\n",
    "    # Extract columns for each metric\n",
    "    columns_to_extract_bet = ['iter'] + [col for col in globalMettric_BetweenSubject.columns if metric in col]\n",
    "    columns_to_extract_with = ['subject'] + [col for col in globalMettric_WithinSubject.columns if metric in col]\n",
    "    \n",
    "    statics_bet = globalMettric_BetweenSubject[columns_to_extract_bet]\n",
    "    statics_with = globalMettric_WithinSubject[columns_to_extract_with]\n",
    "    \n",
    "    # # Calculate standard deviations\n",
    "    # std_devs_bet = statics_bet.drop(['iter'], axis=1).std()\n",
    "    # std_devs_with = statics_with.drop(['subject'], axis=1).std()\n",
    "    \n",
    "    # statics_bet = pd.concat([statics_bet, pd.DataFrame([std_devs_bet], columns=std_devs_bet.index)], ignore_index=True)\n",
    "    # statics_bet.at[statics_bet.index[-1], 'iter'] = 'overall_std'\n",
    "\n",
    "    # statics_with = pd.concat([statics_with, pd.DataFrame([std_devs_with], columns=std_devs_with.index)], ignore_index=True)\n",
    "    # statics_with.at[statics_with.index[-1], 'subject'] = 'overall_std'\n",
    "    \n",
    "    # Melt the DataFrames\n",
    "    bet_melted1 = statics_bet1.melt(id_vars=['iter'], var_name='metric', value_name='value')\n",
    "    with_melted1 = statics_with1.melt(id_vars=['subject'], var_name='metric', value_name='value')\n",
    "    # Melt the DataFrames\n",
    "    bet_melted = statics_bet.melt(id_vars=['iter'], var_name='metric', value_name='value')\n",
    "    with_melted = statics_with.melt(id_vars=['subject'], var_name='metric', value_name='value')\n",
    "    # Plot box plots for Between Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=bet_melted1['metric'], \n",
    "            y=bet_melted1['value'],\n",
    "            showlegend=False,\n",
    "            name=f\"Between Subject With confounds{metric.capitalize()}\",\n",
    "            boxpoints='all',  # Add stripplot-like points\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                        color='rgb(34,139,34)'),\n",
    "            width=0.2\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "     # Plot box plots for Between Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=bet_melted['metric'], \n",
    "            y=bet_melted['value'],\n",
    "            showlegend=False,\n",
    "            name=f\"Between Subject {metric.capitalize()}\",\n",
    "            boxpoints='all',  # Add stripplot-like points\n",
    "            jitter=0.3,\n",
    "            pointpos=0,\n",
    "            marker=dict(\n",
    "                        color='rgb(34,139,34)'),\n",
    "            width=0.2\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )   \n",
    "    # Plot box plots for Within Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=with_melted1['metric'], \n",
    "            y=with_melted1['value'],\n",
    "            showlegend=False,\n",
    "            width=0.2,  # Smaller box width,\n",
    "            name=f\"Within Subject With confounds{metric.capitalize()}\",\n",
    "            boxpoints='suspectedoutliers', # only suspected outliers\n",
    "            jitter=0.3,\n",
    "            pointpos=0.1,\n",
    "            marker=dict(\n",
    "                        color='rgb(8,81,156)',\n",
    "                        outliercolor='rgb(8,81,156)',\n",
    "                        line=dict(\n",
    "                            outliercolor='rgb(8,81,156)',\n",
    "                            outlierwidth=2)),\n",
    "            \n",
    "            text=with_melted1['subject'],  # Hover text showing subject ID\n",
    "            hovertemplate=(\n",
    "                \"<b>Subject:</b> %{text}<br>\"  # Display subject ID\n",
    "                \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "                \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "                \"<extra></extra>\"              # Remove default extra info\n",
    "        )\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "    # Plot box plots for Within Subject\n",
    "    fig.add_trace(\n",
    "        go.Box(\n",
    "            x=with_melted['metric'], \n",
    "            y=with_melted['value'],\n",
    "            showlegend=False,\n",
    "            width=0.2,  # Smaller box width,\n",
    "            name=f\"Within Subject {metric.capitalize()}\",\n",
    "            boxpoints='suspectedoutliers', # only suspected outliers\n",
    "            jitter=0.3,\n",
    "            pointpos=0.1,\n",
    "            marker=dict(\n",
    "                        color='rgb(8,81,156)',\n",
    "                        outliercolor='rgb(8,81,156)',\n",
    "                        line=dict(\n",
    "                            outliercolor='rgb(8,81,156)',\n",
    "                            outlierwidth=2)),\n",
    "            \n",
    "            text=with_melted['subject'],  # Hover text showing subject ID\n",
    "            hovertemplate=(\n",
    "                \"<b>Subject:</b> %{text}<br>\"  # Display subject ID\n",
    "                \"<b>Metric:</b> %{x}<br>\"      # Display metric\n",
    "                \"<b>Value:</b> %{y}<br>\"       # Display value\n",
    "                \"<extra></extra>\"              # Remove default extra info\n",
    "        )\n",
    "        ),\n",
    "        row=i + 1, col=1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=\"Metrics with Anatomical and Numerical Variability\",\n",
    "    height=300 * len(metrics),\n",
    "    showlegend=True,\n",
    "    legend_title=\"Legend\",\n",
    "    margin=dict(l=50, r=50, t=50, b=100)\n",
    ")\n",
    "\n",
    "# Iterate through rows to remove x-axis tick labels for all subplots\n",
    "for row in range(1, len(metrics) + 1):\n",
    "    fig.update_xaxes(showticklabels=False, row=row, col=1)\n",
    "\n",
    "fig.update_xaxes(\n",
    "    title_text=\"Anatomical Variability with confounds\" + \"&nbsp;\" *40 +  \"Anatomical Variability No confounds\" + \"&nbsp;\" *40  + \"Numerical Variability with confounds\" + \"&nbsp;\" *40 + \"Numerical Variability No confounds\",\n",
    "    row=len(metrics),\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Remove annotations for avr_bet\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['subject_id', 'session','acquisition','rep', 'repp', 'region_label', 'dice_coef']\n",
    "\n",
    "# Load the CSV file into a DataFrame with specified column names\n",
    "df_ROI_atlasinT1W = pd.read_csv('/home/ubuntu/Desktop/Thesis/overlap/batchhc/dice_reg_inverse.csv', header=None, names=column_names)\n",
    "df_ROI_atlasinT1W = df_ROI_atlasinT1W.drop(0).reset_index(drop=True)\n",
    "df_ROI_atlasinT1W\n",
    "df_ROI_atlasinT1W['region_label'] = df_ROI_atlasinT1W['region_label'].astype('int64')\n",
    "df_ROI_atlasinT1W['session'] = df_ROI_atlasinT1W['session'].astype('int64')\n",
    "\n",
    "\n",
    "\n",
    "# Load the CSV file into a DataFrame with specified column names\n",
    "df_ROI_atlasinT1W2Bold = pd.read_csv('/home/ubuntu/Desktop/Thesis/overlap/batchhc/dice_coreg_inverse.csv', header=None, names=column_names)\n",
    "# df1 = pd.read_csv('/home/ubuntu/Desktop/Thesis/overlap/batch2/dice_coreg_inverseadd.csv', header=None, names=column_names)\n",
    "# Drop the first row (column names) from the second DataFrame\n",
    "# df1 = df1[1:].reset_index(drop=True)\n",
    "# df_ROI_atlasinT1W2Bold=pd.concat([df, df1], ignore_index=True)\n",
    "df_ROI_atlasinT1W2Bold = df_ROI_atlasinT1W2Bold.drop(0).reset_index(drop=True)\n",
    "df_ROI_atlasinT1W2Bold['region_label'] = df_ROI_atlasinT1W2Bold['region_label'].astype('int64')\n",
    "df_ROI_atlasinT1W2Bold['session'] = df_ROI_atlasinT1W2Bold['session'].astype('int64')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_stackmatrix_OfDice(result_df):\n",
    "    num_sub =len(result_df['subject_id'].unique()) # number of subjects\n",
    "    num_regions = 100  # number of regions\n",
    "    num_MCApairs = 45  # number of dice coefficient combinations\n",
    "    # Initialize stack matrix to hold the data for each subject\n",
    "    stack_matrix = np.empty((num_sub, num_regions, num_MCApairs))\n",
    "    index_to_subject = {}\n",
    "\n",
    "    for index, sub in enumerate(result_df['subject_id'].unique()[:num_sub]):\n",
    "        Dice_coef = np.zeros((num_regions, num_MCApairs))\n",
    "        # Store the mapping of index to subject ID\n",
    "        index_to_subject[index] = sub  \n",
    "        # Filter the DataFrame for the current subject\n",
    "        subject_data = result_df[result_df['subject_id'] == sub]\n",
    "        # print(sub, subject_data)\n",
    "        for region in range(1, num_regions + 1):\n",
    "            # Get the dice coefficients for the current region and subject\n",
    "            region_data = subject_data[subject_data['region_label'] == region]['dice_coef'].values.tolist()[0]\n",
    "            # Ensure we have the expected number of coefficients (45) for each region\n",
    "            if len(region_data) == num_MCApairs:\n",
    "                Dice_coef[region - 1, :] = region_data\n",
    "            else:\n",
    "                print(f\"Warning: Region {region} for subject {sub} has {len(region_data)} values instead of {num_MCApairs}.\")\n",
    "        \n",
    "        # Assign the computed Dice_coef matrix for the current subject to stack_matrix\n",
    "        stack_matrix[index] = Dice_coef\n",
    "    return stack_matrix,index_to_subject\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df_atlasinT1W=df_ROI_atlasinT1W[(df_ROI_atlasinT1W['session']==1) & (df_ROI_atlasinT1W['acquisition']=='acq-RL')] \n",
    "result_df_atlasinT1W=result_df_atlasinT1W.drop(['session', 'acquisition'], axis=1)\n",
    "# Group the DataFrame by subject ID and aggregate the region_label and dice_coef into lists\n",
    "result_df_atlasinT1W = result_df_atlasinT1W.groupby(['subject_id', 'region_label']).agg({\n",
    "\n",
    "    'dice_coef': list\n",
    "}).reset_index()\n",
    "\n",
    "result_df_ROI_atlasinT1W2Bold=df_ROI_atlasinT1W2Bold[(df_ROI_atlasinT1W2Bold['session']==1) & (df_ROI_atlasinT1W2Bold['acquisition']=='acq-RL')] \n",
    "# Group the DataFrame by subject ID and aggregate the region_label and dice_coef into lists\n",
    "result_df_ROI_atlasinT1W2Bold = result_df_ROI_atlasinT1W2Bold.groupby(['subject_id', 'region_label']).agg({\n",
    "\n",
    "    'dice_coef': list\n",
    "}).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Schaefer atlas NIfTI file\n",
    "atlas_path = \"/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_res-01_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.nii.gz\"\n",
    "atlas_img = nib.load(atlas_path)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "# Load the TSV file with atlas labels\n",
    "tsv_file_path = \"/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.tsv\"\n",
    "atlas_df = pd.read_csv(tsv_file_path, delimiter='\\t')\n",
    "\n",
    "# Extract labels\n",
    "atlas_labels = atlas_df['name'].tolist()\n",
    "\n",
    "# Get unique region labels (excluding background if label = 0)\n",
    "unique_labels = np.unique(atlas_data)\n",
    "unique_labels = unique_labels[unique_labels > 0]  # Remove background (0)\n",
    "\n",
    "# Compute voxel count per region\n",
    "region_sizes = {int(label): np.sum(atlas_data == label) for label in unique_labels}\n",
    "\n",
    "# Get voxel size in mm³ (multiply dimensions)\n",
    "voxel_volume = np.prod(atlas_img.header.get_zooms())  # Voxel size in mm³\n",
    "\n",
    "# Convert voxel count to mm³\n",
    "region_sizes_mm3 = {label: count * voxel_volume for label, count in region_sizes.items()}\n",
    "\n",
    "# Create a DataFrame for clarity\n",
    "region_size_df = pd.DataFrame({\n",
    "    'Region': atlas_labels,\n",
    "    'Label': list(region_sizes.keys()),\n",
    "    'Voxel_Count': list(region_sizes.values()),\n",
    "    'Size_mm3': list(region_sizes_mm3.values())\n",
    "})\n",
    "\n",
    "# Display region sizes\n",
    "print(region_size_df)\n",
    "\n",
    "\n",
    "# Sort regions by size (largest to smallest)\n",
    "region_size_df_sorted = region_size_df.sort_values(by=\"Size_mm3\", ascending=False)\n",
    "\n",
    "# Display sorted region sizes\n",
    "print(region_size_df_sorted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select one acusition for subject\n",
    "stack_matrix_T1,indexT=create_stackmatrix_OfDice(result_df_atlasinT1W)\n",
    "stack_matrix_bold,indexB=create_stackmatrix_OfDice(result_df_ROI_atlasinT1W2Bold)\n",
    "\n",
    "Numerical_min_T1=  np.min(stack_matrix_T1,axis=2)\n",
    "numerical_min_bold=np.min(stack_matrix_bold,axis=2)\n",
    "min_T1=np.mean(Numerical_min_T1,axis=0)\n",
    "min_bold=np.mean(numerical_min_bold,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stack_matrix_T1.shape)\n",
    "\n",
    "# Assuming stack_matrix is your (112, 100, 45) matrix\n",
    "average_per_subject = np.mean(stack_matrix_T1, axis=(1, 2))  # Shape will be (112,)\n",
    "top_min_indices = np.argsort(average_per_subject)[:10]\n",
    "print(top_min_indices)\n",
    "average_per_subjectb = np.mean(stack_matrix_bold, axis=(1, 2))  # Shape will be (112,)\n",
    "btop_min_indices = np.argsort(average_per_subjectb)[:10]\n",
    "print(btop_min_indices)\n",
    "\n",
    "filtered_indexT = {k: v for k, v in indexT.items() if k not in top_min_indices}\n",
    "filtered_indexB = {k: v for k, v in indexB.items() if k not in btop_min_indices}\n",
    "print((filtered_indexB))\n",
    "print((indexB))\n",
    "\n",
    "stack_matrix_T1_filtered = np.delete(stack_matrix_T1, top_min_indices, axis=0)\n",
    "stack_matrix_bold_filtered = np.delete(stack_matrix_bold, btop_min_indices, axis=0)\n",
    "Numerical_min_T1_filtered=  np.min(stack_matrix_T1_filtered,axis=2)\n",
    "numerical_min_bold_filtered=np.min(stack_matrix_bold_filtered,axis=2)\n",
    "min_T1_filtered=np.mean(Numerical_min_T1_filtered,axis=0)\n",
    "min_bold_filtered=np.mean(numerical_min_bold_filtered,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stack_matrix_bold.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, datasets, image\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "def Return_Metric_Img(metric_matrix):\n",
    "    # Load an example atlas, e.g., MNI152 template\n",
    "    # atlas = datasets.fetch_atlas_schaefer_2018(n_rois=100, resolution_mm=2)\n",
    "\n",
    "    # # Assume `metric_values` is a list or array of values for the 100 regions\n",
    "    metric_values = metric_matrix # Replace with your actual metric values\n",
    "\n",
    "    # # Load the brain atlas and get the atlas regions\n",
    "    # atlas_img = nib.load(atlas.maps)\n",
    "    # Load your atlas NIfTI file\n",
    "    atlas='/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_res-01_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.nii.gz'\n",
    "    # Define the path to your TSV file\n",
    "    tsv_file_path = \"/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.tsv\"\n",
    "    atlas_img=nib.load(atlas)\n",
    "    atlas_data = atlas_img.get_fdata()\n",
    "    # Create an empty image with the same shape as the atlas\n",
    "    metric_img_data = np.zeros(atlas_img.shape)\n",
    "\n",
    "    # Assign metric values to the atlas regions\n",
    "    for i, region in enumerate(np.unique(atlas_img.get_fdata())):\n",
    "        if region != 0:  # Ignore the background (region 0)\n",
    "            metric_img_data[atlas_img.get_fdata() == region] = metric_values[i-1]  # i-1 to skip background\n",
    "\n",
    "    # Create a new NIfTI image with metric values\n",
    "    metric_img = nib.Nifti1Image(metric_img_data, atlas_img.affine)\n",
    "    return metric_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Assuming you have two different metrics as images\n",
    "metric_img1 = Return_Metric_Img(min_T1)\n",
    "metric_img2 = Return_Metric_Img(min_bold)\n",
    "# vmin = min(np.min(min_T1), np.min(min_bold))\n",
    "vmin=0.85\n",
    "vmax = max(np.max(min_T1), np.max(min_bold))\n",
    "\n",
    "print(np.min(min_T1), np.min(min_bold), np.max(min_T1), np.max(min_bold))\n",
    "# Create a subplot with two figures (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 12))\n",
    "\n",
    "# Set color range and colormap\n",
    "vmax = 1\n",
    "cmap = \"Reds_r\"\n",
    "\n",
    "# Plot the first metric with an individual colorbar\n",
    "plotting.plot_stat_map(\n",
    "    metric_img1,\n",
    "    bg_img=datasets.load_mni152_template(),\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(10, 0, 0),\n",
    "    title='average dice scores of Atlas_InT1W ',\n",
    "    cmap=cmap,\n",
    "    axes=axes[0],\n",
    "    vmin=0.85, #np.min(min_T1),\n",
    "    vmax=vmax,\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "# Plot the second metric with an individual colorbar\n",
    "plotting.plot_stat_map(\n",
    "    metric_img2,\n",
    "    bg_img=datasets.load_mni152_template(),\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(10, 0, 0),\n",
    "    title='average dice scores of Atlas_InBold',\n",
    "    cmap=cmap,\n",
    "    axes=axes[1],\n",
    "    vmin=0.85, #np.min(min_bold),\n",
    "    vmax=vmax,\n",
    "    colorbar=False\n",
    ")\n",
    "# Add a common colorbar for both plots with extended tick marks\n",
    "cbar_ax = fig.add_axes([1, 0.25, 0.02, 0.5])\n",
    "norm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "# Generate custom tick marks across the color range\n",
    "ticks = np.linspace(vmin, vmax, 15)  # Adjust the number of ticks here for finer granularity\n",
    "cbar = fig.colorbar(norm, cax=cbar_ax, label='Dice scores', ticks=ticks)\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Define the text and wrap it to a specific width\n",
    "text = (\"Figure 1: Dice scores were computed for each region across each pair of MCA runs. The minimum Dice scores were identified and then the averages were calculated across subjects for each region. The figure shows the average Dice scores for the registered atlas in subject space (top) and BOLD space (bottom).\")\n",
    "wrapped_text = \"\\n\".join(textwrap.wrap(text, width=180))  # Adjust the width as needed\n",
    "\n",
    "# Add the wrapped text to the figure\n",
    "plt.figtext(0.5, -0.05, wrapped_text, ha=\"center\", fontsize=15,  wrap=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting, datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import textwrap\n",
    "\n",
    "\n",
    "# Assuming you have two different metrics as images\n",
    "metric_img1_filtered = Return_Metric_Img(min_T1_filtered)\n",
    "metric_img2_filtered = Return_Metric_Img(min_bold_filtered)\n",
    "# vmin = min(np.min(min_T1_filtered), np.min(min_bold_filtered))\n",
    "# vmax = max(np.max(min_T1_filtered), np.max(min_bold_filtered))\n",
    "# vmin = min(np.min(min_T1), np.min(min_bold))\n",
    "vmin=0.89\n",
    "vmax = max(np.max(min_T1), np.max(min_bold))\n",
    "print(np.min(min_T1_filtered), np.min(min_bold_filtered), np.max(min_T1_filtered), np.max(min_bold_filtered))\n",
    "# Create a subplot with two figures (1 row, 2 columns)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(20, 12))\n",
    "\n",
    "# Set color range and colormap\n",
    "vmax = 1\n",
    "cmap = \"Reds_r\"\n",
    "\n",
    "# Plot the first metric with an individual colorbar\n",
    "plotting.plot_stat_map(\n",
    "    metric_img1_filtered,\n",
    "    bg_img=datasets.load_mni152_template(),\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(10, 0, 0),\n",
    "    title='average dice scores of Atlas_InT1W ',\n",
    "    cmap=cmap,\n",
    "    axes=axes[0],\n",
    "    vmin=0.89,\n",
    "    vmax=vmax,\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "# Plot the second metric with an individual colorbar\n",
    "plotting.plot_stat_map(\n",
    "    metric_img2_filtered,\n",
    "    bg_img=datasets.load_mni152_template(),\n",
    "    display_mode='ortho',\n",
    "    cut_coords=(10, 0, 0),\n",
    "    title='average dice scores of Atlas_InBold',\n",
    "    cmap=cmap,\n",
    "    axes=axes[1],\n",
    "    vmin=0.89,\n",
    "    vmax=vmax,\n",
    "    colorbar=False\n",
    ")\n",
    "# Add a common colorbar for both plots with extended tick marks\n",
    "cbar_ax = fig.add_axes([1, 0.25, 0.02, 0.5])\n",
    "norm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "\n",
    "# Generate custom tick marks across the color range\n",
    "ticks = np.linspace(vmin, vmax, 15)  # Adjust the number of ticks here for finer granularity\n",
    "cbar = fig.colorbar(norm, cax=cbar_ax, label='Dice scores', ticks=ticks)\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Define the text and wrap it to a specific width\n",
    "text = (\"Figure 1: Dice scores were computed for each region across each pair of MCA runs. The minimum Dice scores were identified and then the averages were calculated across subjects for each region. The figure shows the average Dice scores for the registered atlas in subject space (top) and BOLD space (bottom).\")\n",
    "wrapped_text = \"\\n\".join(textwrap.wrap(text, width=180))  # Adjust the width as needed\n",
    "\n",
    "# Add the wrapped text to the figure\n",
    "plt.figtext(0.5, -0.05, wrapped_text, ha=\"center\", fontsize=15,  wrap=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Numerical Variability:  \\forall r \\in \\left[ 1 \\hspace{0.1cm}, R \\right],  \\hspace{0.5cm} \n",
    "\\Delta_{r} = \\text{mean}_{s \\in \\left[ 1 \\hspace{0.1cm}, S \\right]} \\left( \\min_{(p,p^{'}),(p^{''},p^{'''}) \\in \\left[ 1 \\hspace{0.1cm}, P \\right]} \n",
    "\\hspace{0.2cm} D \\left( V_{r,s,(p,p^{'})}, V_{r,s,(p^{''},p^{'''})} \\right) \\right)\n",
    "\\newline\n",
    "\n",
    "\\hspace{0.5cm}R:\\text{Number of Regions}, \\hspace{0.1cm} S: \\text{Number of Subjects}, \\hspace{0.2cm} \\text{and} \\hspace{0.2cm} P: \\text{Number of MCA pairs}\\,(p,p^{'})\n",
    "\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting based on region size\n",
    "avrofMCApairs_T1=np.mean(stack_matrix_T1,axis=2)\n",
    "avrofMCApairs_bold=np.mean(stack_matrix_bold,axis=2)\n",
    "print(avrofMCApairs_T1.shape)\n",
    "\n",
    "# Step 1: Get sorted region indices based on size\n",
    "sorted_indices = region_size_df_sorted['Label'].index.values  # Get indices after sorting\n",
    "sorted_region=region_size_df_sorted['Region'].values\n",
    "print((sorted_region[0]))\n",
    "# Step 2: Apply sorting to the columns of the matrices\n",
    "sorted_avrofMCApairs_T1 = avrofMCApairs_T1[:, sorted_indices]  # Reorder columns\n",
    "sorted_avrofMCApairs_bold = avrofMCApairs_bold[:, sorted_indices]  # Reorder columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = min(sorted_avrofMCApairs_T1.min(),sorted_avrofMCApairs_bold.min())\n",
    "vmax = max(sorted_avrofMCApairs_T1.max(), sorted_avrofMCApairs_bold.max())\n",
    "# Convert index dictionaries to ordered lists of subject IDs\n",
    "indexT_labels = [indexT[i] for i in range(len(indexT))]  # Extract subject IDs from indexT dictionary\n",
    "indexB_labels = [indexB[i] for i in range(len(indexB))]  # Extract subject IDs from indexB dictionary\n",
    "sorted_region=region_size_df_sorted['Region'].values\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sorted_avrofMCApairs_T1,\n",
    "        y=indexT_labels,  # Use extracted subject IDs for BOLD\n",
    "        x=sorted_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"Average\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sorted_avrofMCApairs_bold,\n",
    "        y=indexB_labels,  # Use extracted subject IDs for BOLD\n",
    "        x=sorted_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of average of MCApairs_T1 and average of MCApairs_bold (Shared Color Map)\",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Remove 5 worse subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting after removing worse subjects based on region size\n",
    "avrofMCApairs_T1_filtered=np.mean(stack_matrix_T1_filtered,axis=2)\n",
    "avrofMCApairs_bold_filtered=np.mean(stack_matrix_bold_filtered,axis=2)\n",
    "\n",
    "# Step 1: Get sorted region indices based on size\n",
    "sorted_indices = region_size_df_sorted['Label'].index.values  # Get indices after sorting\n",
    "sorted_region=region_size_df_sorted['Region'].values\n",
    "print((sorted_region[1]))\n",
    "# Step 2: Apply sorting to the columns of the matrices\n",
    "\n",
    "sorted_avrofMCApairs_T1_filtered = avrofMCApairs_T1_filtered[:, sorted_indices]  # Reorder columns\n",
    "sorted_avrofMCApairs_bold_filtered= avrofMCApairs_bold_filtered[:, sorted_indices]  # Reorder columns\n",
    "\n",
    "# Now sorted_avrofMCApairs_T1 and sorted_avrofMCApairs_bold have regions sorted by increasing size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = min(sorted_avrofMCApairs_T1_filtered.min(), sorted_avrofMCApairs_bold_filtered.min())\n",
    "vmax = max(sorted_avrofMCApairs_T1_filtered.max(), sorted_avrofMCApairs_bold_filtered.max())\n",
    "# Convert index dictionaries to ordered lists of subject IDs\n",
    "indexT_labels_filtered = [filtered_indexT[i] for i in filtered_indexT.keys()]  # Extract subject IDs from indexT dictionary\n",
    "indexB_labels_filtered = [filtered_indexB[i] for i in filtered_indexB.keys()]  # Extract subject IDs from indexB dictionary\n",
    "sorted_region=region_size_df_sorted['Region'].values\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sorted_avrofMCApairs_T1_filtered,\n",
    "        y=indexT_labels_filtered,\n",
    "        x=sorted_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"rainbow_r\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sorted_avrofMCApairs_bold_filtered,\n",
    "        y=indexB_labels_filtered,\n",
    "        x=sorted_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of average dice scores of MCApairs_T1 and average of MCApairs_bold across sorted region by size after removing 10 worse subjects from both T1 and Bold\",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.subplots as sp\n",
    "# import plotly.graph_objects as go\n",
    "# import numpy as np\n",
    "\n",
    "# # Determine shared color map limits\n",
    "# vmin = min(sorted_avrofMCApairs_T1_filtered.min(), sorted_avrofMCApairs_bold_filtered.min())\n",
    "# vmax = max(sorted_avrofMCApairs_T1_filtered.max(), sorted_avrofMCApairs_bold_filtered.max())\n",
    "# # Convert index dictionaries to ordered lists of subject IDs\n",
    "# indexT_labels_filtered = [filtered_indexT[i] for i in filtered_indexT.keys()]  # Extract subject IDs from indexT dictionary\n",
    "# indexB_labels_filtered = [filtered_indexB[i] for i in filtered_indexB.keys()]  # Extract subject IDs from indexB dictionary\n",
    "# sorted_region=region_size_df_sorted['Region'].values\n",
    "\n",
    "# # Create subplots\n",
    "# fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# # Add first heatmap\n",
    "# fig.add_trace(\n",
    "#     go.Heatmap(\n",
    "#         z=sorted_avrofMCApairs_T1_filtered,\n",
    "#         y=indexT_labels_filtered,\n",
    "#         x=sorted_region,\n",
    "#         colorscale='rainbow_r',\n",
    "#         zmin=vmin,\n",
    "#         zmax=vmax,\n",
    "#         colorbar=dict(title=\"rainbow_r\", len=0.8),\n",
    "#     ),\n",
    "#     row=1,\n",
    "#     col=1\n",
    "# )\n",
    "\n",
    "# # Add second heatmap (share the same color scale)\n",
    "# fig.add_trace(\n",
    "#     go.Heatmap(\n",
    "#         z=sorted_avrofMCApairs_bold_filtered,\n",
    "#         y=indexB_labels_filtered,\n",
    "#         x=sorted_region,\n",
    "#         colorscale='rainbow_r',\n",
    "#         zmin=vmin,\n",
    "#         zmax=vmax,\n",
    "#         showscale=False,  # Hide extra colorbar\n",
    "#     ),\n",
    "#     row=1,\n",
    "#     col=2\n",
    "# )\n",
    "\n",
    "# # Update y-axis labels\n",
    "# fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "# fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     height=1000, width=2000,\n",
    "#     title_text=\"Heatmaps of average dice scores of MCApairs_T1 and average of MCApairs_bold across sorted region by size after removing 10 worse subjects from both T1 and Bold\",\n",
    "#     coloraxis_colorbar=dict(len=0.8)\n",
    "# )\n",
    "\n",
    "# fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sorting based on region size and average across subjects values\n",
    "import numpy as np\n",
    "\n",
    "# Compute mean across subjects (axis=0)\n",
    "mean_values = np.mean(sorted_avrofMCApairs_T1, axis=0)  \n",
    "mean_valuesb = np.mean(sorted_avrofMCApairs_bold, axis=0)  \n",
    "\n",
    "# Get sorted indices based on mean values\n",
    "sortedavr_indices = np.argsort(mean_values).astype(int)  \n",
    "sortedavr_indicesb = np.argsort(mean_valuesb).astype(int)  \n",
    "\n",
    "# Convert region labels to NumPy array and reorder\n",
    "sorted_region = np.array(region_size_df_sorted['Region'].values)  \n",
    "sortedavr_region = sorted_region[sortedavr_indices]  \n",
    "sortedavr_regionb = sorted_region[sortedavr_indices]  \n",
    "\n",
    "# Reorder matrix columns (regions)\n",
    "sortedavr_avrofMCApairs_T1= sorted_avrofMCApairs_T1[:, sortedavr_indices]\n",
    "sortedavr_avrofMCApairs_bold_T1= sorted_avrofMCApairs_bold[:, sortedavr_indices]\n",
    "sortedavr_avrofMCApairs_bold= sorted_avrofMCApairs_bold[:, sortedavr_indicesb]\n",
    "\n",
    "# Compute mean across regions for each subject (axis=1)\n",
    "Smean_values = np.mean(sortedavr_avrofMCApairs_T1, axis=1)  \n",
    "Smean_valuesb = np.mean(sortedavr_avrofMCApairs_bold, axis=1)  \n",
    "\n",
    "# Get sorted indices for subjects (convert to integer type)\n",
    "Ssortedavr_indices = np.argsort(Smean_values).astype(int)  \n",
    "Ssortedavr_indicesb = np.argsort(Smean_valuesb).astype(int)  \n",
    "\n",
    "# Convert subject labels to NumPy array\n",
    "indexT_labels= np.array([indexT[i] for i in indexT.keys()])  \n",
    "indexB_labels= np.array([indexB[i] for i in indexB.keys()])  \n",
    "\n",
    "# Reorder subject labels **(For reference, NOT for indexing)**\n",
    "sorted_indexT_labels = indexT_labels[Ssortedavr_indices]  \n",
    "sorted_indexB_labels= indexB_labels[Ssortedavr_indicesb]  \n",
    "\n",
    "# sortedavr_stdofMCApairs_T1 = sortedavr_stdofMCApairs_T1[Ssortedavr_indices, :]  \n",
    "# sortedavr_stdofMCApairs_bold= sortedavr_stdofMCApairs_bold[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_T1= sortedavr_avrofMCApairs_T1[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_bold_T1= sortedavr_avrofMCApairs_bold_T1[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_bold= sortedavr_avrofMCApairs_bold[Ssortedavr_indicesb, :]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = 0.85 # min(sortedavr_avrofMCApairs_T1.min(), sortedavr_avrofMCApairs_bold.min())\n",
    "vmax = max(sortedavr_avrofMCApairs_T1.max(), sortedavr_avrofMCApairs_bold_T1.max())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_T1,\n",
    "        y=sorted_indexT_labels,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"rainbow_r\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_bold_T1,\n",
    "        y=sorted_indexT_labels,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of the AVerage of MCA pairs_T1 and Average of MCApairs_bold sorted based on the average dice scores across subject and across regions of T1 \",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = 0.85 # min(sortedavr_avrofMCApairs_T1.min(), sortedavr_avrofMCApairs_bold.min())\n",
    "vmax = max(sortedavr_avrofMCApairs_T1.max(), sortedavr_avrofMCApairs_bold.max())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_T1,\n",
    "        y=sorted_indexT_labels,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"rainbow_r\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_bold,\n",
    "        y=sorted_indexB_labels,\n",
    "        x=sortedavr_regionb,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of the AVerage of MCA pairs_T1 and Average of MCApairs_bold sorted based on the average dice scores across subject and across regions of T1 and bold separatedly \",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Compute mean across subjects (axis=0)\n",
    "mean_values = np.mean(sorted_avrofMCApairs_T1_filtered, axis=0)  \n",
    "mean_valuesb = np.mean(sorted_avrofMCApairs_bold_filtered, axis=0)  \n",
    "\n",
    "# Get sorted indices based on mean values\n",
    "sortedavr_indices = np.argsort(mean_values).astype(int)  \n",
    "sortedavr_indicesb = np.argsort(mean_valuesb).astype(int)  \n",
    "\n",
    "# Convert region labels to NumPy array and reorder\n",
    "sorted_region = np.array(region_size_df_sorted['Region'].values)  \n",
    "sortedavr_region = sorted_region[sortedavr_indices]  \n",
    "sortedavr_regionb = sorted_region[sortedavr_indicesb]  \n",
    "\n",
    "# Reorder matrix columns (regions)\n",
    "\n",
    "sortedavr_avrofMCApairs_T1_filtered = sorted_avrofMCApairs_T1_filtered[:, sortedavr_indices]\n",
    "sortedavr_avrofMCApairs_bold_T1_filtered = sorted_avrofMCApairs_bold_filtered[:, sortedavr_indices]\n",
    "sortedavr_avrofMCApairs_bold_filtered = sorted_avrofMCApairs_bold_filtered[:, sortedavr_indicesb]\n",
    "\n",
    "# Compute mean across regions for each subject (axis=1)\n",
    "Smean_values = np.mean(sortedavr_avrofMCApairs_T1_filtered, axis=1)  \n",
    "Smean_valuesb = np.mean(sortedavr_avrofMCApairs_bold_filtered, axis=1)  \n",
    "\n",
    "# Get sorted indices for subjects (convert to integer type)\n",
    "Ssortedavr_indices = np.argsort(Smean_values).astype(int)  \n",
    "Ssortedavr_indicesb = np.argsort(Smean_valuesb).astype(int)  \n",
    "\n",
    "# Convert subject labels to NumPy array\n",
    "indexT_labels_filtered = np.array([filtered_indexT[i] for i in filtered_indexT.keys()])  \n",
    "indexB_labels_filtered = np.array([filtered_indexB[i] for i in filtered_indexB.keys()])  \n",
    "\n",
    "# Reorder subject labels **(For reference, NOT for indexing)**\n",
    "sorted_indexT_labels_filtered = indexT_labels_filtered[Ssortedavr_indices]  \n",
    "sorted_indexB_labels_filtered = indexB_labels_filtered[Ssortedavr_indicesb]  \n",
    "\n",
    "\n",
    "sortedavr_avrofMCApairs_T1_filtered = sortedavr_avrofMCApairs_T1_filtered[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_bold_T1_filtered = sortedavr_avrofMCApairs_bold_T1_filtered[Ssortedavr_indices, :]  \n",
    "sortedavr_avrofMCApairs_bold_filtered = sortedavr_avrofMCApairs_bold_filtered[Ssortedavr_indicesb, :]  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = 0.85 # min(sortedavr_avrofMCApairs_T1.min(), sortedavr_avrofMCApairs_bold.min())\n",
    "vmax = max(sortedavr_avrofMCApairs_T1_filtered.max(), sortedavr_avrofMCApairs_bold_T1_filtered.max())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_T1_filtered,\n",
    "        y=sorted_indexT_labels_filtered,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"rainbow_r\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_bold_T1_filtered,\n",
    "        y=sorted_indexT_labels_filtered,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of the AVerage of MCA pairs_T1 and Average of MCApairs_bold sorted based on the average dice scores across subject and across regions of T1 \",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = 0.85 #min(sortedavr_avrofMCApairs_T1_filtered.min(), sortedavr_avrofMCApairs_bold_filtered.min())\n",
    "vmax = max(sortedavr_avrofMCApairs_T1_filtered.max(), sortedavr_avrofMCApairs_bold_filtered.max())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_T1_filtered,\n",
    "        y=sorted_indexT_labels_filtered,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"Average\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_bold_filtered,\n",
    "        y=sorted_indexB_labels_filtered,\n",
    "        x=sortedavr_regionb,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of the AVerage of MCA pairs_T1 and Average of MCApairs_bold sorted based on the average dice scores across subject and across regions of T1 and bold separatedly\",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Determine shared color map limits\n",
    "vmin = 0.85 #min(sortedavr_avrofMCApairs_T1_filtered.min(), sortedavr_avrofMCApairs_bold_filtered.min())\n",
    "vmax = max(sortedavr_avrofMCApairs_T1_filtered.max(), sortedavr_avrofMCApairs_bold_filtered.max())\n",
    "\n",
    "\n",
    "# Create subplots\n",
    "fig = sp.make_subplots(rows=1, cols=2, subplot_titles=[\"T1\", \"Bold\"])\n",
    "\n",
    "# Add first heatmap\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_T1_filtered,\n",
    "        y=sorted_indexT_labels_filtered,\n",
    "        x=sortedavr_region,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        colorbar=dict(title=\"Average\", len=0.8),\n",
    "    ),\n",
    "    row=1,\n",
    "    col=1\n",
    ")\n",
    "\n",
    "# Add second heatmap (share the same color scale)\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=sortedavr_avrofMCApairs_bold_filtered,\n",
    "        y=sorted_indexB_labels_filtered,\n",
    "        x=sortedavr_regionb,\n",
    "        colorscale='rainbow_r',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,  # Hide extra colorbar\n",
    "    ),\n",
    "    row=1,\n",
    "    col=2\n",
    ")\n",
    "\n",
    "# Update y-axis labels\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Subject ID\", row=1, col=2)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=1000, width=2000,\n",
    "    title_text=\"Heatmaps of the AVerage of MCA pairs_T1 and Average of MCApairs_bold sorted based on the average dice scores across subject and across regions of T1 and bold separatedly\",\n",
    "    coloraxis_colorbar=dict(len=0.8)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_matrix_T11=create_stackmatrix_OfDice(result_df_atlasinT1W1)\n",
    "stack_matrix_bold1=create_stackmatrix_OfDice(result_df_ROI_atlasinT1W2Bold1)\n",
    "\n",
    "Numerical_min_T11=  np.min(stack_matrix_T11,axis=2)\n",
    "numerical_min_bold1=np.min(stack_matrix_bold1,axis=2)\n",
    "min_T11=np.mean(Numerical_min_T11,axis=0)\n",
    "min_bold1=np.mean(numerical_min_bold1,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- $$\n",
    "Numerical Variability:  \\forall r \\in \\left[ 1 \\hspace{0.1cm}, R \\right],  \\hspace{0.5cm} \n",
    "\\Delta_{r} = \\text{mean}_{s \\in \\left[ 1 \\hspace{0.1cm}, S \\right]} \\left( \\min_{(p,p^{'}),(p^{''},p^{'''}) \\in \\left[ 1 \\hspace{0.1cm}, P \\right]} \n",
    "\\hspace{0.2cm} D \\left( V_{r,s,(p,p^{'})}, V_{r,s,(p^{''},p^{'''})} \\right) \\right)\n",
    "\\newline\n",
    "Anatomical Variability:  \\forall r \\in \\left[ 1 \\hspace{0.1cm}, R \\right], \\hspace{0.5cm} \\Delta^{'}_{r} = \\text{mean}_{(p,p^{'}) \\in \\left[ 1 \\hspace{0.1cm}, P \\right]} \\left( \\min_{(s,s^{'}) \\in \\left[ 1 \\hspace{0.1cm}, S \\right]} \n",
    "\\hspace{0.2cm} D \\left( V_{r,s,(p,p^{'})}, V_{r,s^{'},(p,p^{'})} \\right) \\right)\\\\\n",
    "\\newline\n",
    "\\hspace{0.5cm}R:\\text{Number of Regions}, \\hspace{0.1cm} S: \\text{Number of Subjects}, \\hspace{0.2cm} \\text{and} \\hspace{0.2cm} P: \\text{Number of MCA pairs}\\,(p,p^{'})\n",
    "\n",
    "\n",
    "$$ -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second formula without computing mean for across subject\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Example lists\n",
    "region_size = list(region_sizes.values())\n",
    "Dice_value = min_T1   # Your list of 100 numerical values\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "correlation, _ = pearsonr(Dice_value, region_size)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(region_size, Dice_value, color='blue', label=f'Pearson Correlation: {correlation:.2f}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('region size')\n",
    "plt.ylabel('Dice Values of T1 registration')\n",
    "plt.title('Scatter Plot of Pearson Correlation')\n",
    "\n",
    "# Show the correlation value on the plot\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#second formula without computing mean for across subject\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Example lists\n",
    "region_size = list(region_sizes.values())\n",
    "Dice_value = min_bold   # Your list of 100 numerical values\n",
    "\n",
    "# Calculate Pearson correlation\n",
    "correlation, _ = pearsonr(Dice_value, region_size)\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(region_size, Dice_value, color='blue', label=f'Pearson Correlation: {correlation:.2f}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('region size')\n",
    "plt.ylabel('Dice Values of bold registration')\n",
    "plt.title('Scatter Plot of Pearson Correlation')\n",
    "\n",
    "# Show the correlation value on the plot\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets, image\n",
    "import numpy as np\n",
    "from scipy.ndimage import center_of_mass\n",
    "# Load your atlas NIfTI file\n",
    "atlas='/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_res-01_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.nii.gz'\n",
    "# Define the path to your TSV file\n",
    "tsv_file_path = \"/home/ubuntu/Desktop/Thesis/overlap/tpl-MNI152NLin2009cAsym_atlas-Schaefer2018_desc-100Parcels7Networks_dseg.tsv\"\n",
    "atlas_img=nib.load(atlas)\n",
    "atlas_data = atlas_img.get_fdata()\n",
    "\n",
    "# Read the TSV file into a DataFrame\n",
    "atlas_datas = pd.read_csv(tsv_file_path, delimiter='\\t')\n",
    "atlas_labels=atlas_datas['name'].tolist\n",
    "\n",
    "# Initialize lists to store coordinates\n",
    "x_coords, y_coords, z_coords = [], [], []\n",
    "\n",
    "# Loop through each unique region ID in the atlas (excluding background if ID = 0)\n",
    "region_ids = np.unique(atlas_data)\n",
    "region_ids = region_ids[region_ids != 0]  # Remove background ID\n",
    "\n",
    "for region_id in region_ids:\n",
    "    # Create a binary mask for the region\n",
    "    region_mask = atlas_data == region_id\n",
    "    \n",
    "    # Calculate the center of mass (centroid) for the region\n",
    "    com = center_of_mass(region_mask)\n",
    "    \n",
    "    # Append coordinates to lists\n",
    "    x_coords.append(com[0])\n",
    "    y_coords.append(com[1])\n",
    "    z_coords.append(com[2])\n",
    "\n",
    "# Convert to numpy arrays\n",
    "x_coords = np.array(x_coords)\n",
    "y_coords = np.array(y_coords)\n",
    "z_coords = np.array(z_coords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "\n",
    "Dice_value = min_T1   # Your list of 100 numerical values\n",
    "\n",
    "# Calculate correlations\n",
    "corr_x, p_x = pearsonr(Dice_value, x_coords)\n",
    "corr_y, p_y = pearsonr(Dice_value, y_coords)\n",
    "corr_z, p_z = pearsonr(Dice_value, z_coords)\n",
    "\n",
    "# Create subplots for each coordinate\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scatter plot for X-coordinate vs Dice values\n",
    "axs[0].scatter(x_coords, Dice_value, color='b', alpha=0.6)\n",
    "axs[0].set_xlabel(\"X Coordinate\")\n",
    "axs[0].set_ylabel(\"Dice Values of T1\")\n",
    "axs[0].set_title(f\"X Coordinate vs Dice Values\\nPearson Correlation: {corr_x:.2f} (p-value: {p_x:.3f})\")\n",
    "\n",
    "# Scatter plot for Y-coordinate vs Dice values\n",
    "axs[1].scatter(y_coords, Dice_value, color='g', alpha=0.6)\n",
    "axs[1].set_xlabel(\"Y Coordinate\")\n",
    "axs[1].set_ylabel(\"Dice Values of T1\")\n",
    "axs[1].set_title(f\"Y Coordinate vs Dice Values\\nPearson Correlation: {corr_y:.2f} (p-value: {p_y:.3f})\")\n",
    "\n",
    "# Scatter plot for Z-coordinate vs Dice values\n",
    "axs[2].scatter(z_coords, Dice_value, color='r', alpha=0.6)\n",
    "axs[2].set_xlabel(\"Z Coordinate\")\n",
    "axs[2].set_ylabel(\"Dice Values of T1\")\n",
    "axs[2].set_title(f\"Z Coordinate vs Dice Values\\nPearson Correlation: {corr_z:.2f} (p-value: {p_z:.3f})\")\n",
    "\n",
    "# Adjust layout and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dice_value=min_T1\n",
    "\n",
    "# Define the reference point (e.g., origin or center of brain)\n",
    "reference_point = np.array([0, 0, 0])\n",
    "\n",
    "# Calculate Euclidean distances from each region's (x, y, z) to the reference point\n",
    "distances = np.sqrt((x_coords - reference_point[0])**2 + \n",
    "                    (y_coords - reference_point[1])**2 + \n",
    "                    (z_coords - reference_point[2])**2)\n",
    "\n",
    "# Calculate the correlation between Dice values and distances\n",
    "corr_dist, p_dist = pearsonr(Dice_value, distances)\n",
    "print(f\"Correlation between Dice values and Euclidean distance: {corr_dist:.2f} (p-value: {p_dist:.3f})\")\n",
    "\n",
    "# Plot Dice values vs. Euclidean distance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(distances, Dice_value, color='purple', alpha=0.6)\n",
    "plt.xlabel(\"Euclidean Distance from Reference Point\")\n",
    "plt.ylabel(\"Dice Values of T1\")\n",
    "plt.title(f\"Dice Values vs Euclidean Distance\\nPearson Correlation: {corr_dist:.2f} (p-value: {p_dist:.3f})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
